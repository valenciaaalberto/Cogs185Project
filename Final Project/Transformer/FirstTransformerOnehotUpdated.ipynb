{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-14T23:57:20.051203Z",
     "start_time": "2019-05-14T23:57:19.626384Z"
    }
   },
   "outputs": [],
   "source": [
    "import string\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Choose a Device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-14T23:57:20.129301Z",
     "start_time": "2019-05-14T23:57:20.081156Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "# If there are GPUs, choose the first one for computing. Otherwise use CPU.\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)  \n",
    "# If 'cuda:0' is printed, it means GPU is available."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Changes attempted\n",
    "- only encoder\n",
    "- making the input and target basically the same except for one shifted sequences\n",
    "- generating a mask in training loop as opposed to in the tranformer module"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Transformer Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        # Ensure that the model dimension (d_model) is divisible by the number of heads\n",
    "        assert d_model % num_heads == 0, \"d_model must be divisible by num_heads\"\n",
    "        \n",
    "        # Initialize dimensions\n",
    "        self.d_model = d_model # Model's dimension\n",
    "        self.num_heads = num_heads # Number of attention heads\n",
    "        self.d_k = d_model // num_heads # Dimension of each head's key, query, and value\n",
    "        \n",
    "        # Linear layers for transforming inputs\n",
    "        self.W_q = nn.Linear(d_model, d_model) # Query transformation\n",
    "        self.W_k = nn.Linear(d_model, d_model) # Key transformation\n",
    "        self.W_v = nn.Linear(d_model, d_model) # Value transformation\n",
    "        self.W_o = nn.Linear(d_model, d_model) # Output transformation\n",
    "        \n",
    "    def scaled_dot_product_attention(self, Q, K, V, mask=None):\n",
    "        # Calculate attention scores\n",
    "        attn_scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.d_k)\n",
    "        \n",
    "        # Apply mask if provided (useful for preventing attention to certain parts like padding)\n",
    "        if mask is not None:\n",
    "            attn_scores = attn_scores.masked_fill(mask == 0, -1e9)\n",
    "        \n",
    "        # Softmax is applied to obtain attention probabilities\n",
    "        attn_probs = torch.softmax(attn_scores, dim=-1)\n",
    "        \n",
    "        # Multiply by values to obtain the final output\n",
    "        output = torch.matmul(attn_probs, V)\n",
    "        return output\n",
    "        \n",
    "    def split_heads(self, x):\n",
    "        # Reshape the input to have num_heads for multi-head attention\n",
    "        batch_size, seq_length, d_model = x.size()\n",
    "        return x.view(batch_size, seq_length, self.num_heads, self.d_k).transpose(1, 2)\n",
    "        \n",
    "    def combine_heads(self, x):\n",
    "        # Combine the multiple heads back to original shape\n",
    "        batch_size, _, seq_length, d_k = x.size()\n",
    "        return x.transpose(1, 2).contiguous().view(batch_size, seq_length, self.d_model)\n",
    "        \n",
    "    def forward(self, Q, K, V, mask=None):\n",
    "        # Apply linear transformations and split heads\n",
    "        Q = self.split_heads(self.W_q(Q))\n",
    "        K = self.split_heads(self.W_k(K))\n",
    "        V = self.split_heads(self.W_v(V))\n",
    "        \n",
    "        # Perform scaled dot-product attention\n",
    "        attn_output = self.scaled_dot_product_attention(Q, K, V, mask)\n",
    "        \n",
    "        # Combine heads and apply output transformation\n",
    "        output = self.W_o(self.combine_heads(attn_output))\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_seq_length):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        \n",
    "        pe = torch.zeros(max_seq_length, d_model)\n",
    "        position = torch.arange(0, max_seq_length, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * -(math.log(10000.0) / d_model))\n",
    "        \n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        \n",
    "        self.register_buffer('pe', pe.unsqueeze(0))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return x + self.pe[:, :x.size(1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionWiseFeedForward(nn.Module):\n",
    "    def __init__(self, d_model, d_ff):\n",
    "        super(PositionWiseFeedForward, self).__init__()\n",
    "        self.fc1 = nn.Linear(d_model, d_ff)\n",
    "        self.fc2 = nn.Linear(d_ff, d_model)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, d_ff, dropout):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "        self.self_attn = MultiHeadAttention(d_model, num_heads)\n",
    "        self.feed_forward = PositionWiseFeedForward(d_model, d_ff)\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x, mask):\n",
    "        attn_output = self.self_attn(x, x, x, mask)\n",
    "        x = self.norm1(x + self.dropout(attn_output))\n",
    "        ff_output = self.feed_forward(x)\n",
    "        x = self.norm2(x + self.dropout(ff_output))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class DecoderLayer(nn.Module):\n",
    "#     def __init__(self, d_model, num_heads, d_ff, dropout):\n",
    "#         super(DecoderLayer, self).__init__()\n",
    "#         self.self_attn = MultiHeadAttention(d_model, num_heads)\n",
    "#         self.cross_attn = MultiHeadAttention(d_model, num_heads)\n",
    "#         self.feed_forward = PositionWiseFeedForward(d_model, d_ff)\n",
    "#         self.norm1 = nn.LayerNorm(d_model)\n",
    "#         self.norm2 = nn.LayerNorm(d_model)\n",
    "#         self.norm3 = nn.LayerNorm(d_model)\n",
    "#         self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "#     #for decoder, attempt only self attention. \n",
    "        \n",
    "#     def forward(self, x, enc_output, src_mask, tgt_mask):\n",
    "#         attn_output = self.self_attn(x, x, x, tgt_mask)\n",
    "#         # print(\"Attention output\", attn_output)\n",
    "#         x = self.norm1(x + self.dropout(attn_output))\n",
    "#         attn_output = self.cross_attn(x, enc_output, enc_output, src_mask)\n",
    "#         x = self.norm2(x + self.dropout(attn_output))\n",
    "#         ff_output = self.feed_forward(x)\n",
    "#         x = self.norm3(x + self.dropout(ff_output))\n",
    "#         return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(self, src_vocab_size, tgt_vocab_size, d_model, num_heads, num_layers, d_ff, max_seq_length, dropout):\n",
    "        super(Transformer, self).__init__()\n",
    "        self.encoder_embedding = nn.Embedding(src_vocab_size, d_model)\n",
    "        self.decoder_embedding = nn.Embedding(tgt_vocab_size, d_model)\n",
    "        self.positional_encoding = PositionalEncoding(d_model, max_seq_length)\n",
    "\n",
    "        self.encoder_layers = nn.ModuleList([EncoderLayer(d_model, num_heads, d_ff, dropout) for _ in range(num_layers)])\n",
    "        # self.decoder_layers = nn.ModuleList([DecoderLayer(d_model, num_heads, d_ff, dropout) for _ in range(num_layers)])\n",
    "\n",
    "        self.fc = nn.Linear(d_model, tgt_vocab_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    # def generate_mask(self, src, tgt):\n",
    "    #     src_mask = (src != 0).unsqueeze(1).unsqueeze(2)\n",
    "    #     tgt_mask = (tgt != 0).unsqueeze(1).unsqueeze(3)\n",
    "    #     seq_length = tgt.size(1)\n",
    "    #     nopeak_mask = (1 - torch.triu(torch.ones(1, seq_length, seq_length), diagonal=1)).bool().to(device)\n",
    "    #     tgt_mask = (tgt_mask & nopeak_mask).to(device)\n",
    "    #     return src_mask, tgt_mask\n",
    "\n",
    "\n",
    "\n",
    "    # def forward(self, src, tgt):\n",
    "    #     src_mask, tgt_mask = self.generate_mask(src, tgt)\n",
    "    #     src_embedded = self.dropout(self.positional_encoding(self.encoder_embedding(src)))\n",
    "    #     # tgt_embedded = self.dropout(self.positional_encoding(self.decoder_embedding(tgt)))\n",
    "    #     tgt_embedded = self.dropout(self.positional_encoding(self.encoder_embedding(src)))\n",
    "    \n",
    "    #     enc_output = src_embedded\n",
    "    #     for enc_layer in self.encoder_layers:\n",
    "    #         enc_output = enc_layer(enc_output, src_mask)\n",
    "      \n",
    "      \n",
    "    #     dec_output = tgt_embedded\n",
    "    #     for dec_layer in self.decoder_layers:\n",
    "    #         dec_output = dec_layer(dec_output, enc_output, src_mask, tgt_mask)\n",
    "\n",
    "    #     output = self.fc(dec_output).to(device)\n",
    "    #     return output\n",
    "\n",
    "    #perhaps we should pass in mask instead of target; i get this idea from gpt's\n",
    "    def forward(self, src, target, src_mask):\n",
    "        src_embedded = self.dropout(self.positional_encoding(self.encoder_embedding(src)))\n",
    "        # tgt_embedded = self.dropout(self.positional_encoding(self.decoder_embedding(tgt)))\n",
    "        # src_mask_embedded = self.dropout(self.positional_encoding(self.encoder_embedding(src_mask)))\n",
    "        # tgt_embedded = self.dropout(self.positional_encoding(self.decoder_embedding(target)))\n",
    "        tgt_embedded = self.dropout(self.positional_encoding(self.encoder_embedding(target)))\n",
    "        \n",
    "        enc_output = src_embedded\n",
    "        for enc_layer in self.encoder_layers:\n",
    "            enc_output = enc_layer(enc_output, src_mask)\n",
    "    \n",
    "        enc_tgt_output = tgt_embedded \n",
    "        for enc_layer in self.encoder_layers:\n",
    "            enc_output = enc_layer(enc_tgt_output, src_mask)\n",
    "\n",
    "      \n",
    "\n",
    "        # dec_output = tgt_embedded\n",
    "        # for dec_layer in self.decoder_layers:\n",
    "        #     dec_output = dec_layer(dec_output, enc_output, src_mask, target)\n",
    "\n",
    "        output = self.fc(enc_output).to(device)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of file: 1115394\n",
      "All possible characters: 0123456789abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~ \t\n",
      "\u000b\f\n",
      "Number of all possible characters: 100\n"
     ]
    }
   ],
   "source": [
    "all_chars       = string.printable\n",
    "n_chars         = len(all_chars)\n",
    "file            = open('../Data/shakespeare.txt').read()\n",
    "file_len        = len(file)\n",
    "\n",
    "print('Length of file: {}'.format(file_len))\n",
    "print('All possible characters: {}'.format(all_chars))\n",
    "print('Number of all possible characters: {}'.format(n_chars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a random sequence of the Shakespeare dataset.\n",
    "def get_random_seq():\n",
    "    seq_len     = 128  # The length of an input sequence.\n",
    "    start_index = random.randint(0, file_len - seq_len)\n",
    "    end_index   = start_index + seq_len + 1\n",
    "    return file[start_index:end_index]\n",
    "\n",
    "# Convert the sequence to one-hot tensor.\n",
    "def seq_to_onehot(seq):\n",
    "    tensor = torch.zeros(len(seq), n_chars,dtype = torch.long) \n",
    "    # Shape of the tensor:\n",
    "    #     (sequence length, batch size, classes)\n",
    "    # Here we use batch size = 1 and classes = number of unique characters.\n",
    "    for t, char in enumerate(seq):\n",
    "        index = all_chars.index(char) \n",
    "        # tensor[t][0][index] = 1\n",
    "        tensor[t][index] = 1\n",
    "    return tensor\n",
    "\n",
    "# Convert the sequence to index tensor.\n",
    "def seq_to_index(seq):\n",
    "    tensor = torch.zeros(len(seq), dtype = torch.long)\n",
    "    # Shape of the tensor: \n",
    "    #     (sequence length, batch size).\n",
    "    # Here we use batch size = 1.\n",
    "    for t, char in enumerate(seq):\n",
    "        tensor[t] = all_chars.index(char)+ 1\n",
    "    return tensor\n",
    "\n",
    "# Sample a mini-batch including input tensor and target tensor.\n",
    "def get_input_and_target():\n",
    "    seq    = get_random_seq()\n",
    "    input  = seq_to_index(seq[:-1]).long()      # Input is represented in one-hot.\n",
    "    target = seq_to_index(seq[1:]).long() # Target is represented in index.\n",
    "    return input, target\n",
    "\n",
    "def generate_square_mask(src):\n",
    "    mask = torch.triu(torch.ones(src.size(0), src.size(0),dtype=torch.long) == 1).transpose(0, 1)\n",
    "    mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n",
    "    return mask.long()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instantiate Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Transformer(\n",
       "  (encoder_embedding): Embedding(100, 512)\n",
       "  (decoder_embedding): Embedding(100, 512)\n",
       "  (positional_encoding): PositionalEncoding()\n",
       "  (encoder_layers): ModuleList(\n",
       "    (0-3): 4 x EncoderLayer(\n",
       "      (self_attn): MultiHeadAttention(\n",
       "        (W_q): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (W_k): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (W_v): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (W_o): Linear(in_features=512, out_features=512, bias=True)\n",
       "      )\n",
       "      (feed_forward): PositionWiseFeedForward(\n",
       "        (fc1): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (fc2): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (relu): ReLU()\n",
       "      )\n",
       "      (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (fc): Linear(in_features=512, out_features=100, bias=True)\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Instantiate the Transformer model\n",
    "d_model = 512\n",
    "num_heads = 4\n",
    "num_layers = 4\n",
    "d_ff = 512\n",
    "max_seq_length = 128\n",
    "dropout = 0.1\n",
    "\n",
    "model = Transformer(\n",
    "    src_vocab_size=100,\n",
    "    tgt_vocab_size=100,\n",
    "    d_model=d_model,\n",
    "    num_heads=num_heads,\n",
    "    num_layers=num_layers,\n",
    "    d_ff=d_ff,\n",
    "    max_seq_length=max_seq_length,\n",
    "    dropout=dropout\n",
    ")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Procedure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter:199/20000 loss:4.850490093231201\n",
      "iter:399/20000 loss:4.857835292816162\n",
      "iter:599/20000 loss:4.8516669273376465\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[192], line 24\u001b[0m\n\u001b[1;32m     22\u001b[0m input_mask \u001b[38;5;241m=\u001b[39m generate_square_mask(\u001b[38;5;28minput\u001b[39m)\n\u001b[1;32m     23\u001b[0m \u001b[38;5;66;03m# predicted = model(input,input_mask)\u001b[39;00m\n\u001b[0;32m---> 24\u001b[0m predicted \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43minput_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     25\u001b[0m predicted \u001b[38;5;241m=\u001b[39m predicted\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m0\u001b[39m,\u001b[38;5;241m2\u001b[39m,\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     26\u001b[0m loss \u001b[38;5;241m=\u001b[39m loss_func(predicted,target)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[188], line 52\u001b[0m, in \u001b[0;36mTransformer.forward\u001b[0;34m(self, src, target, src_mask)\u001b[0m\n\u001b[1;32m     50\u001b[0m enc_output \u001b[38;5;241m=\u001b[39m src_embedded\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m enc_layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoder_layers:\n\u001b[0;32m---> 52\u001b[0m     enc_output \u001b[38;5;241m=\u001b[39m \u001b[43menc_layer\u001b[49m\u001b[43m(\u001b[49m\u001b[43menc_output\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msrc_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     54\u001b[0m enc_tgt_output \u001b[38;5;241m=\u001b[39m tgt_embedded \n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m enc_layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoder_layers:\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[186], line 12\u001b[0m, in \u001b[0;36mEncoderLayer.forward\u001b[0;34m(self, x, mask)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, mask):\n\u001b[1;32m     11\u001b[0m     attn_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mself_attn(x, x, x, mask)\n\u001b[0;32m---> 12\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnorm1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdropout\u001b[49m\u001b[43m(\u001b[49m\u001b[43mattn_output\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m     ff_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfeed_forward(x)\n\u001b[1;32m     14\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm2(x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(ff_output))\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/torch/nn/modules/normalization.py:201\u001b[0m, in \u001b[0;36mLayerNorm.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    200\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 201\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayer_norm\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    202\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnormalized_shape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meps\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/torch/nn/functional.py:2546\u001b[0m, in \u001b[0;36mlayer_norm\u001b[0;34m(input, normalized_shape, weight, bias, eps)\u001b[0m\n\u001b[1;32m   2542\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_variadic(\u001b[38;5;28minput\u001b[39m, weight, bias):\n\u001b[1;32m   2543\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m   2544\u001b[0m         layer_norm, (\u001b[38;5;28minput\u001b[39m, weight, bias), \u001b[38;5;28minput\u001b[39m, normalized_shape, weight\u001b[38;5;241m=\u001b[39mweight, bias\u001b[38;5;241m=\u001b[39mbias, eps\u001b[38;5;241m=\u001b[39meps\n\u001b[1;32m   2545\u001b[0m     )\n\u001b[0;32m-> 2546\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayer_norm\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnormalized_shape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackends\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcudnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menabled\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Number of iterations.\n",
    "iters       = 20000\n",
    "# Number of training iterations.\n",
    "print_iters = 200   # Number of iterations for each log printing.\n",
    "\n",
    "# The loss variables.\n",
    "all_losses = []\n",
    "loss_sum   = 0\n",
    "\n",
    "# Initialize the optimizer and the loss function.\n",
    "opt       = torch.optim.Adam(model.parameters(), lr=0.005)\n",
    "loss_func = nn.CrossEntropyLoss()\n",
    "\n",
    "# Training procedure.\n",
    "for i in range(iters):\n",
    "    input, target = get_input_and_target()            # Fetch input and target.\n",
    "    # print(input.shape)\n",
    "    # print(target.shape)\n",
    "    input, target = input.to(device), target.to(device) # Move to GPU memory.\n",
    "    input = input[None] \n",
    "    target = target[None] \n",
    "    input_mask = generate_square_mask(input)\n",
    "    # predicted = model(input,input_mask)\n",
    "    predicted = model(input,target,input_mask)\n",
    "    predicted = predicted.permute(0,2,1)\n",
    "    loss = loss_func(predicted,target)\n",
    "    loss_sum += loss                                  # Accumulate the loss.\n",
    "\n",
    "    # Print the log.\n",
    "    if i % print_iters == print_iters - 1:\n",
    "        print('iter:{}/{} loss:{}'.format(i, iters, loss_sum / print_iters))\n",
    "#         print('generated sequence: {}\\n'.format(eval_step(net)))\n",
    "              \n",
    "        # Track the loss.\n",
    "        all_losses.append(loss_sum / print_iters)\n",
    "        loss_sum = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training Loss Curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-15T00:38:13.728474Z",
     "start_time": "2019-05-15T00:38:13.559531Z"
    }
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAkIAAAG2CAYAAACTTOmSAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjguMCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy81sbWrAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAiyElEQVR4nO3df1BVdf7H8dcV5F7TuI5YiIKAP8OcLGE1ILZyFVOzdbZGWndSW91iyghIS3LLdJzYanLLFPqluc1asf5cZyKTZkrxR7UStI6wqyUJGsSCG6C2kHC+fzTe77KgCd0fcD/Px8yd2fvxnHvft7N2n51zudgsy7IEAABgoF6+HgAAAMBXCCEAAGAsQggAABiLEAIAAMYihAAAgLEIIQAAYCxCCAAAGIsQAgAAxiKEAACAsQghAABgLJ+G0N69ezVz5kwNHjxYNptNO3bs+NF99uzZo9jYWDkcDg0bNkwvv/yy5wcFAAB+yachdPbsWY0bN05r1669rO3Ly8s1ffp0JSUlqbi4WI8//rjS0tK0detWD08KAAD8ka27/NJVm82m7du3a9asWRfd5rHHHtPOnTtVVlbmWktNTdXnn3+ugwcPemFKAADgTwJ9PUBnHDx4UMnJyW3Wpk6dqvXr1+v7779X79692+3T1NSkpqYm1/3W1ladPn1aISEhstlsHp8ZAAD8dJZlqbGxUYMHD1avXu67oNWjQqi6ulqhoaFt1kJDQ3X+/HnV1tYqLCys3T7Z2dlasWKFt0YEAAAeVFlZqfDwcLc9Xo8KIUntzuJcuLJ3sbM7WVlZyszMdN2vr6/X0KFDVVlZqeDgYM8NCgAA3KahoUERERG68sor3fq4PSqEBg0apOrq6jZrNTU1CgwMVEhISIf72O122e32duvBwcGEEAAAPYy7P9bSo75HKD4+XgUFBW3Wdu/erbi4uA4/HwQAAHApPg2hM2fOqKSkRCUlJZJ++PH4kpISVVRUSPrhstbcuXNd26empurEiRPKzMxUWVmZNmzYoPXr12vx4sW+GB8AAPRwPr00dujQId16662u+xc+yzNv3jxt3LhRVVVVriiSpOjoaOXn5ysjI0Pr1q3T4MGDtWbNGt15551enx0AAPR83eZ7hLyloaFBTqdT9fX1fEYIAIAewlPv3z3qM0IAAADuRAgBAABjEUIAAMBYhBAAADAWIQQAAIxFCAEAAGMRQgAAwFiEEAAAMBYhBAAAjEUIAQAAYxFCAADAWIQQAAAwFiEEAACMRQgBAABjEUIAAMBYhBAAADAWIQQAAIxFCAEAAGMRQgAAwFiEEAAAMBYhBAAAjEUIAQAAYxFCAADAWIQQAAAwFiEEAACMRQgBAABjEUIAAMBYhBAAADAWIQQAAIxFCAEAAGMRQgAAwFiEEAAAMBYhBAAAjEUIAQAAYxFCAADAWIQQAAAwFiEEAACMRQgBAABjEUIAAMBYhBAAADAWIQQAAIxFCAEAAGMRQgAAwFiEEAAAMBYhBAAAjEUIAQAAYxFCAADAWIQQAAAwFiEEAACMRQgBAABjEUIAAMBYhBAAADAWIQQAAIxFCAEAAGMRQgAAwFiEEAAAMBYhBAAAjEUIAQAAYxFCAADAWIQQAAAwFiEEAACMRQgBAABjEUIAAMBYPg+hnJwcRUdHy+FwKDY2VoWFhZfcftOmTRo3bpyuuOIKhYWF6d5771VdXZ2XpgUAAP7EpyGUl5en9PR0LVu2TMXFxUpKStK0adNUUVHR4fb79u3T3LlztWDBAh05ckSbN2/W3/72Ny1cuNDLkwMAAH/g0xBavXq1FixYoIULFyomJkYvvPCCIiIilJub2+H2H3/8saKiopSWlqbo6GjddNNNuv/++3Xo0CEvTw4AAPyBz0KoublZRUVFSk5ObrOenJysAwcOdLhPQkKCTp48qfz8fFmWpW+++UZbtmzRjBkzLvo8TU1NamhoaHMDAACQfBhCtbW1amlpUWhoaJv10NBQVVdXd7hPQkKCNm3apJSUFAUFBWnQoEHq37+/XnrppYs+T3Z2tpxOp+sWERHh1tcBAAB6Lp9/WNpms7W5b1lWu7ULSktLlZaWpieffFJFRUXatWuXysvLlZqaetHHz8rKUn19vetWWVnp1vkBAEDPFeirJx44cKACAgLanf2pqalpd5boguzsbCUmJmrJkiWSpOuuu059+/ZVUlKSVq1apbCwsHb72O122e12978AAADQ4/nsjFBQUJBiY2NVUFDQZr2goEAJCQkd7nPu3Dn16tV25ICAAEk/nEkCAADoDJ9eGsvMzNTrr7+uDRs2qKysTBkZGaqoqHBd6srKytLcuXNd28+cOVPbtm1Tbm6ujh8/rv379ystLU0TJkzQ4MGDffUyAABAD+WzS2OSlJKSorq6Oq1cuVJVVVUaO3as8vPzFRkZKUmqqqpq851C8+fPV2Njo9auXatHHnlE/fv316RJk/TMM8/46iUAAIAezGYZdk2poaFBTqdT9fX1Cg4O9vU4AADgMnjq/dvnPzUGAADgK4QQAAAwFiEEAACMRQgBAABjEUIAAMBYhBAAADAWIQQAAIxFCAEAAGMRQgAAwFiEEAAAMBYhBAAAjEUIAQAAYxFCAADAWIQQAAAwFiEEAACMRQgBAABjEUIAAMBYhBAAADAWIQQAAIxFCAEAAGMRQgAAwFiEEAAAMBYhBAAAjEUIAQAAYxFCAADAWIQQAAAwFiEEAACMRQgBAABjEUIAAMBYhBAAADAWIQQAAIxFCAEAAGMRQgAAwFiEEAAAMBYhBAAAjEUIAQAAYxFCAADAWIQQAAAwFiEEAACMRQgBAABjEUIAAMBYhBAAADAWIQQAAIxFCAEAAGMRQgAAwFiEEAAAMBYhBAAAjEUIAQAAYxFCAADAWIQQAAAwFiEEAACMRQgBAABjEUIAAMBYhBAAADAWIQQAAIxFCAEAAGMRQgAAwFiEEAAAMBYhBAAAjEUIAQAAYxFCAADAWIQQAAAwFiEEAACM5fMQysnJUXR0tBwOh2JjY1VYWHjJ7ZuamrRs2TJFRkbKbrdr+PDh2rBhg5emBQAA/iTQl0+el5en9PR05eTkKDExUa+88oqmTZum0tJSDR06tMN9Zs+erW+++Ubr16/XiBEjVFNTo/Pnz3t5cgAA4A9slmVZvnryiRMnavz48crNzXWtxcTEaNasWcrOzm63/a5du3T33Xfr+PHjGjBgQJees6GhQU6nU/X19QoODu7y7AAAwHs89f7ts0tjzc3NKioqUnJycpv15ORkHThwoMN9du7cqbi4OD377LMaMmSIRo0apcWLF+u777676PM0NTWpoaGhzQ0AAEDy4aWx2tpatbS0KDQ0tM16aGioqqurO9zn+PHj2rdvnxwOh7Zv367a2lo98MADOn369EU/J5Sdna0VK1a4fX4AANDz+fzD0jabrc19y7LarV3Q2toqm82mTZs2acKECZo+fbpWr16tjRs3XvSsUFZWlurr6123yspKt78GAADQM/nsjNDAgQMVEBDQ7uxPTU1Nu7NEF4SFhWnIkCFyOp2utZiYGFmWpZMnT2rkyJHt9rHb7bLb7e4dHgAA+AWfnREKCgpSbGysCgoK2qwXFBQoISGhw30SExP19ddf68yZM661o0ePqlevXgoPD/fovAAAwP/49NJYZmamXn/9dW3YsEFlZWXKyMhQRUWFUlNTJf1wWWvu3Lmu7efMmaOQkBDde++9Ki0t1d69e7VkyRL99re/VZ8+fXz1MgAAQA/l0+8RSklJUV1dnVauXKmqqiqNHTtW+fn5ioyMlCRVVVWpoqLCtX2/fv1UUFCghx56SHFxcQoJCdHs2bO1atUqX70EAADQg/n0e4R8ge8RAgCg5/G77xECAADwNUIIAAAYixACAADGIoQAAICxCCEAAGAsQggAABiLEAIAAMYihAAAgLEIIQAAYCxCCAAAGIsQAgAAxiKEAACAsQghAABgLEIIAAAYixACAADGIoQAAICxCCEAAGAsQggAABiLEAIAAMYihAAAgLEIIQAAYKwuhdCf/vQnvfvuu677jz76qPr376+EhASdOHHCbcMBAAB4UpdC6Omnn1afPn0kSQcPHtTatWv17LPPauDAgcrIyHDrgAAAAJ4S2JWdKisrNWLECEnSjh07dNddd+m+++5TYmKibrnlFnfOBwAA4DFdOiPUr18/1dXVSZJ2796tyZMnS5IcDoe+++47900HAADgQV06IzRlyhQtXLhQN9xwg44ePaoZM2ZIko4cOaKoqCh3zgcAAOAxXTojtG7dOsXHx+tf//qXtm7dqpCQEElSUVGRfv3rX7t1QAAAAE+xWZZl+XoIb2poaJDT6VR9fb2Cg4N9PQ4AALgMnnr/7tIZoV27dmnfvn2u++vWrdP111+vOXPm6N///rfbhgMAAPCkLoXQkiVL1NDQIEk6fPiwHnnkEU2fPl3Hjx9XZmamWwcEAADwlC59WLq8vFxjxoyRJG3dulW33367nn76aX322WeaPn26WwcEAADwlC6dEQoKCtK5c+ckSR988IGSk5MlSQMGDHCdKQIAAOjuunRG6KabblJmZqYSExP16aefKi8vT5J09OhRhYeHu3VAAAAAT+nSGaG1a9cqMDBQW7ZsUW5uroYMGSJJeu+993Tbbbe5dUAAAABP4cfnAQBAt+ep9+8uXRqTpJaWFu3YsUNlZWWy2WyKiYnRL3/5SwUEBLhtOAAAAE/qUgh98cUXmj59uk6dOqXRo0fLsiwdPXpUERERevfddzV8+HB3zwkAAOB2XfqMUFpamoYPH67Kykp99tlnKi4uVkVFhaKjo5WWlubuGQEAADyiS2eE9uzZo48//lgDBgxwrYWEhOgPf/iDEhMT3TYcAACAJ3XpjJDdbldjY2O79TNnzigoKOgnDwUAAOANXQqh22+/Xffdd58++eQTWZYly7L08ccfKzU1VXfccYe7ZwQAAPCILoXQmjVrNHz4cMXHx8vhcMjhcCghIUEjRozQCy+84OYRAQAAPKNLnxHq37+//vrXv+qLL75QWVmZLMvSmDFjNGLECHfPBwAA4DGXHUI/9lvlP/roI9f/Xr16dZcHAgAA8JbLDqHi4uLL2s5ms3V5GAAAAG+67BD68MMPPTkHAACA13Xpw9IAAAD+gBACAADGIoQAAICxCCEAAGAsQggAABiLEAIAAMYihAAAgLEIIQAAYCxCCAAAGIsQAgAAxiKEAACAsQghAABgLEIIAAAYixACAADGIoQAAICxCCEAAGAsQggAABjL5yGUk5Oj6OhoORwOxcbGqrCw8LL2279/vwIDA3X99dd7dkAAAOC3fBpCeXl5Sk9P17Jly1RcXKykpCRNmzZNFRUVl9yvvr5ec+fO1S9+8QsvTQoAAPyRzbIsy1dPPnHiRI0fP165ubmutZiYGM2aNUvZ2dkX3e/uu+/WyJEjFRAQoB07dqikpOSyn7OhoUFOp1P19fUKDg7+KeMDAAAv8dT7t8/OCDU3N6uoqEjJyclt1pOTk3XgwIGL7vfGG2/oyy+/1PLlyy/reZqamtTQ0NDmBgAAIPkwhGpra9XS0qLQ0NA266Ghoaquru5wn2PHjmnp0qXatGmTAgMDL+t5srOz5XQ6XbeIiIifPDsAAPAPPv+wtM1ma3Pfsqx2a5LU0tKiOXPmaMWKFRo1atRlP35WVpbq6+tdt8rKyp88MwAA8A+Xd1rFAwYOHKiAgIB2Z39qamranSWSpMbGRh06dEjFxcVatGiRJKm1tVWWZSkwMFC7d+/WpEmT2u1nt9tlt9s98yIAAECP5rMzQkFBQYqNjVVBQUGb9YKCAiUkJLTbPjg4WIcPH1ZJSYnrlpqaqtGjR6ukpEQTJ0701ugAAMBP+OyMkCRlZmbqnnvuUVxcnOLj4/Xqq6+qoqJCqampkn64rHXq1Cm9+eab6tWrl8aOHdtm/6uvvloOh6PdOgAAwOXwaQilpKSorq5OK1euVFVVlcaOHav8/HxFRkZKkqqqqn70O4UAAAC6yqffI+QLfI8QAAA9j999jxAAAICvEUIAAMBYhBAAADAWIQQAAIxFCAEAAGMRQgAAwFiEEAAAMBYhBAAAjEUIAQAAYxFCAADAWIQQAAAwFiEEAACMRQgBAABjEUIAAMBYhBAAADAWIQQAAIxFCAEAAGMRQgAAwFiEEAAAMBYhBAAAjEUIAQAAYxFCAADAWIQQAAAwFiEEAACMRQgBAABjEUIAAMBYhBAAADAWIQQAAIxFCAEAAGMRQgAAwFiEEAAAMBYhBAAAjEUIAQAAYxFCAADAWIQQAAAwFiEEAACMRQgBAABjEUIAAMBYhBAAADAWIQQAAIxFCAEAAGMRQgAAwFiEEAAAMBYhBAAAjEUIAQAAYxFCAADAWIQQAAAwFiEEAACMRQgBAABjEUIAAMBYhBAAADAWIQQAAIxFCAEAAGMRQgAAwFiEEAAAMBYhBAAAjEUIAQAAYxFCAADAWIQQAAAwFiEEAACMRQgBAABjEUIAAMBYhBAAADCWz0MoJydH0dHRcjgcio2NVWFh4UW33bZtm6ZMmaKrrrpKwcHBio+P1/vvv+/FaQEAgD/xaQjl5eUpPT1dy5YtU3FxsZKSkjRt2jRVVFR0uP3evXs1ZcoU5efnq6ioSLfeeqtmzpyp4uJiL08OAAD8gc2yLMtXTz5x4kSNHz9eubm5rrWYmBjNmjVL2dnZl/UY1157rVJSUvTkk09e1vYNDQ1yOp2qr69XcHBwl+YGAADe5an3b5+dEWpublZRUZGSk5PbrCcnJ+vAgQOX9Ritra1qbGzUgAEDLrpNU1OTGhoa2twAAAAkH4ZQbW2tWlpaFBoa2mY9NDRU1dXVl/UYzz//vM6ePavZs2dfdJvs7Gw5nU7XLSIi4ifNDQAA/IfPPyxts9na3Lcsq91aR95++2099dRTysvL09VXX33R7bKyslRfX++6VVZW/uSZAQCAfwj01RMPHDhQAQEB7c7+1NTUtDtL9L/y8vK0YMECbd68WZMnT77ktna7XXa7/SfPCwAA/I/PzggFBQUpNjZWBQUFbdYLCgqUkJBw0f3efvttzZ8/X2+99ZZmzJjh6TEBAIAf89kZIUnKzMzUPffco7i4OMXHx+vVV19VRUWFUlNTJf1wWevUqVN68803Jf0QQXPnztWLL76oG2+80XU2qU+fPnI6nT57HQAAoGfyaQilpKSorq5OK1euVFVVlcaOHav8/HxFRkZKkqqqqtp8p9Arr7yi8+fP68EHH9SDDz7oWp83b542btzo7fEBAEAP59PvEfIFvkcIAICex+++RwgAAMDXCCEAAGAsQggAABiLEAIAAMYihAAAgLEIIQAAYCxCCAAAGIsQAgAAxiKEAACAsQghAABgLEIIAAAYixACAADGIoQAAICxCCEAAGAsQggAABiLEAIAAMYihAAAgLEIIQAAYCxCCAAAGIsQAgAAxiKEAACAsQghAABgLEIIAAAYixACAADGIoQAAICxCCEAAGAsQggAABiLEAIAAMYihAAAgLEIIQAAYCxCCAAAGIsQAgAAxiKEAACAsQghAABgLEIIAAAYixACAADGIoQAAICxCCEAAGAsQggAABiLEAIAAMYihAAAgLEIIQAAYCxCCAAAGIsQAgAAxiKEAACAsQghAABgLEIIAAAYixACAADGIoQAAICxCCEAAGAsQggAABiLEAIAAMYihAAAgLEIIQAAYCxCCAAAGIsQAgAAxiKEAACAsQghAABgLEIIAAAYixACAADGIoQAAICxCCEAAGAsQggAABjL5yGUk5Oj6OhoORwOxcbGqrCw8JLb79mzR7GxsXI4HBo2bJhefvllL00KAAD8jU9DKC8vT+np6Vq2bJmKi4uVlJSkadOmqaKiosPty8vLNX36dCUlJam4uFiPP/640tLStHXrVi9PDgAA/IHNsizLV08+ceJEjR8/Xrm5ua61mJgYzZo1S9nZ2e22f+yxx7Rz506VlZW51lJTU/X555/r4MGDl/WcDQ0Ncjqdqq+vV3Bw8E9/EQAAwOM89f4d6LZH6qTm5mYVFRVp6dKlbdaTk5N14MCBDvc5ePCgkpOT26xNnTpV69ev1/fff6/evXu326epqUlNTU2u+/X19ZJ++AcKAAB6hgvv2+4+f+OzEKqtrVVLS4tCQ0PbrIeGhqq6urrDfaqrqzvc/vz586qtrVVYWFi7fbKzs7VixYp26xERET9hegAA4At1dXVyOp1uezyfhdAFNputzX3Lstqt/dj2Ha1fkJWVpczMTNf9b7/9VpGRkaqoqHDrP0h0TUNDgyIiIlRZWcmlSh/jWHQfHIvug2PRfdTX12vo0KEaMGCAWx/XZyE0cOBABQQEtDv7U1NT0+6szwWDBg3qcPvAwECFhIR0uI/dbpfdbm+37nQ6+T91NxIcHMzx6CY4Ft0Hx6L74Fh0H716uffnvHz2U2NBQUGKjY1VQUFBm/WCggIlJCR0uE98fHy77Xfv3q24uLgOPx8EAABwKT798fnMzEy9/vrr2rBhg8rKypSRkaGKigqlpqZK+uGy1ty5c13bp6am6sSJE8rMzFRZWZk2bNig9evXa/Hixb56CQAAoAfz6WeEUlJSVFdXp5UrV6qqqkpjx45Vfn6+IiMjJUlVVVVtvlMoOjpa+fn5ysjI0Lp16zR48GCtWbNGd95552U/p91u1/Llyzu8XAbv43h0HxyL7oNj0X1wLLoPTx0Ln36PEAAAgC/5/FdsAAAA+AohBAAAjEUIAQAAYxFCAADAWH4ZQjk5OYqOjpbD4VBsbKwKCwsvuf2ePXsUGxsrh8OhYcOG6eWXX/bSpP6vM8di27ZtmjJliq666ioFBwcrPj5e77//vhen9X+d/btxwf79+xUYGKjrr7/eswMapLPHoqmpScuWLVNkZKTsdruGDx+uDRs2eGla/9bZY7Fp0yaNGzdOV1xxhcLCwnTvvfeqrq7OS9P6r71792rmzJkaPHiwbDabduzY8aP7uOX92/Iz77zzjtW7d2/rtddes0pLS62HH37Y6tu3r3XixIkOtz9+/Lh1xRVXWA8//LBVWlpqvfbaa1bv3r2tLVu2eHly/9PZY/Hwww9bzzzzjPXpp59aR48etbKysqzevXtbn332mZcn90+dPR4XfPvtt9awYcOs5ORka9y4cd4Z1s915Vjccccd1sSJE62CggKrvLzc+uSTT6z9+/d7cWr/1NljUVhYaPXq1ct68cUXrePHj1uFhYXWtddea82aNcvLk/uf/Px8a9myZdbWrVstSdb27dsvub273r/9LoQmTJhgpaamtlm75pprrKVLl3a4/aOPPmpdc801bdbuv/9+68Ybb/TYjKbo7LHoyJgxY6wVK1a4ezQjdfV4pKSkWL///e+t5cuXE0Ju0tlj8d5771lOp9Oqq6vzxnhG6eyxeO6556xhw4a1WVuzZo0VHh7usRlNdDkh5K73b7+6NNbc3KyioiIlJye3WU9OTtaBAwc63OfgwYPttp86daoOHTqk77//3mOz+ruuHIv/1draqsbGRrf/gj0TdfV4vPHGG/ryyy+1fPlyT49ojK4ci507dyouLk7PPvushgwZolGjRmnx4sX67rvvvDGy3+rKsUhISNDJkyeVn58vy7L0zTffaMuWLZoxY4Y3RsZ/cdf7t89/+7w71dbWqqWlpd0vbQ0NDW33y1ovqK6u7nD78+fPq7a2VmFhYR6b15915Vj8r+eff15nz57V7NmzPTGiUbpyPI4dO6alS5eqsLBQgYF+9a8Kn+rKsTh+/Lj27dsnh8Oh7du3q7a2Vg888IBOnz7N54R+gq4ci4SEBG3atEkpKSn6z3/+o/Pnz+uOO+7QSy+95I2R8V/c9f7tV2eELrDZbG3uW5bVbu3Htu9oHZ3X2WNxwdtvv62nnnpKeXl5uvrqqz01nnEu93i0tLRozpw5WrFihUaNGuWt8YzSmb8bra2tstls2rRpkyZMmKDp06dr9erV2rhxI2eF3KAzx6K0tFRpaWl68sknVVRUpF27dqm8vNz1OzLhXe54//ar/8wbOHCgAgIC2pV8TU1Nu2q8YNCgQR1uHxgYqJCQEI/N6u+6ciwuyMvL04IFC7R582ZNnjzZk2Mao7PHo7GxUYcOHVJxcbEWLVok6Yc3Y8uyFBgYqN27d2vSpElemd3fdOXvRlhYmIYMGSKn0+lai4mJkWVZOnnypEaOHOnRmf1VV45Fdna2EhMTtWTJEknSddddp759+yopKUmrVq3iKoIXuev926/OCAUFBSk2NlYFBQVt1gsKCpSQkNDhPvHx8e223717t+Li4tS7d2+PzervunIspB/OBM2fP19vvfUW19zdqLPHIzg4WIcPH1ZJSYnrlpqaqtGjR6ukpEQTJ0701uh+pyt/NxITE/X111/rzJkzrrWjR4+qV69eCg8P9+i8/qwrx+LcuXPq1avtW2dAQICk/z8bAe9w2/t3pz5a3QNc+FHI9evXW6WlpVZ6errVt29f66uvvrIsy7KWLl1q3XPPPa7tL/z4XUZGhlVaWmqtX7+eH593k84ei7feessKDAy01q1bZ1VVVblu3377ra9egl/p7PH4X/zUmPt09lg0NjZa4eHh1l133WUdOXLE2rNnjzVy5Ehr4cKFvnoJfqOzx+KNN96wAgMDrZycHOvLL7+09u3bZ8XFxVkTJkzw1UvwG42NjVZxcbFVXFxsSbJWr15tFRcXu77KwFPv334XQpZlWevWrbMiIyOtoKAga/z48daePXtcfzZv3jzr5ptvbrP9Rx99ZN1www1WUFCQFRUVZeXm5np5Yv/VmWNx8803W5La3ebNm+f9wf1UZ/9u/DdCyL06eyzKysqsyZMnW3369LHCw8OtzMxM69y5c16e2j919lisWbPGGjNmjNWnTx8rLCzM+s1vfmOdPHnSy1P7nw8//PCS7wGeev+2WRbn8gAAgJn86jNCAAAAnUEIAQAAYxFCAADAWIQQAAAwFiEEAACMRQgBAABjEUIAAMBYhBCAbumWW25Renq6r8cA4Of4QkUA3dLp06fVu3dvXXnllYqKilJ6ejphBMDt/Oq3zwPwHwMGDHD7YzY3NysoKMjtjwug5+LSGIBu6cKlsVtuuUUnTpxQRkaGbDabbDaba5sDBw7o5z//ufr06aOIiAilpaXp7Nmzrj+PiorSqlWrNH/+fDmdTv3ud79Tc3OzFi1apLCwMDkcDkVFRSk7O9sXLxFAN0AIAejWtm3bpvDwcK1cuVJVVVWqqqqSJB0+fFhTp07Vr371K/39739XXl6e9u3bp0WLFrXZ/7nnntPYsWNVVFSkJ554QmvWrNHOnTv1l7/8Rf/85z/15z//WVFRUT54ZQC6Ay6NAejWBgwYoICAAF155ZUaNGiQa/25557TnDlzXJ8bGjlypNasWaObb75Zubm5cjgckqRJkyZp8eLFrv0qKio0cuRI3XTTTbLZbIqMjPTq6wHQvXBGCECPVFRUpI0bN6pfv36u29SpU9Xa2qry8nLXdnFxcW32mz9/vkpKSjR69GilpaVp9+7d3h4dQDfCGSEAPVJra6vuv/9+paWltfuzoUOHuv5337592/zZ+PHjVV5ervfee08ffPCBZs+ercmTJ2vLli0enxlA90MIAej2goKC1NLS0mZt/PjxOnLkiEaMGNHpxwsODlZKSopSUlJ011136bbbbtPp06c98pNqALo3Lo0B6PaioqK0d+9enTp1SrW1tZKkxx57TAcPHtSDDz6okpISHTt2TDt37tRDDz10ycf64x//qHfeeUf/+Mc/dPToUW3evFmDBg1S//79vfBKAHQ3hBCAbm/lypX66quvNHz4cF111VWSpOuuu0579uzRsWPHlJSUpBtuuEFPPPGEwsLCLvlY/fr10zPPPKO4uDj97Gc/01dffaX8/Hz16sW/DgET8c3SAADAWPwnEAAAMBYhBAAAjEUIAQAAYxFCAADAWIQQAAAwFiEEAACMRQgBAABjEUIAAMBYhBAAADAWIQQAAIxFCAEAAGMRQgAAwFj/B4yfCkBIiT0CAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.xlabel('iters')\n",
    "plt.ylabel('loss')\n",
    "all_losses = [loss.item() for loss in all_losses]\n",
    "plt.plot(np.array(all_losses))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluation: A Sample of Generated Sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-15T03:10:52.267837Z",
     "start_time": "2019-05-15T03:10:51.986701Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'eval_step' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[27], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43meval_step\u001b[49m(model, predicted_len\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m600\u001b[39m))\n",
      "\u001b[0;31mNameError\u001b[0m: name 'eval_step' is not defined"
     ]
    }
   ],
   "source": [
    "print(eval_step(model, predicted_len=600))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
