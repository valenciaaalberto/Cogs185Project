{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-14T23:57:20.051203Z",
     "start_time": "2019-05-14T23:57:19.626384Z"
    }
   },
   "outputs": [],
   "source": [
    "import string\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Choose a Device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-14T23:57:20.129301Z",
     "start_time": "2019-05-14T23:57:20.081156Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "# If there are GPUs, choose the first one for computing. Otherwise use CPU.\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)  \n",
    "# If 'cuda:0' is printed, it means GPU is available."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Transformer Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        # Ensure that the model dimension (d_model) is divisible by the number of heads\n",
    "        assert d_model % num_heads == 0, \"d_model must be divisible by num_heads\"\n",
    "        \n",
    "        # Initialize dimensions\n",
    "        self.d_model = d_model # Model's dimension\n",
    "        self.num_heads = num_heads # Number of attention heads\n",
    "        self.d_k = d_model // num_heads # Dimension of each head's key, query, and value\n",
    "        \n",
    "        # Linear layers for transforming inputs\n",
    "        self.W_q = nn.Linear(d_model, d_model) # Query transformation\n",
    "        self.W_k = nn.Linear(d_model, d_model) # Key transformation\n",
    "        self.W_v = nn.Linear(d_model, d_model) # Value transformation\n",
    "        self.W_o = nn.Linear(d_model, d_model) # Output transformation\n",
    "        \n",
    "    def scaled_dot_product_attention(self, Q, K, V, mask=None):\n",
    "        # Calculate attention scores\n",
    "        attn_scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.d_k)\n",
    "        \n",
    "        # Apply mask if provided (useful for preventing attention to certain parts like padding)\n",
    "        if mask is not None:\n",
    "            attn_scores = attn_scores.masked_fill(mask == 0, -1e9)\n",
    "        \n",
    "        # Softmax is applied to obtain attention probabilities\n",
    "        attn_probs = torch.softmax(attn_scores, dim=-1)\n",
    "        \n",
    "        # Multiply by values to obtain the final output\n",
    "        output = torch.matmul(attn_probs, V)\n",
    "        return output\n",
    "        \n",
    "    def split_heads(self, x):\n",
    "        # Reshape the input to have num_heads for multi-head attention\n",
    "        batch_size, seq_length, d_model = x.size()\n",
    "        return x.view(batch_size, seq_length, self.num_heads, self.d_k).transpose(1, 2)\n",
    "        \n",
    "    def combine_heads(self, x):\n",
    "        # Combine the multiple heads back to original shape\n",
    "        batch_size, _, seq_length, d_k = x.size()\n",
    "        return x.transpose(1, 2).contiguous().view(batch_size, seq_length, self.d_model)\n",
    "        \n",
    "    def forward(self, Q, K, V, mask=None):\n",
    "        # Apply linear transformations and split heads\n",
    "        Q = self.split_heads(self.W_q(Q))\n",
    "        K = self.split_heads(self.W_k(K))\n",
    "        V = self.split_heads(self.W_v(V))\n",
    "        \n",
    "        # Perform scaled dot-product attention\n",
    "        attn_output = self.scaled_dot_product_attention(Q, K, V, mask)\n",
    "        \n",
    "        # Combine heads and apply output transformation\n",
    "        output = self.W_o(self.combine_heads(attn_output))\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_seq_length):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        \n",
    "        pe = torch.zeros(max_seq_length, d_model)\n",
    "        position = torch.arange(0, max_seq_length, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * -(math.log(10000.0) / d_model))\n",
    "        \n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        \n",
    "        self.register_buffer('pe', pe.unsqueeze(0))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return x + self.pe[:, :x.size(1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionWiseFeedForward(nn.Module):\n",
    "    def __init__(self, d_model, d_ff):\n",
    "        super(PositionWiseFeedForward, self).__init__()\n",
    "        self.fc1 = nn.Linear(d_model, d_ff)\n",
    "        self.fc2 = nn.Linear(d_ff, d_model)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, d_ff, dropout):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "        self.self_attn = MultiHeadAttention(d_model, num_heads)\n",
    "        self.cross_attn = MultiHeadAttention(d_model, num_heads)\n",
    "        self.feed_forward = PositionWiseFeedForward(d_model, d_ff)\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.norm3 = nn.LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x, enc_output, src_mask, tgt_mask):\n",
    "        attn_output = self.self_attn(x, x, x, tgt_mask)\n",
    "        x = self.norm1(x + self.dropout(attn_output))\n",
    "        ff_output = self.feed_forward(x)\n",
    "        x = self.norm3(x + self.dropout(ff_output))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(self, src_vocab_size, tgt_vocab_size, d_model, num_heads, num_layers, d_ff, max_seq_length, dropout):\n",
    "        super(Transformer, self).__init__()\n",
    "        self.encoder_embedding = nn.Embedding(src_vocab_size, d_model)\n",
    "        self.decoder_embedding = nn.Embedding(tgt_vocab_size, d_model)\n",
    "        self.positional_encoding = PositionalEncoding(d_model, max_seq_length)\n",
    "\n",
    "        self.decoder_layers = nn.ModuleList([DecoderLayer(d_model, num_heads, d_ff, dropout) for _ in range(num_layers)])\n",
    "\n",
    "        self.fc = nn.Linear(d_model, tgt_vocab_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def generate_mask(self, src, tgt):\n",
    "        src_mask = (src != 0).unsqueeze(1).unsqueeze(2)\n",
    "        tgt_mask = (tgt != 0).unsqueeze(1).unsqueeze(3)\n",
    "        seq_length = tgt.size(1)\n",
    "        nopeak_mask = (1 - torch.triu(torch.ones(1, seq_length, seq_length), diagonal=1)).bool().to(device)\n",
    "        tgt_mask = (tgt_mask & nopeak_mask).to(device)\n",
    "        return src_mask, tgt_mask\n",
    "\n",
    "\n",
    "    def forward(self, src, tgt):\n",
    "        src_mask, tgt_mask = self.generate_mask(src, tgt)\n",
    "        tgt_embedded = self.dropout(self.positional_encoding(self.decoder_embedding(tgt)))\n",
    "        \n",
    "        enc_output = None \n",
    "        dec_output = tgt_embedded\n",
    "        for dec_layer in self.decoder_layers:\n",
    "            dec_output = dec_layer(dec_output, enc_output, src_mask, tgt_mask)\n",
    "\n",
    "        output = self.fc(dec_output).to(device)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of file: 1115394\n",
      "All possible characters: 0123456789abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~ \t\n",
      "\r\n",
      "\u000b\n",
      "\f\n",
      "\n",
      "Number of all possible characters: 100\n"
     ]
    }
   ],
   "source": [
    "all_chars       = string.printable\n",
    "n_chars         = len(all_chars)\n",
    "file            = open('../Data/shakespeare.txt').read()\n",
    "file_len        = len(file)\n",
    "\n",
    "print('Length of file: {}'.format(file_len))\n",
    "print('All possible characters: {}'.format(all_chars))\n",
    "print('Number of all possible characters: {}'.format(n_chars))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a random sequence of the Shakespeare dataset.\n",
    "def get_random_seq():\n",
    "    seq_len     = 128  # The length of an input sequence.\n",
    "    start_index = random.randint(0, file_len - seq_len)\n",
    "    end_index   = start_index + seq_len + 1\n",
    "    return file[start_index:end_index]\n",
    "\n",
    "# Convert the sequence to index tensor.\n",
    "def seq_to_index(seq):\n",
    "    tensor = torch.zeros(len(seq), dtype = torch.long)\n",
    "    # Shape of the tensor: \n",
    "    #     (sequence length, batch size).\n",
    "    # Here we use batch size = 1.\n",
    "    for t, char in enumerate(seq):\n",
    "        tensor[t] = all_chars.index(char) + 1\n",
    "    return tensor\n",
    "\n",
    "# Sample a mini-batch including input tensor and target tensor.\n",
    "def get_input_and_target():\n",
    "    seq    = get_random_seq()\n",
    "    input  = seq_to_index(seq[:-1])      # Input is represented in one-hot.\n",
    "    target = seq_to_index(seq[1:]) # Target is represented in index.\n",
    "    return input, target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instantiate Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Transformer(\n",
       "  (encoder_embedding): Embedding(100, 128)\n",
       "  (decoder_embedding): Embedding(100, 128)\n",
       "  (positional_encoding): PositionalEncoding()\n",
       "  (decoder_layers): ModuleList(\n",
       "    (0-3): 4 x DecoderLayer(\n",
       "      (self_attn): MultiHeadAttention(\n",
       "        (W_q): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (W_k): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (W_v): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (W_o): Linear(in_features=128, out_features=128, bias=True)\n",
       "      )\n",
       "      (cross_attn): MultiHeadAttention(\n",
       "        (W_q): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (W_k): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (W_v): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (W_o): Linear(in_features=128, out_features=128, bias=True)\n",
       "      )\n",
       "      (feed_forward): PositionWiseFeedForward(\n",
       "        (fc1): Linear(in_features=128, out_features=512, bias=True)\n",
       "        (fc2): Linear(in_features=512, out_features=128, bias=True)\n",
       "        (relu): ReLU()\n",
       "      )\n",
       "      (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      (norm3): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (fc): Linear(in_features=128, out_features=100, bias=True)\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Instantiate the Transformer model\n",
    "d_model = 128\n",
    "num_heads = 4\n",
    "num_layers = 4\n",
    "d_ff = 512\n",
    "max_seq_length = 128\n",
    "dropout = 0.1\n",
    "\n",
    "model = Transformer(\n",
    "    src_vocab_size=100,\n",
    "    tgt_vocab_size=100,\n",
    "    d_model=d_model,\n",
    "    num_heads=num_heads,\n",
    "    num_layers=num_layers,\n",
    "    d_ff=d_ff,\n",
    "    max_seq_length=max_seq_length,\n",
    "    dropout=dropout\n",
    ")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(model,target,start_seq='W', gen_len=100,temperature=1.0):\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    \n",
    "    indices = \"\"\n",
    "    tensor = start_seq[0]\n",
    "    for char in tensor:\n",
    "        indices += all_chars[char-1] \n",
    "        \n",
    "    generated_sequence = \"\"\n",
    "        \n",
    "    input_seq = start_seq\n",
    "    \n",
    "    for _ in range(gen_len):\n",
    "        with torch.no_grad():\n",
    "            output = model(input_seq, target)\n",
    "        \n",
    "        # Get the last output and convert to probabilities\n",
    "        next_char_logits = output[-1, -1, :] # Shape: (output_dim)\n",
    "        next_char_probs = torch.softmax(next_char_logits, dim=-1)/temperature\n",
    "\n",
    "        predicted_char_index = torch.argmax(next_char_probs)\n",
    "        predicted_char = all_chars[predicted_char_index]\n",
    "        \n",
    "        indices += predicted_char\n",
    "        generated_sequence += predicted_char\n",
    "        \n",
    "        # Append the next character to the input sequence\n",
    "        next_char_tensor = torch.tensor([predicted_char_index], dtype=torch.long).unsqueeze(1).to(device)\n",
    "        input_seq = torch.cat([input_seq, next_char_tensor], dim=1)[:, 1:]\n",
    "    \n",
    "    return generated_sequence\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Procedure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter:199/20000 loss:2.8250939297676085\n",
      "generated sequence: \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\n",
      "\n",
      "iter:399/20000 loss:2.570035401582718\n",
      "generated sequence: eeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeee\n",
      "\n",
      "iter:599/20000 loss:2.522367714643478\n",
      "generated sequence: ffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffff\n",
      "\n",
      "iter:799/20000 loss:2.471663833856583\n",
      "generated sequence: uuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuu\n",
      "\n",
      "iter:999/20000 loss:2.435884931087494\n",
      "generated sequence: UUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUU\n",
      "\n",
      "iter:1199/20000 loss:2.357465410232544\n",
      "generated sequence: \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\n",
      "\n",
      "iter:1399/20000 loss:2.3037016904354095\n",
      "generated sequence: uuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuu\n",
      "\n",
      "iter:1599/20000 loss:2.2496965777873994\n",
      "generated sequence: uuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuu\n",
      "\n",
      "iter:1799/20000 loss:2.2025495094060896\n",
      "generated sequence: \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\n",
      "\n",
      "iter:1999/20000 loss:2.160917032957077\n",
      "generated sequence: \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\n",
      "\n",
      "iter:2199/20000 loss:2.134833873510361\n",
      "generated sequence: \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\n",
      "\n",
      "iter:2399/20000 loss:2.0885507810115813\n",
      "generated sequence: zzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzz\n",
      "\n",
      "iter:2599/20000 loss:2.071515390872955\n",
      "generated sequence: TTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTT\n",
      "\n",
      "iter:2799/20000 loss:2.047014216184616\n",
      "generated sequence: uuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuu\n",
      "\n",
      "iter:2999/20000 loss:1.9931949579715729\n",
      "generated sequence: uuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuu\n",
      "\n",
      "iter:3199/20000 loss:1.9904568958282471\n",
      "generated sequence: uuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuu\n",
      "\n",
      "iter:3399/20000 loss:1.9599273014068603\n",
      "generated sequence: ssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssss\n",
      "\n",
      "iter:3599/20000 loss:1.9625926786661148\n",
      "generated sequence: ffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffff\n",
      "\n",
      "iter:3799/20000 loss:1.9293261682987213\n",
      "generated sequence: gggggggggggggggggggggggggggggggggggggggggggggggggggggggggggggggggggggggggggggggggggggggggggggggggggg\n",
      "\n",
      "iter:3999/20000 loss:1.917620887160301\n",
      "generated sequence: pppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppp\n",
      "\n",
      "iter:4199/20000 loss:1.893572629094124\n",
      "generated sequence: pppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppp\n",
      "\n",
      "iter:4399/20000 loss:1.8844165527820587\n",
      "generated sequence: ssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssss\n",
      "\n",
      "iter:4599/20000 loss:1.8718185591697694\n",
      "generated sequence: oooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooo\n",
      "\n",
      "iter:4799/20000 loss:1.853315421938896\n",
      "generated sequence: oooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooo\n",
      "\n",
      "iter:4999/20000 loss:1.8479077076911927\n",
      "generated sequence: ffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffff\n",
      "\n",
      "iter:5199/20000 loss:1.8224485182762147\n",
      "generated sequence: pppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppp\n",
      "\n",
      "iter:5399/20000 loss:1.848618295788765\n",
      "generated sequence: nnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnn\n",
      "\n",
      "iter:5599/20000 loss:1.8233487939834594\n",
      "generated sequence: pppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppp\n",
      "\n",
      "iter:5799/20000 loss:1.7868636107444764\n",
      "generated sequence: vvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvv\n",
      "\n",
      "iter:5999/20000 loss:1.7884524065256118\n",
      "generated sequence: uuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuu\n",
      "\n",
      "iter:6199/20000 loss:1.7977842903137207\n",
      "generated sequence: \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\n",
      "\n",
      "iter:6399/20000 loss:1.76207484126091\n",
      "generated sequence: vvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvv\n",
      "\n",
      "iter:6599/20000 loss:1.7648102343082428\n",
      "generated sequence: jjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjj\n",
      "\n",
      "iter:6799/20000 loss:1.7680108565092088\n",
      "generated sequence: \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\n",
      "\n",
      "iter:6999/20000 loss:1.7375832587480544\n",
      "generated sequence: iiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiii\n",
      "\n",
      "iter:7199/20000 loss:1.756797684431076\n",
      "generated sequence: pppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppp\n",
      "\n",
      "iter:7399/20000 loss:1.761580729484558\n",
      "generated sequence: bbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbb\n",
      "\n",
      "iter:7599/20000 loss:1.7350867825746537\n",
      "generated sequence: ffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffff\n",
      "\n",
      "iter:7799/20000 loss:1.7088554817438126\n",
      "generated sequence: iiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiii\n",
      "\n",
      "iter:7999/20000 loss:1.7229955512285233\n",
      "generated sequence: \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\n",
      "\n",
      "iter:8199/20000 loss:1.7403204947710038\n",
      "generated sequence: EEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEE\n",
      "\n",
      "iter:8399/20000 loss:1.7159024453163148\n",
      "generated sequence: uuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuu\n",
      "\n",
      "iter:8599/20000 loss:1.726181188225746\n",
      "generated sequence: \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\n",
      "\n",
      "iter:8799/20000 loss:1.7004891550540924\n",
      "generated sequence: SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS\n",
      "\n",
      "iter:8999/20000 loss:1.702993467450142\n",
      "generated sequence: iiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiii\n",
      "\n",
      "iter:9199/20000 loss:1.6916182482242583\n",
      "generated sequence: iiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiii\n",
      "\n",
      "iter:9399/20000 loss:1.6966943114995956\n",
      "generated sequence: tttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttt\n",
      "\n",
      "iter:9599/20000 loss:1.6985242748260498\n",
      "generated sequence: iiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiii\n",
      "\n",
      "iter:9799/20000 loss:1.698126568198204\n",
      "generated sequence: uuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuu\n",
      "\n",
      "iter:9999/20000 loss:1.7040248996019363\n",
      "generated sequence: vvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvv\n",
      "\n",
      "iter:10199/20000 loss:1.690545946955681\n",
      "generated sequence: bbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbb\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter:10399/20000 loss:1.6774147486686706\n",
      "generated sequence: uuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuu\n",
      "\n",
      "iter:10599/20000 loss:1.6827391344308853\n",
      "generated sequence: iiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiii\n",
      "\n",
      "iter:10799/20000 loss:1.6855670702457428\n",
      "generated sequence: \n",
      "\n",
      "iter:10999/20000 loss:1.690186800956726\n",
      "generated sequence: ffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffff\n",
      "\n",
      "iter:11199/20000 loss:1.66389055788517\n",
      "generated sequence: ;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;\n",
      "\n",
      "iter:11399/20000 loss:1.6760028260946274\n",
      "generated sequence: \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\n",
      "\n",
      "iter:11599/20000 loss:1.6508042618632317\n",
      "generated sequence: pppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppp\n",
      "\n",
      "iter:11799/20000 loss:1.6672725701332092\n",
      "generated sequence: pppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppp\n",
      "\n",
      "iter:11999/20000 loss:1.6685615187883378\n",
      "generated sequence: tttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttt\n",
      "\n",
      "iter:12199/20000 loss:1.6502217972278594\n",
      "generated sequence: NNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNN\n",
      "\n",
      "iter:12399/20000 loss:1.6363075804710387\n",
      "generated sequence: \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\n",
      "\n",
      "iter:12599/20000 loss:1.6456912630796432\n",
      "generated sequence: tttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttt\n",
      "\n",
      "iter:12799/20000 loss:1.6220038795471192\n",
      "generated sequence: tttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttt\n",
      "\n",
      "iter:12999/20000 loss:1.654689672589302\n",
      "generated sequence: oooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooo\n",
      "\n",
      "iter:13199/20000 loss:1.6328583407402038\n",
      "generated sequence: vvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvv\n",
      "\n",
      "iter:13399/20000 loss:1.6374924033880234\n",
      "generated sequence: iiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiii\n",
      "\n",
      "iter:13599/20000 loss:1.6350907617807389\n",
      "generated sequence: ffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffff\n",
      "\n",
      "iter:13799/20000 loss:1.6161064153909683\n",
      "generated sequence: NNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNN\n",
      "\n",
      "iter:13999/20000 loss:1.6217937821149826\n",
      "generated sequence: oooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooo\n",
      "\n",
      "iter:14199/20000 loss:1.6308744388818741\n",
      "generated sequence: pppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppp\n",
      "\n",
      "iter:14399/20000 loss:1.631428104043007\n",
      "generated sequence: eeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeee\n",
      "\n",
      "iter:14599/20000 loss:1.6506892102956772\n",
      "generated sequence: \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\n",
      "\n",
      "iter:14799/20000 loss:1.6129358464479446\n",
      "generated sequence: \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\n",
      "\n",
      "iter:14999/20000 loss:1.5953204673528671\n",
      "generated sequence: \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\n",
      "\n",
      "iter:15199/20000 loss:1.6220343911647797\n",
      "generated sequence: \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\n",
      "\n",
      "iter:15399/20000 loss:1.6203377652168274\n",
      "generated sequence: mmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmm\n",
      "\n",
      "iter:15599/20000 loss:1.6112848287820816\n",
      "generated sequence: mmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmm\n",
      "\n",
      "iter:15799/20000 loss:1.6143446797132492\n",
      "generated sequence: \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\n",
      "\n",
      "iter:15999/20000 loss:1.6026350271701812\n",
      "generated sequence: \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\n",
      "\n",
      "iter:16199/20000 loss:1.5954755938053131\n",
      "generated sequence: zzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzz\n",
      "\n",
      "iter:16399/20000 loss:1.6195346307754517\n",
      "generated sequence: uuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuu\n",
      "\n",
      "iter:16599/20000 loss:1.5967477369308472\n",
      "generated sequence: \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\n",
      "\n",
      "iter:16799/20000 loss:1.5928553754091264\n",
      "generated sequence: bbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbb\n",
      "\n",
      "iter:16999/20000 loss:1.5952469664812088\n",
      "generated sequence: oooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooo\n",
      "\n",
      "iter:17199/20000 loss:1.5863433641195297\n",
      "generated sequence: \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\n",
      "\n",
      "iter:17399/20000 loss:1.596849246621132\n",
      "generated sequence: oooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooo\n",
      "\n",
      "iter:17599/20000 loss:1.6129317849874496\n",
      "generated sequence: UUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUU\n",
      "\n",
      "iter:17799/20000 loss:1.5716044700145722\n",
      "generated sequence: \n",
      "\n",
      "iter:17999/20000 loss:1.6095440632104874\n",
      "generated sequence: oooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooo\n",
      "\n",
      "iter:18199/20000 loss:1.5955689519643783\n",
      "generated sequence: JJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJ\n",
      "\n",
      "iter:18399/20000 loss:1.5733877784013748\n",
      "generated sequence: uuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuu\n",
      "\n",
      "iter:18599/20000 loss:1.5830076551437378\n",
      "generated sequence: bbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbb\n",
      "\n",
      "iter:18799/20000 loss:1.5729611575603486\n",
      "generated sequence: gggggggggggggggggggggggggggggggggggggggggggggggggggggggggggggggggggggggggggggggggggggggggggggggggggg\n",
      "\n",
      "iter:18999/20000 loss:1.6046821403503417\n",
      "generated sequence: mmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmm\n",
      "\n",
      "iter:19199/20000 loss:1.6023407304286956\n",
      "generated sequence: ;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;\n",
      "\n",
      "iter:19399/20000 loss:1.594802888929844\n",
      "generated sequence: \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\n",
      "\n",
      "iter:19599/20000 loss:1.5810270559787751\n",
      "generated sequence: \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\n",
      "\n",
      "iter:19799/20000 loss:1.593421677350998\n",
      "generated sequence: \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\n",
      "\n",
      "iter:19999/20000 loss:1.5725024661421776\n",
      "generated sequence: zzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzz\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Number of iterations.\n",
    "iters       = 20000\n",
    "# Number of printing iterations\n",
    "print_iters = 200   \n",
    "\n",
    "# The loss variables.\n",
    "all_losses = []\n",
    "loss_sum   = 0\n",
    "\n",
    "# Initialize the optimizer and the loss function.\n",
    "opt       = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "# opt       = torch.optim.Adam(model.parameters(), lr=0.002)\n",
    "loss_func = nn.CrossEntropyLoss()\n",
    "\n",
    "# Training procedure.\n",
    "for i in range(iters):\n",
    "    opt.zero_grad()\n",
    "    input, target = get_input_and_target()            # Fetch input and target.\n",
    "    input, target = input.to(device), target.to(device) # Move to GPU memory.\n",
    "    input = input[None] \n",
    "    target = target[None] \n",
    "    predicted = model(input,input)\n",
    "    predicted = predicted.permute(0,2,1)\n",
    "    loss = loss_func(predicted,target)\n",
    "    \n",
    "    loss.backward()\n",
    "    loss_sum += loss.item()                                # Accumulate the loss.\n",
    "    opt.step()\n",
    "\n",
    "    # Print the log.\n",
    "    if i % print_iters == print_iters - 1:\n",
    "        print('iter:{}/{} loss:{}'.format(i, iters, loss_sum / print_iters))\n",
    "        print('generated sequence: {}\\n'.format(generate_text(model,input,start_seq = input)))\n",
    "              \n",
    "        # Track the loss.\n",
    "        all_losses.append(loss_sum / print_iters)\n",
    "        loss_sum = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training Loss Curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-15T00:38:13.728474Z",
     "start_time": "2019-05-15T00:38:13.559531Z"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.xlabel('iters')\n",
    "plt.ylabel('loss')\n",
    "plt.plot(np.array(all_losses))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluation: A Sample of Generated Sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-15T03:10:52.267837Z",
     "start_time": "2019-05-15T03:10:51.986701Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'vvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvv'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rand_input, rand_target = get_input_and_target()  \n",
    "rand_input = rand_input[None].to(device)\n",
    "rand_target = rand_target[None].to(device)\n",
    "generate_text(model,rand_target,start_seq = rand_input,gen_len=600)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis\n",
    "\n",
    "Since the baseline and updated baseline have some information leakage, this was created to address those problems by creating a decoder only achitecture. Moreover, the generate text function was fixed to only output predicted results and not any input."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.1.-1"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
