{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-14T23:57:20.051203Z",
     "start_time": "2019-05-14T23:57:19.626384Z"
    }
   },
   "outputs": [],
   "source": [
    "import string\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Choose a Device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-14T23:57:20.129301Z",
     "start_time": "2019-05-14T23:57:20.081156Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "# If there are GPUs, choose the first one for computing. Otherwise use CPU.\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)  \n",
    "# If 'cuda:0' is printed, it means GPU is available."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Transformer Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        # Ensure that the model dimension (d_model) is divisible by the number of heads\n",
    "        assert d_model % num_heads == 0, \"d_model must be divisible by num_heads\"\n",
    "        \n",
    "        # Initialize dimensions\n",
    "        self.d_model = d_model # Model's dimension\n",
    "        self.num_heads = num_heads # Number of attention heads\n",
    "        self.d_k = d_model // num_heads # Dimension of each head's key, query, and value\n",
    "        \n",
    "        # Linear layers for transforming inputs\n",
    "        self.W_q = nn.Linear(d_model, d_model) # Query transformation\n",
    "        self.W_k = nn.Linear(d_model, d_model) # Key transformation\n",
    "        self.W_v = nn.Linear(d_model, d_model) # Value transformation\n",
    "        self.W_o = nn.Linear(d_model, d_model) # Output transformation\n",
    "        \n",
    "    def scaled_dot_product_attention(self, Q, K, V, mask=None):\n",
    "        # Calculate attention scores\n",
    "        attn_scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.d_k)\n",
    "        \n",
    "        # Apply mask if provided (useful for preventing attention to certain parts like padding)\n",
    "        if mask is not None:\n",
    "            attn_scores = attn_scores.masked_fill(mask == 0, -1e9)\n",
    "        \n",
    "        # Softmax is applied to obtain attention probabilities\n",
    "        attn_probs = torch.softmax(attn_scores, dim=-1)\n",
    "        \n",
    "        # Multiply by values to obtain the final output\n",
    "        output = torch.matmul(attn_probs, V)\n",
    "        return output\n",
    "        \n",
    "    def split_heads(self, x):\n",
    "        # Reshape the input to have num_heads for multi-head attention\n",
    "        batch_size, seq_length, d_model = x.size()\n",
    "        return x.view(batch_size, seq_length, self.num_heads, self.d_k).transpose(1, 2)\n",
    "        \n",
    "    def combine_heads(self, x):\n",
    "        # Combine the multiple heads back to original shape\n",
    "        batch_size, _, seq_length, d_k = x.size()\n",
    "        return x.transpose(1, 2).contiguous().view(batch_size, seq_length, self.d_model)\n",
    "        \n",
    "    def forward(self, Q, K, V, mask=None):\n",
    "        # Apply linear transformations and split heads\n",
    "        Q = self.split_heads(self.W_q(Q))\n",
    "        K = self.split_heads(self.W_k(K))\n",
    "        V = self.split_heads(self.W_v(V))\n",
    "        \n",
    "        # Perform scaled dot-product attention\n",
    "        attn_output = self.scaled_dot_product_attention(Q, K, V, mask)\n",
    "        \n",
    "        # Combine heads and apply output transformation\n",
    "        output = self.W_o(self.combine_heads(attn_output))\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_seq_length):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        \n",
    "        pe = torch.zeros(max_seq_length, d_model)\n",
    "        position = torch.arange(0, max_seq_length, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * -(math.log(10000.0) / d_model))\n",
    "        \n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        \n",
    "        self.register_buffer('pe', pe.unsqueeze(0))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return x + self.pe[:, :x.size(1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionWiseFeedForward(nn.Module):\n",
    "    def __init__(self, d_model, d_ff):\n",
    "        super(PositionWiseFeedForward, self).__init__()\n",
    "        self.fc1 = nn.Linear(d_model, d_ff)\n",
    "        self.fc2 = nn.Linear(d_ff, d_model)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, d_ff, dropout):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "        self.self_attn = MultiHeadAttention(d_model, num_heads)\n",
    "        self.feed_forward = PositionWiseFeedForward(d_model, d_ff)\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x, mask):\n",
    "        attn_output = self.self_attn(x, x, x, mask)\n",
    "        x = self.norm1(x + self.dropout(attn_output))\n",
    "        ff_output = self.feed_forward(x)\n",
    "        x = self.norm2(x + self.dropout(ff_output))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, d_ff, dropout):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "        self.self_attn = MultiHeadAttention(d_model, num_heads)\n",
    "        self.cross_attn = MultiHeadAttention(d_model, num_heads)\n",
    "        self.feed_forward = PositionWiseFeedForward(d_model, d_ff)\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.norm3 = nn.LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x, enc_output, src_mask, tgt_mask):\n",
    "        attn_output = self.self_attn(x, x, x, tgt_mask)\n",
    "        x = self.norm1(x + self.dropout(attn_output))\n",
    "        attn_output = self.cross_attn(x, enc_output, enc_output, src_mask)\n",
    "        x = self.norm2(x + self.dropout(attn_output))\n",
    "        ff_output = self.feed_forward(x)\n",
    "        x = self.norm3(x + self.dropout(ff_output))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(self, src_vocab_size, tgt_vocab_size, d_model, num_heads, num_layers, d_ff, max_seq_length, dropout):\n",
    "        super(Transformer, self).__init__()\n",
    "        self.encoder_embedding = nn.Embedding(src_vocab_size, d_model)\n",
    "        self.decoder_embedding = nn.Embedding(tgt_vocab_size, d_model)\n",
    "        self.positional_encoding = PositionalEncoding(d_model, max_seq_length)\n",
    "\n",
    "        self.encoder_layers = nn.ModuleList([EncoderLayer(d_model, num_heads, d_ff, dropout) for _ in range(num_layers)])\n",
    "        self.decoder_layers = nn.ModuleList([DecoderLayer(d_model, num_heads, d_ff, dropout) for _ in range(num_layers)])\n",
    "\n",
    "        self.fc = nn.Linear(d_model, tgt_vocab_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def generate_mask(self, src, tgt):\n",
    "        src_mask = (src != 0).unsqueeze(1).unsqueeze(2)\n",
    "        tgt_mask = (tgt != 0).unsqueeze(1).unsqueeze(3)\n",
    "        seq_length = tgt.size(1)\n",
    "        nopeak_mask = (1 - torch.triu(torch.ones(1, seq_length, seq_length), diagonal=1)).bool().to(device)\n",
    "        tgt_mask = (tgt_mask & nopeak_mask).to(device)\n",
    "        return src_mask, tgt_mask\n",
    "\n",
    "\n",
    "    def forward(self, src, tgt):\n",
    "        src_mask, tgt_mask = self.generate_mask(src, tgt)\n",
    "        src_embedded = self.dropout(self.positional_encoding(self.encoder_embedding(src)))\n",
    "        tgt_embedded = self.dropout(self.positional_encoding(self.decoder_embedding(tgt)))\n",
    "\n",
    "        enc_output = src_embedded\n",
    "        for enc_layer in self.encoder_layers:\n",
    "            enc_output = enc_layer(enc_output, src_mask)\n",
    "    \n",
    "        dec_output = src_embedded\n",
    "        for dec_layer in self.decoder_layers:\n",
    "            dec_output = dec_layer(dec_output, enc_output, src_mask, tgt_mask)\n",
    "\n",
    "        output = self.fc(dec_output).to(device)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of file: 1115394\n",
      "All possible characters: 0123456789abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~ \t\n",
      "\u000b\f\n",
      "Number of all possible characters: 100\n"
     ]
    }
   ],
   "source": [
    "all_chars       = string.printable\n",
    "n_chars         = len(all_chars)\n",
    "file            = open('shakespeare.txt').read()\n",
    "file_len        = len(file)\n",
    "\n",
    "print('Length of file: {}'.format(file_len))\n",
    "print('All possible characters: {}'.format(all_chars))\n",
    "print('Number of all possible characters: {}'.format(n_chars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a random sequence of the Shakespeare dataset.\n",
    "def get_random_seq():\n",
    "    seq_len     = 128  # The length of an input sequence.\n",
    "    start_index = random.randint(0, file_len - seq_len)\n",
    "    end_index   = start_index + seq_len + 1\n",
    "    return file[start_index:end_index]\n",
    "\n",
    "# Convert the sequence to index tensor.\n",
    "def seq_to_index(seq):\n",
    "    tensor = torch.zeros(len(seq), dtype = torch.long)\n",
    "    # Shape of the tensor: \n",
    "    #     (sequence length, batch size).\n",
    "    # Here we use batch size = 1.\n",
    "    for t, char in enumerate(seq):\n",
    "        tensor[t] = all_chars.index(char) + 1\n",
    "    return tensor\n",
    "\n",
    "# Sample a mini-batch including input tensor and target tensor.\n",
    "def get_input_and_target():\n",
    "    seq    = get_random_seq()\n",
    "    input  = seq_to_index(seq[:-1])      # Input is represented in one-hot.\n",
    "    target = seq_to_index(seq[1:]) # Target is represented in index.\n",
    "    return input, target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instantiate Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Transformer(\n",
       "  (encoder_embedding): Embedding(100, 128)\n",
       "  (decoder_embedding): Embedding(100, 128)\n",
       "  (positional_encoding): PositionalEncoding()\n",
       "  (encoder_layers): ModuleList(\n",
       "    (0-3): 4 x EncoderLayer(\n",
       "      (self_attn): MultiHeadAttention(\n",
       "        (W_q): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (W_k): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (W_v): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (W_o): Linear(in_features=128, out_features=128, bias=True)\n",
       "      )\n",
       "      (feed_forward): PositionWiseFeedForward(\n",
       "        (fc1): Linear(in_features=128, out_features=512, bias=True)\n",
       "        (fc2): Linear(in_features=512, out_features=128, bias=True)\n",
       "        (relu): ReLU()\n",
       "      )\n",
       "      (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (decoder_layers): ModuleList(\n",
       "    (0-3): 4 x DecoderLayer(\n",
       "      (self_attn): MultiHeadAttention(\n",
       "        (W_q): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (W_k): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (W_v): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (W_o): Linear(in_features=128, out_features=128, bias=True)\n",
       "      )\n",
       "      (cross_attn): MultiHeadAttention(\n",
       "        (W_q): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (W_k): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (W_v): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (W_o): Linear(in_features=128, out_features=128, bias=True)\n",
       "      )\n",
       "      (feed_forward): PositionWiseFeedForward(\n",
       "        (fc1): Linear(in_features=128, out_features=512, bias=True)\n",
       "        (fc2): Linear(in_features=512, out_features=128, bias=True)\n",
       "        (relu): ReLU()\n",
       "      )\n",
       "      (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      (norm3): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (fc): Linear(in_features=128, out_features=100, bias=True)\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Instantiate the Transformer model\n",
    "d_model = 128\n",
    "num_heads = 4\n",
    "num_layers = 4\n",
    "d_ff = 512\n",
    "# d_ff = 2048\n",
    "# d_ff = 128\n",
    "max_seq_length = 128\n",
    "dropout = 0.1\n",
    "\n",
    "model = Transformer(\n",
    "    src_vocab_size=100,\n",
    "    tgt_vocab_size=100,\n",
    "    d_model=d_model,\n",
    "    num_heads=num_heads,\n",
    "    num_layers=num_layers,\n",
    "    d_ff=d_ff,\n",
    "    max_seq_length=max_seq_length,\n",
    "    dropout=dropout\n",
    ")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(model,target,start_seq='Wha', gen_len=10,temperature=1.0):\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    \n",
    "    indices = \"\"\n",
    "    tensor = start_seq[0]\n",
    "    for char in tensor:\n",
    "        indices += all_chars[char-1] \n",
    "        \n",
    "    input_seq = start_seq\n",
    "    \n",
    "    for _ in range(gen_len):\n",
    "        with torch.no_grad():\n",
    "            output = model(input_seq, target)\n",
    "        \n",
    "        # Get the last output and convert to probabilities\n",
    "        next_char_logits = output[-1, 0, :] # Shape: (output_dim)\n",
    "        next_char_probs = torch.softmax(next_char_logits, dim=-1)/temperature\n",
    "\n",
    "        predicted_char_index = torch.argmax(next_char_probs)\n",
    "        predicted_char = all_chars[predicted_char_index]\n",
    "        \n",
    "        indices += predicted_char\n",
    "        \n",
    "        # Append the next character to the input sequence\n",
    "        next_char_tensor = torch.tensor([predicted_char_index], dtype=torch.long).unsqueeze(1).to(device)\n",
    "        input_seq = torch.cat([input_seq, next_char_tensor], dim=1)[:, 1:]\n",
    "    \n",
    "    return indices\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Procedure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter:199/20000 loss:4.107639816999435\n",
      "generated sequence: e.\n",
      "I am too old to fawn upon a nurse,\n",
      "Too far in years to be a pupil now:\n",
      "What is thy sentence then but speechless death,\n",
      "Which \t\t\t\t\t\t\t\tu\t\n",
      "\n",
      "iter:399/20000 loss:3.644861087799072\n",
      "generated sequence: ell before a boisterous storm.\n",
      "But leave it all to God. whither away?\n",
      "\n",
      "Second Citizen:\n",
      "Marry, we were sent for to the justices.\n",
      "\t\t\t\tf\t\t\t\t\t\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Number of iterations.\n",
    "iters       = 20000\n",
    "# Number of printing iterations\n",
    "print_iters = 200   \n",
    "\n",
    "# The loss variables.\n",
    "all_losses = []\n",
    "loss_sum   = 0\n",
    "\n",
    "# Initialize the optimizer and the loss function.\n",
    "opt       = torch.optim.SGD(model.parameters(), lr=0.001)\n",
    "loss_func = nn.CrossEntropyLoss()\n",
    "\n",
    "# Training procedure.\n",
    "for i in range(iters):\n",
    "    opt.zero_grad()\n",
    "    input, target = get_input_and_target()            # Fetch input and target.\n",
    "    input, target = input.to(device), target.to(device) # Move to GPU memory.\n",
    "    input = input[None] \n",
    "    target = target[None] \n",
    "    predicted = model(input,input)\n",
    "    predicted = predicted.permute(0,2,1)\n",
    "    loss = loss_func(predicted,target)\n",
    "    \n",
    "    loss.backward()\n",
    "    loss_sum += loss.item()                                # Accumulate the loss.\n",
    "    opt.step()\n",
    "\n",
    "    # Print the log.\n",
    "    if i % print_iters == print_iters - 1:\n",
    "        print('iter:{}/{} loss:{}'.format(i, iters, loss_sum / print_iters))\n",
    "        print('generated sequence: {}\\n'.format(generate_text(model,target,start_seq = input)))\n",
    "              \n",
    "        # Track the loss.\n",
    "        all_losses.append(loss_sum / print_iters)\n",
    "        loss_sum = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training Loss Curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-15T00:38:13.728474Z",
     "start_time": "2019-05-15T00:38:13.559531Z"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEGCAYAAABo25JHAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjQuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8rg+JYAAAACXBIWXMAAAsTAAALEwEAmpwYAAAq1UlEQVR4nO3dd3yV5f3/8dcnexASCAkjJIS9ZUU2gnVP9OvCPUuttFVLf1VbrbXDDq3V1lJFsXUgrQoK1l2FKiIgYcgIGxISRsJISAhkXr8/zhEhJBCEw0lyv5+PRx6ecZ1zPhfE8+a+rvu+LnPOISIi3hUS7AJERCS4FAQiIh6nIBAR8TgFgYiIxykIREQ8LizYBRyvVq1aufT09GCXISLSqGRmZu50ziXV9lyjC4L09HQWLVoU7DJERBoVM8uu6zkNDYmIeJyCQETE4xQEIiIepyAQEfE4BYGIiMcpCEREPE5BICLicZ4JgjXbi3n8gzXs2Vce7FJERBoUzwTBpp0lPD17PduKDgS7FBGRBsUzQRAfHQFA4X4dEYiIHMozQZAQEw5AUWlFkCsREWlYPBME8dH+INivIBAROZRnguDrI4JCBYGIyGE8EwTR4aFEhIZQqKEhEZHDeCYIzIzm0eEaGhIRqSFgQWBmqWY228yyzGylmd1dS5t4M3vbzJb529waqHrANzxUpLOGREQOE8iNaSqBic65xWYWB2Sa2UfOuVWHtJkArHLOXWJmScAaM5vqnAvIt3VCdLiGhkREagjYEYFzbptzbrH/djGQBaTUbAbEmZkBzYDd+AIkIOI1NCQicoRTMkdgZunAAGBBjaeeBnoCW4HlwN3OuepaXj/ezBaZ2aKCgoJvXUd8jI4IRERqCngQmFkzYDpwj3Nub42nzwOWAu2A/sDTZta85ns45yY75zKccxlJSbXuvVwvCdEROiIQEakhoEFgZuH4QmCqc25GLU1uBWY4n/XAJqBHoOqJjw6npKySiqojDjpERDwrkGcNGTAFyHLOPVFHsxzgLH/71kB3YGOgavr6orK9OioQETkokGcNjQBuBJab2VL/Yz8D0gCcc88Avwb+aWbLAQPuc87tDFRBh15dnNgsMlAfIyLSqAQsCJxzc/F9uR+tzVbg3EDVUJPWGxIROZJnriyGQ4JAZw6JiBzkqSBIiNGeBCIiNXkrCHREICJyBE8FQfNoLUUtIlKTp4IgNMSIiwrT1cUiIofwVBCA7xRSXUcgIvINzwVBfHS4hoZERA7huSBIiI6gsFRnDYmIfM1zQRAfo6WoRUQO5b0g0J4EIiKH8VwQfL1LmXMu2KWIiDQI3guCmHAqqx2l5VXBLkVEpEHwXhBEf73MhIaHRETAg0Fw8OpinTkkIgJ4MAi+3pNAE8YiIj7eDQItMyEiAngwCOK18JyIyGE8FwRfTxZraEhExMdzQRAVHkJEWIhWIBUR8fNcEJiZ/+pinTUkIgIeDALwXV2soSERER9vBkFMuIaGRET8AhYEZpZqZrPNLMvMVprZ3XW0G2NmS/1t/heoeg4VH60gEBH5WlgA37sSmOicW2xmcUCmmX3knFv1dQMzSwAmAec753LMLDmA9RwUHx1B1rbiU/FRIiINXsCOCJxz25xzi/23i4EsIKVGs+uAGc65HH+7/EDVc6gE7UkgInLQKZkjMLN0YACwoMZT3YAWZjbHzDLN7KY6Xj/ezBaZ2aKCgoITric+OpySskoqqqpP+L1ERBq7gAeBmTUDpgP3OOf21ng6DBgEXAScBzxkZt1qvodzbrJzLsM5l5GUlHTCNWm9IRGRbwQ0CMwsHF8ITHXOzailSS7wvnNun3NuJ/Ap0C+QNcE3y0woCEREAnvWkAFTgCzn3BN1NJsJjDKzMDOLAYbgm0sIqIQY3zITO/YeCPRHiYg0eIE8a2gEcCOw3MyW+h/7GZAG4Jx7xjmXZWbvA18B1cDzzrkVAawJgP6pCUSHhzJr6VaGd24V6I8TEWnQAhYEzrm5gNWj3WPAY4Gqozbx0eFcNqAdby7J44ELehLvnzMQEfEiT15ZDHDD0A4cqKjmjcW5wS5FRCSoPBsEvdvFM6hDC16Zn011tQt2OSIiQePZIAC4aVgHNu3cx9z1O4NdiohI0Hg6CM7v04bE2Ahe+iI72KWIiASNp4MgMiyUcYNT+WT1DnL3lAa7HBGRoPB0EABcOziNagdvLs4LdikiIkHh+SBo3yKGwR1b8tbSPJzTpLGIeI/ngwDgsv4pbCjYx8qtNZdCEhFp+hQEwIV92xAeary1RMNDIuI9CgJ8aw+N6Z7MrGVbqdI1BSLiMQoCv7H925FfXMb8jbuCXYqIyCmlIPA7u2drmkWGaXhIRDxHQeAXFR7Keb3b8P6K7RyoqAp2OSIip4yC4BCXDWhHcVkln6w+JVsni4g0CAqCQwzv3IrWzSOZoRVJRcRDFASHCA0xLhuQwuw1BRQUlwW7HBGRU0JBUMOVA9tTVe2YuVSTxiLiDQqCGrq2jqNf+3ima+0hEfEIBUEtrhjUnqxte1m5tSjYpYiIBJyCoBaXnNaOiNAQpmfqqEBEmj4FQS1axEZwVs9kZi7No6KqOtjliIgElIKgDlcMbM+ufeXMWVMQ7FJERAIqYEFgZqlmNtvMssxspZndfZS2p5tZlZldGah6jtfo7km0iAnnna+2BrsUEZGACuQRQSUw0TnXExgKTDCzXjUbmVko8AfggwDWctzCQ0M4u2drPl6dT3mlhodEpOkKWBA457Y55xb7bxcDWUBKLU1/CEwHGty6Duf1bkPxgUq+0IqkItKEnZI5AjNLBwYAC2o8ngJcDjxzjNePN7NFZraooODUjdmP7NqKmIhQPli5/ZR9pojIqRbwIDCzZvj+xX+Pc67mXpBPAvc554663KdzbrJzLsM5l5GUlBSgSo8UFR7Kmd2T+XDlDm1YIyJNVkCDwMzC8YXAVOfcjFqaZAD/MrPNwJXAJDO7LJA1Ha/z+rRhZ0kZS3L2BLsUEZGACORZQwZMAbKcc0/U1sY519E5l+6cSwfeAO5yzr0VqJq+jTO7JxERGqLhIRFpsgJ5RDACuBH4jpkt9f9caGZ3mtmdAfzckyouKpwRXRJ5f+V2nNPwkIg0PWGBemPn3FzAjqP9LYGq5USd17sNs2csJ2tbMb3aNQ92OSIiJ5WuLK6Hs3u1JsTgneW6uExEmh4FQT20ahbJ6G5JvL4oV2sPiUiToyCopxuHdSC/uIyPVu0IdikiIieVgqCeRndLJiUhmlfmZwe7FBGRk0pBUE+hIcZ1Q9KYt2EX6/NLgl2OiMhJoyA4Dtecnkp4qDF1gY4KRKTpUBAch1bNIjm/T1umZ+ayv/yoq2KIiDQaCoLjdOPQDuw9UMmsZdrGUkSaBgXBcTo9vQW92jbnLx+vp7S8MtjliIicMAXBcTIzHhnbm7zC/fz1k/XBLkdE5IQpCL6F09NbctWg9jz36UbW7SgOdjkiIidEQfAt3X9BD2Ijw3ho5gotRicijZqC4FtKbBbJfef3YP7G3by1VBPHItJ4KQhOwLjTUzmtfTxPfLSWSq1BJCKNlILgBISEGBPO7MKW3ft5b4U2rhGRxqleQWBmd5tZc/OZYmaLzezcQBfXGJzTszWdWsXy7KcbNFcgIo1SfY8IbvNvPH8ukATcCvw+YFU1IiEhxvgzOrEiby/zNuwKdjkiIsetvkHw9U5jFwL/cM4t4zh2H2vqLhuQQqtmkTz76cZglyIictzqGwSZZvYhviD4wMziAM2O+kWFh3LriHQ+XVvAqq17g12OiMhxqW8Q3A7cD5zunCsFwvEND4nfDUM6EBsRyqQ5utpYRBqX+gbBMGCNc67QzG4AHgSKAldW4xMfE84tI9L5z1fbWLR5d7DLERGpt/oGwd+BUjPrB/wUyAZeClhVjdRdY7rQpnkUv5i5kqpqnUEkIo1DfYOg0vnOjRwLPOWcewqIO9oLzCzVzGabWZaZrTSzu2tpc72ZfeX/mecPmkYrNjKMn1/Uk1Xb9jJtYU6wyxERqZf6BkGxmT0A3Ai8Y2ah+OYJjqYSmOic6wkMBSaYWa8abTYBo51zpwG/BibXv/SG6eLT2jK0U0se/3ANe/aVB7scEZFjqm8QXAOU4bueYDuQAjx2tBc457Y55xb7bxcDWf7XHdpmnnNuj//ufKD9cdTeIJkZj1zah+IDlTz+4ZpglyMickz1CgL/l/9UIN7MLgYOOOfqPUdgZunAAGDBUZrdDrxXx+vHm9kiM1tUUFBQ348Nmu5t4rh2cCqvLdpCQXFZsMsRETmq+i4xcTWwELgKuBpYYGZX1vO1zYDpwD3+q5Nra3MmviC4r7bnnXOTnXMZzrmMpKSk+nxs0N02oiMVVU5zBSLS4NV3aOjn+K4huNk5dxMwGHjoWC8ys3B8ITDVOTejjjanAc8DY51zTWaNhk5JzTijWxJTF2RToZVJRaQBq28QhDjn8g+5v+tYrzUzA6YAWc65J+pokwbMAG50zq2tZy2Nxi3DO7Bjbxnva2VSEWnAwurZ7n0z+wCY5r9/DfDuMV4zAt9ZRsvNbKn/sZ8BaQDOuWeAXwCJwCRfblDpnMuod/UN3JhuyXRIjOHFeZu5pF+7YJcjIlKregWBc+7/mdkV+L7cDZjsnHvzGK+ZyzEWpnPO3QHcUc9aG52QEOPGoR34zTtZrMgrok9KfLBLEhE5Qr03pnHOTXfO/dg5d++xQkC+cVVGKjERofxz3uZglyIiUqtjjfMXm9neWn6KzUzLbNZDfHQ4V2ekMmNxLnPW5B/7BSIip9hRg8A5F+eca17LT5xzrvmpKrKx++n53enRpjk/fHUJ6/OLg12OiMhhtGfxKRATEcZzN2cQGR7K7S8u0tITItKgKAhOkZSEaJ69cRDbCg/ww2lLtL+xiDQYCoJTaFCHFjx0cU/mrt/JnLUNf6kMEfEGBcEpNm5wGikJ0fz143U6KhCRBkFBcIqFh4Zw55jOLM4p5IuNTWZFDRFpxBQEQXDVoPYkx0Xy9Cfa31hEgk9BEARR4aGMP6MT8zbsIjN7z7FfICISQAqCILluSBotYsJ5/IM1ZGbvYWvhfiq1SqmIBEF9F52TkywmIowJZ3bhN+9kccXf5wHQqlkkb00YTvsWMUGuTkS8REEQRHeM6sTobknkFu4nd89+fv32KibN2cCjl/cNdmki4iEKgiDr2jqOrq3jAFizfS///nILPzizC+0SooNcmYh4heYIGpDvj+kCwDP/2xDkSkTESxQEDUhKQjRXDGzPvxZuYXvRgWCXIyIeoSBoYO4a04Uq53j2Ux0ViMipoSBoYNISY7h8QAqvLshhY0FJsMsREQ9QEDRAd5/VldjIMK59br7CQEQCTkHQAKW2jGHad4dSWeUYN3k+GxQGIhJACoIGqnubOKaNH0q184VB9q59wS5JRJqogAWBmaWa2WwzyzKzlWZ2dy1tzMz+YmbrzewrMxsYqHoao26t45j23aGUV1bzvZczKS2vDHZJItIEBfKIoBKY6JzrCQwFJphZrxptLgC6+n/GA38PYD2NUtfWcfzl2gGs2VHM/dOXaw8DETnpAhYEzrltzrnF/tvFQBaQUqPZWOAl5zMfSDCztoGqqbEa3S2Jn5zbnVnLtjJl7qZglyMiTcwpmSMws3RgALCgxlMpwJZD7udyZFgIcNeYzpzXuzW/e2817y7fFuxyRKQJCXgQmFkzYDpwj3Nub82na3nJEWMfZjbezBaZ2aKCAm/u9Wtm/Onq/vRrH89dUxfzt9nrNUwkIidFQIPAzMLxhcBU59yMWprkAqmH3G8PbK3ZyDk32TmX4ZzLSEpKCkyxjUCzyDBe/e5QxvZvx2MfrGHia8soq6wKdlki0sgF8qwhA6YAWc65J+poNgu4yX/20FCgyDmncY+jiAoP5clr+jPxnG7MWJLHj6YtoapaRwYi8u0FchnqEcCNwHIzW+p/7GdAGoBz7hngXeBCYD1QCtwawHqaDDPjh/6rj3/1n1U88vZKHrm0N77sFRE5PgELAufcXGqfAzi0jQMmBKqGpu62kR3ZvvcAkz/dSNv4aL4/pnOwSxKRRkgb0zRy95/fg21FB/jD+6spPlDBnWM60zwqPNhliUgjoiBo5EJCjMevOo1Qg0lzNvDqwhzuGtOZm4alExUeGuzyRKQR0FpDTUBkWChPjhvA2z8YyWntE3j03dX8+LWlwS5LRBoJBUET0rd9PC/dNpiJ53Tj3eXb+e+qHcEuSUQaAQVBE/S90Z3p1roZD89ayb4yLVQnIkenIGiCIsJCePTyvuQV7ufPH60Ndjki0sApCJqojPSWXDs4jRc+38SKvKJglyMiDZiCoAm7//wetIyN5CevaykKEambgqAJi48J5w9X9GX19mIe/2BNsMsRkQZKQdDEndWzNdcPSeO5zzbx+fqdwS5HRBogBYEHPHhRLzolxTLxtWUUlpYHuxwRaWAUBB4QHRHKU9cMYGdJGXe8uIiNBSXBLklEGhAFgUf0bR/Pn67ux5odxZz/5Gc88dFaDlRoAllEFASeMrZ/Ch9PHM0Ffdvwl4/Xcfmkeezep6EiEa9TEHhMclwUT40bwJSbM9hYUMJ1z81nV0lZsMsSkSBSEHjUWT1b88Itp7N51z6uf36BwkDEwxQEHjaiSyteuNkXBlc9+wVZ2/Ye9vyXm3fz1H/X8cWGXVRUVQepShEJNPNtEtZ4ZGRkuEWLFgW7jCZl4abdTHh1MUX7K3jwop6c37sNv3tvNW8uyTvYpllkGGf3TOY3l/elWaS2sRBpbMws0zmXUetzCgIB2FVSxk9eX8bsNQWEhRghZow/oxM3D09nSc4eZq/J57VFuZzRtRXP3ZRBWKgOJkUaEwWB1ItzjhfnbWZxTiH3ntONjq1iD3t+6oJsfv7mCm4c2oFfje2N2VG3pBaRBuRoQaBjfDnIzLhlREduGVH789cP6UDOrlKe/XQjHRJjuGNUpzrfa8++cu6b/hW3j+zIkE6JAapYRE4GHd/Lcbnv/B5c0KcNv303i4/q2AHNOccDM5bz4aod/OhfS9ijaxVEGrSABYGZvWBm+Wa2oo7n483sbTNbZmYrzezWQNUiJ09IiPHna/rTNyWee/61hNXb9x7R5vXMXN5fuZ1rMlLZva+cB2Ysp7ENQYp4SSCPCP4JnH+U5ycAq5xz/YAxwJ/MLCKA9chJEhUeyuQbM4iNDOOOFxcddg1C9q59PDJrJUM7teTR/+vLxHO78/7K7byemRvEikXkaAIWBM65T4HdR2sCxJlvxrGZv6022G0k2sRHMfmmDPKLyxj/ciYvztvM859t5AevLiEkxPjT1f0JDTG+O6oTQzu15JFZK1m3ozjYZYtILQJ61pCZpQP/cc71qeW5OGAW0AOIA65xzr1Tx/uMB8YDpKWlDcrOzg5YzXJ8Zi7NY+Jry6is9v0eRYSG8Odr+nPRaW0Ptskr3M/Ff/mM0vIqJp7bjdtGdCTEjP+tLeDVhTmc3TOZa05PC1YXRDwhaKePHiMIrgRGAD8GOgMfAf2cc0cOOh9Cp482PHsPVFBZ5QgNMSLDQogKDz2izfaiAzz41gr+m7WD3u2as7+iio0F+wgxiI0I47P7ziQhRiODIoFytCAI5llDtwIznM96YBO+owNpZJpHhdMyNoL46PBaQwB8Q0nP3TSIp68bQGFpBXGRYTw1rj+zfjCSkvJKnv10Y62vq652/G32eh6euULLZosESDCvI8gBzgI+M7PWQHeg9m8DaRLMjItPa8fFp7U77PFL+7Xjn59v5tYR6STHRR18vKi0gnv+vYTZawoAWJZbxOSbBh3WRkROXCBPH50GfAF0N7NcM7vdzO40szv9TX4NDDez5cDHwH3OOW2q60H3nN2N8qpqJs3ecPCxFXlFXPq3ucxdv5NfX9aHZ24YxJrtxVz29OdHLI4nIicmYEcEzrlrj/H8VuDcQH2+NB4dW8Vy5cD2vLogh6GdEnkjcwv/zconKS6Sf40fyqAOLQFo32IYd7y4iCv/Po+/3zCIM7olBblykaZBVxZLg/Cjs7sCcOcrmWRm7+Hes7vx0b1nHAwBgD4p8cz8wQhSW8Zw2z+/5A1dmyByUmitIWkQUhKieeyq0ygsreCqjPbERNT+q9m6eRSv3zmMO1/J5CevL2PdjmLO7d2anm2b1/kaETk6rT4qjVJ5ZTX3z/iKGYt9eyaYQc82zfnxOd04q2eyVkYVqUHLUEuTtbVwPyu37mXl1iJmLdvKxoJ9jOrainvO7ka1c2wt3E9lleOSfu2ICNNIqHiXgkA8oaKqmpe/yObJ/65l74HDVyu5bkgaj17eN0iViQSf9iMQTwgPDeG2kR25bEAK/1ubT8vYSFISonhtUS6TP91I35R4rh185FIWObtKeWNxLn1T4jmrRzIhIcceVtq9rxznHInNIgPRFZFTSkEgTU7L2AguH9D+4P37zu9B1ra9PDxzJd3bxDEwrQXOOdbll/DM/zYwc+lWqvxrJXVvHcddZ3bmzB7JNI8Kr/X9C0vLueSvcynaX8HPL+rJuNNTNSchjZqGhsQTCkvLueTpuewvryKtZQzr8ksoPlBJdHgo1w9J47aRHZm/cReT5mxgfX4JAHFRYaQkRHPdkDRuGpYO+Dbd+d7Lmcxek89p7RPIzN7DqK6t+P0Vp5GSEB3EHoocneYIRIBVW/dy/4yviIkIpWtyHN3axHFhnzaHDe9UVzv+t66AtduL2Vq4n+V5RSzOKeSW4ek8dHEvXpmfzcOzVvLgRT25bURHpi7I5nfvrQbg7rO6cuuIjkSEhbCzpIwX520mr3A/D1/Sm/jo2o8uRE4VBYHIt1Rd7fjtu1lMmbuJkV1asXDTbkZ2bcWUmzMODgdt2V3KI2+v4r9ZO+iS3IxBaS14c2keFVXVhJrRsVUsL9xyOqktY4LcG/Gyhrr6qEiDFxJiPHRxLx6+pBefb9hJi9hwHr+q32FzAqktY3j+5gym3JxBWWUVby7J44qB7fn4x6N5+fYh7Nh7gMsnzWPplsIj3t85R/GBihOu0znHI2+v5PVFW074vcR7dEQgUk+Z2XtIjI0gvVVsnW0qq6opr6o+7Crn9fnF3PKPL8nds58OiTEMTGtBassYVuYVsThnD3sPVPLGncMYkNbiW9f27vJt3DV1MZFhIXw8cTTtW3xz9PFGZi7lldVcN0Sb/3iZhoZEgmxXSRlvZOayOGcPi3MKKSguo3NSLIM6tOCT1QWkJ8bw+p3DDjvSWJFXxLwNO1m0eQ8rt+6lRWw4XZPj6Nq6GVcObE9yc99y3PvLqzj7if8RFR5CXuF+vtMjmUnXDwJg3vqdXD9lASFmvH/3KLq2jjuuug9UVDFvw07GdKvfabXScOk6ApEgS2wWyfdGdwZ8wzhlldUHN/GZuiCbn7+5gg9X7eC83m0AmJ6Zy8TXlwGQ1jKGgR1aUFhazoKNu3hzSR6vfJHNS7cPoUtyM/4+Zz15hfv59/ihLNy0mz99tJbP1++kW+s47v73Ujq2imVncRm/eSeLF28bfLCmrG172bK7lHP9n1mb376Txcvzs7ljZEcevLhXoP54JMgUBCKnmJkdtpPbNRmpvDB3E394bzXf6ZHMV7mFPDBjOcM7J/LkuP5HbMSzIq+IW/7xJVc9M49fje3DM59uZGz/dgzplEi/1ARey9zCL2etpHXzKPbur+Dl2wczd91OfvNOFnPW5DOmezIr8oq4dvJ8SiuqWPzQObWe1bQ8t4hXFmSTkhDN83M30TYhmttHdgz4n4+ceposFgmysNAQ7r+gJxt37uPPH63ley9n0jYhiknXD6x1N7Y+KfFM//4wmkWF8cNpSwgLMR64oCcAUeGhPHhRL9bllzB3/U4eubQ3Pdo056Zh6aQnxvDbd7JYu6OYm19YCAZV1Y7P1hUc8RlV1Y4H31pOYmwk7/5oFOf3bsNv3lnFO19tO2pfqqodk+as593l2yivrD45f0AScAoCkQbg7J7JDE5vyaQ5GyirqGbKzRkkxETU2b5DYizT7xzOqK6t+OUlvWkT/01gnNurNVdntOe2ER255vRUACLCQnjgwp6syy/hkr/OxQzevGs48dHhzF59ZBBMW5jDstwiHrq4J/Ex4Tw5rj+D0lpw77+XMmvZ1jrr+v17Wfzx/TXcNXUxw373Mb99ZxUbCkoOa1O0v4LnP9vIvPXakLCh0GSxSAOxPLeIH0xbzK/G9mF0AHZfc85x/fMLWJ5XxL/HD6NXu+b8aNoS5m3YycKfnX1wMnhnSRnfeXwOvdvF8+p3hxycwC4sLeeOFxexKHsP48/oxE/P605Y6Df/lvx6XuPGoR34Ts9k/r1wC//N2kFltWN450TGDU5j9ba9vPxFNsVllbSLj+LTn5552HtI4OisIREBfGcBlZZX0TLWd7Tx5pJc7v33MmZOGEG/1AQAHnl7JS99kc0H94yiS/LhZxmVV1bzm3dW8dIX2QzvnMgtw9MZkNaCvML9XP3sFwxKa8FLtw8m3P/lXlBcxmuLtvDqghzyCvdjBhf2aUvf9vH8/r3VPH3dAC4+rd0p/TPwKp01JCKAbw7h0Inq0d2SMYNPVufTLzWBnSVlTFuYw2X9U44IAfANMf1qbB/6pMTzi5krmLdhFwDhoUbr5lH87fqBB0MAICkukglnduHO0Z1ZuGk3rZtH0impGVXVjmkLc5gyd1PQgiBnVymfrN7B2b1aH3bdhRcpCEQ8rGVsBP1TE5izJp97z+nGlLmbKKus5q4zOx/1dVdnpHJpv3asyCtiSU4hq7cX873RnQ4eadQUGmIM65x42P1bh6fzy7dXsThnDwNrXExXWFrOl5v3EBMRyogureqso7raUXygkviYo6/lVFRaQfbufWwtPED2rn28v3I7S3IKAfhy8x7+dv3Ao76+qVMQiHjcd7on86eP1rKhoISXv8jmwr5t6ZzU7JiviwoPJSO9JRnpLb/V516ZkcqfPlzLC3M3MfC6FlRWVfPC55uYnpnHmh3FAIQYvHL7EIbXCIPcPaVMz8zjjcVb2FFUxrt3j6z1CAZg7rqd3Pbil4edxdSrbXPuv6AHGwtKmLE4j+1FBw6bcA+EkrJKPs7awehuSUc9EeBQVdWO0FNwIV/AgsDMXgAuBvKdc33qaDMGeBIIB3Y650YHqh4Rqd2ZPXxBMGHqYkrKKpkwpssp+dxmkWGMG5zKC59v5rN1Bfzpw7Us3VLI4I4tmXhONwZ2aMHDs1byg2lLePuHI0lJiKayqprfv7eaKZ9vwjkY0SWRXSXlPP3Jep4cN+CIz8jetY8Jry4mPTGGn5zbnXYJ0bRLiD545JK9ax+vZ+by6sIcfnxOt1rrPFBRRUVVNXF17E9RH0X7K7j5hYUs3VJIRFgIF5/WluuHdGBQh7qXFVm3o5hrn5vPbSM7cleA/04CNllsZmcAJcBLtQWBmSUA84DznXM5ZpbsnMs/1vtqsljk5HLOMeTRj8kvLuOsHslMueX0U/bZuXtKOeOPs6l2kBATzq/G9uGS09oePFNpQ0EJY5/+nE5JsTxzwyB+/NpS5m/czXVD0vj+6M6ktozh0XezeP6zjXwyccxh60CVlFXyf5M+J7+4jFkTRpKWWPs8wK3/WMiKrXv5/L7vHLGv9eKcPYx/aRF791dybu/WXJ2RyogurY75r/TKquqDZ0Pt2VfOjS8sYM32Yn5xSW/Wbi/mzSV5lJRVMqJLIj85t/sR60wVlpYz9m+fk72rlBCDad8dypBOibV9VL0F7awhM0sH/lNHENwFtHPOPXg876kgEDn5fvrGMl5blMuMu4YfMV4faH94fzW5e/bz0EU9D66fdKj3V2znzlcyiQgNwQwevbwvVwz6Zge6/OIDjPrDbMb2b8cfr+wH+IZU7nwlk09W5/PybYOPGFo61OzV+dz6zy/567UDuKTfNxPX//lqKxNfW0ab+CjGdEti5rKtFJZWkBgbwdDOiQzvnMioLkmHBczaHcX8ctZK5m/cRdfkOPq2j2dFXhEbd+7j2RsGcWaPZAD2lVUybWEOk+ZsYPe+cs7u2Zrvje5ERocWVFU7bv3nl8zfuIspN5/OL2auoKyymvfuHlXvIaXaNNQgeBLfkFBvIA54yjn3Uh3vMx4YD5CWljYoOzs7UCWLeFLunlK+3Lz7sC0+G5KnP1nHzKVbeeLq/vRtH3/E87+ctZJX5mcz5/+NoXl0OHdPW8LsNQU8fEkvbh1x9GUxqqsdYx6fQ5vmUbx25zCKSiuY/NkG/jZ7AxkdWjD5pgxaxkZwoKKK/2bt4JPV+cxbv4vtew8Avu1Nz+vdmr0HKnl5fjbNIsO4fEAKm3ft46vcIiqqqpl0/UBGdT3y2pCSskr+MXcTkz/bSPGBSnq0iSOtZQwfrtrBH67oyzWnp7E8t4j/+/vnnNk9mWdvHPStt0VtqEHwNJABnAVEA18AFznn1h7tPXVEICI1bSvazxl/nM2Z3ZNZn19Czu5SHr60NzcMSavXF+dzn27kt+9mceWg9ry7fBul5VVcMbA9j/5fHyLDQo9o75xj0859zF5TwIcrt/Pl5t044LrBafzk3O608M9BOOeoqnbHvGiutLySmUu38vIX2azatpebh3XgkbHffG0+/9lGfvNOFr++rA83Du1wfH84fg31OoJcfBPE+4B9ZvYp0A84ahCIiNTUNj6aKwelMm1hDomxEUy9Y8hxjalfldGeJz5ay1tL8ri0XzvuGNWJXu2a19nezOiU1IxOSc24fWRHdpWUcaCy+oh9q82MsNBjB1FMRBjXDk5j3OmpZO8qJa3Gbna3jejI8rwiWsdF1vEOJyaYQTATeNrMwoAIYAjw5yDWIyKN2L3ndCUmIpTbRnY84gv5WBJiIpj1gxHERYV/q9NID933+kSYWa0bH4WEGE/VclbUyRLI00enAWOAVmaWCzyMb04A59wzzrksM3sf+AqoBp53zq0IVD0i0rQlx0Xx0AnsmXC8m/Y0JQELAufctfVo8xjwWKBqEBGRY9OyfyIiHqcgEBHxOAWBiIjHKQhERDxOQSAi4nEKAhERj1MQiIh4XKPbs9jMCoBvu+pcK2DnSSynsfBiv73YZ/Bmv73YZzj+fndwzh258h2NMAhOhJktqmvRpabMi/32Yp/Bm/32Yp/h5PZbQ0MiIh6nIBAR8TivBcHkYBcQJF7stxf7DN7stxf7DCex356aIxARkSN57YhARERqUBCIiHicZ4LAzM43szVmtt7M7g92PYFgZqlmNtvMssxspZnd7X+8pZl9ZGbr/P9tEexaTzYzCzWzJWb2H/99L/Q5wczeMLPV/r/zYR7p973+3+8VZjbNzKKaWr/N7AUzyzezFYc8VmcfzewB/3fbGjM773g/zxNBYGahwN+AC4BewLVm9u23Mmq4KoGJzrmewFBggr+f9wMfO+e6Ah/77zc1dwNZh9z3Qp+fAt53zvXAt993Fk2832aWAvwIyHDO9QFCgXE0vX7/Ezi/xmO19tH///g4oLf/NZP833n15okgAAYD651zG51z5cC/gLFBrumkc85tc84t9t8uxvfFkIKvry/6m70IXBaUAgPEzNoDFwHPH/JwU+9zc+AMYAqAc67cOVdIE++3XxgQ7d/vPAbYShPrt3PuU2B3jYfr6uNY4F/OuTLn3CZgPb7vvHrzShCkAFsOuZ/rf6zJMrN0YACwAGjtnNsGvrAAkoNYWiA8CfwU397XX2vqfe4EFAD/8A+JPW9msTTxfjvn8oDHgRxgG1DknPuQJt5vv7r6eMLfb14JAqvlsSZ73qyZNQOmA/c45/YGu55AMrOLgXznXGawaznFwoCBwN+dcwOAfTT+4ZBj8o+LjwU6Au2AWDO7IbhVBd0Jf795JQhygdRD7rfHdzjZ5JhZOL4QmOqcm+F/eIeZtfU/3xbID1Z9ATACuNTMNuMb8vuOmb1C0+4z+H6nc51zC/z338AXDE2932cDm5xzBc65CmAGMJym32+ou48n/P3mlSD4EuhqZh3NLALfxMqsINd00pmZ4RszznLOPXHIU7OAm/23bwZmnuraAsU594Bzrr1zLh3f3+snzrkbaMJ9BnDObQe2mFl3/0NnAato4v3GNyQ01Mxi/L/vZ+GbC2vq/Ya6+zgLGGdmkWbWEegKLDyud3bOeeIHuBBYC2wAfh7segLUx5H4Dgm/Apb6fy4EEvGdZbDO/9+Wwa41QP0fA/zHf7vJ9xnoDyzy/32/BbTwSL8fAVYDK4CXgcim1m9gGr45kAp8/+K//Wh9BH7u/25bA1xwvJ+nJSZERDzOK0NDIiJSBwWBiIjHKQhERDxOQSAi4nEKAhERj1MQiByDmc3z/zfdzK4Ldj0iJ5uCQOQYnHPD/TfTgeMKguNdBVIkGBQEIsdgZiX+m78HRpnZUv+a+KFm9piZfWlmX5nZ9/ztx/j3hXgVWG5msWb2jpkt86+hf03QOiNSi7BgFyDSiNwP/MQ5dzGAmY3Ht/rl6WYWCXxuZh/62w4G+jjnNpnZFcBW59xF/tfFB6N4kbroiEDk2zsXuMnMluJb7jsR3zovAAudb214gOXA2Wb2BzMb5ZwrOvWlitRNQSDy7RnwQ+dcf/9PR+dbGx98y0ID4JxbCwzCFwi/M7NfBKFWkTopCETqrxiIO+T+B8D3/Ut/Y2bd/JvDHMbM2gGlzrlX8G2qMvBUFCtSX5ojEKm/r4BKM1uGb0/Zp/CdSbTYvyRyAbVvkdgXeMzMqvGtJvn9U1GsSH1p9VEREY/T0JCIiMcpCEREPE5BICLicQoCERGPUxCIiHicgkBExOMUBCIiHvf/ASKYGSdn53ccAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.xlabel('iters')\n",
    "plt.ylabel('loss')\n",
    "plt.plot(np.array(all_losses))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluation: A Sample of Generated Sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-15T03:10:52.267837Z",
     "start_time": "2019-05-15T03:10:51.986701Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'asures seized, our soldiers put to flight,\\nAnd, as thou seest, ourselves in heavy plight.\\n\\nKING LEWIS XI:\\nRenowned queen, with po\\tsfs\\tu\\tsozs\\t\\tu\\tsfu\\t\\tf\\tosf\\tufsiui\\tuffo\\tfi\\tUO\\t\\t\\tuo\\tuif\\tsu\\tss\\ti\\tu\\tsf\\tsffs\\tuo\\tufsof\\tuffo\\tfi\\rUUjJDTupOiJuuUJ\\rUJs\\t\\tf\\ts\\tuvsss\\t\\tufoifuf\\tufsfuiuf\\t\\tfuuiufsiuusu\\tfsuisffifuiss\\tusfuiDuuui\\tuifsufiuffufuiufsufssfui\\tuisf\\tsuiss\\tusfUiioJBui\\tDfJiiiJUiJfuusufuisfffuuis\\tfsisuisfsifisuusiifisffiifiusfiffssfsifffuifsifBiiifuifsfisfissisifisfisffsifuiffsufifffuifsiff\\tJOifuBsJfffJifJsiifisiffsssiiffusfffiffsffsffiifffsffssffsfifsfssffsffsssifsffsOfffsifsfsffsffffffsffsffssffsifssfisfsssifsffssuJDfsiOfJsssJfsJfffsfffssfffffssifsssfssfssfssffsssfssffssfsfsfsffssfssffffsfssfDsssffsfsfssfssssssfssfssffssffsffsffsffffsfs'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rand_input, rand_target = get_input_and_target()  \n",
    "rand_input = rand_input[None].to(device)\n",
    "rand_target = rand_target[None].to(device)\n",
    "generate_text(model,rand_target,start_seq = rand_input,gen_len=600)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis\n",
    "\n",
    "In attempting to printing the output of the current model, the main change was for the model to pass in the input twice,since the target gave away the answers. This prevented the model from generalizing to other input and thus had bad behavior in printing words."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
