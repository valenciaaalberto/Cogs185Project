{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-14T23:57:20.051203Z",
     "start_time": "2019-05-14T23:57:19.626384Z"
    }
   },
   "outputs": [],
   "source": [
    "import string\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Choose a Device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-14T23:57:20.129301Z",
     "start_time": "2019-05-14T23:57:20.081156Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "# If there are GPUs, choose the first one for computing. Otherwise use CPU.\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)  \n",
    "# If 'cuda:0' is printed, it means GPU is available."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Transformer Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        # Ensure that the model dimension (d_model) is divisible by the number of heads\n",
    "        assert d_model % num_heads == 0, \"d_model must be divisible by num_heads\"\n",
    "        \n",
    "        # Initialize dimensions\n",
    "        self.d_model = d_model # Model's dimension\n",
    "        self.num_heads = num_heads # Number of attention heads\n",
    "        self.d_k = d_model // num_heads # Dimension of each head's key, query, and value\n",
    "        \n",
    "        # Linear layers for transforming inputs\n",
    "        self.W_q = nn.Linear(d_model, d_model) # Query transformation\n",
    "        self.W_k = nn.Linear(d_model, d_model) # Key transformation\n",
    "        self.W_v = nn.Linear(d_model, d_model) # Value transformation\n",
    "        self.W_o = nn.Linear(d_model, d_model) # Output transformation\n",
    "        \n",
    "    def scaled_dot_product_attention(self, Q, K, V, mask=None):\n",
    "        # Calculate attention scores\n",
    "        attn_scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.d_k)\n",
    "        \n",
    "        # Apply mask if provided (useful for preventing attention to certain parts like padding)\n",
    "        if mask is not None:\n",
    "            attn_scores = attn_scores.masked_fill(mask == 0, -1e9)\n",
    "        \n",
    "        # Softmax is applied to obtain attention probabilities\n",
    "        attn_probs = torch.softmax(attn_scores, dim=-1)\n",
    "        \n",
    "        # Multiply by values to obtain the final output\n",
    "        output = torch.matmul(attn_probs, V)\n",
    "        return output\n",
    "        \n",
    "    def split_heads(self, x):\n",
    "        # Reshape the input to have num_heads for multi-head attention\n",
    "        batch_size, seq_length, d_model = x.size()\n",
    "        return x.view(batch_size, seq_length, self.num_heads, self.d_k).transpose(1, 2)\n",
    "        \n",
    "    def combine_heads(self, x):\n",
    "        # Combine the multiple heads back to original shape\n",
    "        batch_size, _, seq_length, d_k = x.size()\n",
    "        return x.transpose(1, 2).contiguous().view(batch_size, seq_length, self.d_model)\n",
    "        \n",
    "    def forward(self, Q, K, V, mask=None):\n",
    "        # Apply linear transformations and split heads\n",
    "        Q = self.split_heads(self.W_q(Q))\n",
    "        K = self.split_heads(self.W_k(K))\n",
    "        V = self.split_heads(self.W_v(V))\n",
    "        \n",
    "        # Perform scaled dot-product attention\n",
    "        attn_output = self.scaled_dot_product_attention(Q, K, V, mask)\n",
    "        \n",
    "        # Combine heads and apply output transformation\n",
    "        output = self.W_o(self.combine_heads(attn_output))\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_seq_length):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        \n",
    "        pe = torch.zeros(max_seq_length, d_model)\n",
    "        position = torch.arange(0, max_seq_length, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * -(math.log(10000.0) / d_model))\n",
    "        \n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        \n",
    "        self.register_buffer('pe', pe.unsqueeze(0))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return x + self.pe[:, :x.size(1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionWiseFeedForward(nn.Module):\n",
    "    def __init__(self, d_model, d_ff):\n",
    "        super(PositionWiseFeedForward, self).__init__()\n",
    "        self.fc1 = nn.Linear(d_model, d_ff)\n",
    "        self.fc2 = nn.Linear(d_ff, d_model)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, d_ff, dropout):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "        self.self_attn = MultiHeadAttention(d_model, num_heads)\n",
    "        self.feed_forward = PositionWiseFeedForward(d_model, d_ff)\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x, mask):\n",
    "        attn_output = self.self_attn(x, x, x, mask)\n",
    "        x = self.norm1(x + self.dropout(attn_output))\n",
    "        ff_output = self.feed_forward(x)\n",
    "        x = self.norm2(x + self.dropout(ff_output))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, d_ff, dropout):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "        self.self_attn = MultiHeadAttention(d_model, num_heads)\n",
    "        self.cross_attn = MultiHeadAttention(d_model, num_heads)\n",
    "        self.feed_forward = PositionWiseFeedForward(d_model, d_ff)\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.norm3 = nn.LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x, enc_output, src_mask, tgt_mask):\n",
    "        attn_output = self.self_attn(x, x, x, tgt_mask)\n",
    "        x = self.norm1(x + self.dropout(attn_output))\n",
    "        attn_output = self.cross_attn(x, enc_output, enc_output, src_mask)\n",
    "        x = self.norm2(x + self.dropout(attn_output))\n",
    "        ff_output = self.feed_forward(x)\n",
    "        x = self.norm3(x + self.dropout(ff_output))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(self, src_vocab_size, tgt_vocab_size, d_model, num_heads, num_layers, d_ff, max_seq_length, dropout):\n",
    "        super(Transformer, self).__init__()\n",
    "        self.encoder_embedding = nn.Embedding(src_vocab_size, d_model)\n",
    "        self.decoder_embedding = nn.Embedding(tgt_vocab_size, d_model)\n",
    "        self.positional_encoding = PositionalEncoding(d_model, max_seq_length)\n",
    "\n",
    "        self.encoder_layers = nn.ModuleList([EncoderLayer(d_model, num_heads, d_ff, dropout) for _ in range(num_layers)])\n",
    "        self.decoder_layers = nn.ModuleList([DecoderLayer(d_model, num_heads, d_ff, dropout) for _ in range(num_layers)])\n",
    "\n",
    "        self.fc = nn.Linear(d_model, tgt_vocab_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def generate_mask(self, src, tgt):\n",
    "        src_mask = (src != 0).unsqueeze(1).unsqueeze(2)\n",
    "        tgt_mask = (tgt != 0).unsqueeze(1).unsqueeze(3)\n",
    "        seq_length = tgt.size(1)\n",
    "        nopeak_mask = (1 - torch.triu(torch.ones(1, seq_length, seq_length), diagonal=1)).bool().to(device)\n",
    "        tgt_mask = (tgt_mask & nopeak_mask).to(device)\n",
    "        return src_mask, tgt_mask\n",
    "\n",
    "\n",
    "    def forward(self, src, tgt):\n",
    "        src_mask, tgt_mask = self.generate_mask(src, tgt)\n",
    "        src_embedded = self.dropout(self.positional_encoding(self.encoder_embedding(src)))\n",
    "        tgt_embedded = self.dropout(self.positional_encoding(self.decoder_embedding(tgt)))\n",
    "\n",
    "        enc_output = src_embedded\n",
    "        for enc_layer in self.encoder_layers:\n",
    "            enc_output = enc_layer(enc_output, src_mask)\n",
    "    \n",
    "        dec_output = tgt_embedded\n",
    "        for dec_layer in self.decoder_layers:\n",
    "            dec_output = dec_layer(dec_output, enc_output, src_mask, tgt_mask)\n",
    "\n",
    "        output = self.fc(dec_output).to(device)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of file: 1115394\n",
      "All possible characters: 0123456789abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~ \t\n",
      "\r",
      "\u000b",
      "\f",
      "\n",
      "Number of all possible characters: 100\n"
     ]
    }
   ],
   "source": [
    "all_chars       = string.printable\n",
    "n_chars         = len(all_chars)\n",
    "file            = open('../Data/shakespeare.txt').read()\n",
    "file_len        = len(file)\n",
    "\n",
    "print('Length of file: {}'.format(file_len))\n",
    "print('All possible characters: {}'.format(all_chars))\n",
    "print('Number of all possible characters: {}'.format(n_chars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a random sequence of the Shakespeare dataset.\n",
    "def get_random_seq():\n",
    "    seq_len     = 128  # The length of an input sequence.\n",
    "    start_index = random.randint(0, file_len - seq_len)\n",
    "    end_index   = start_index + seq_len + 1\n",
    "    return file[start_index:end_index]\n",
    "\n",
    "# Convert the sequence to index tensor.\n",
    "def seq_to_index(seq):\n",
    "    tensor = torch.zeros(len(seq), dtype = torch.long)\n",
    "    # Shape of the tensor: \n",
    "    #     (sequence length, batch size).\n",
    "    # Here we use batch size = 1.\n",
    "    for t, char in enumerate(seq):\n",
    "        tensor[t] = all_chars.index(char) + 1\n",
    "    return tensor\n",
    "\n",
    "# Sample a mini-batch including input tensor and target tensor.\n",
    "def get_input_and_target():\n",
    "    seq    = get_random_seq()\n",
    "    input  = seq_to_index(seq[:-1])      # Input is represented in one-hot.\n",
    "    target = seq_to_index(seq[1:]) # Target is represented in index.\n",
    "    return input, target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instantiate Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Transformer(\n",
       "  (encoder_embedding): Embedding(100, 128)\n",
       "  (decoder_embedding): Embedding(100, 128)\n",
       "  (positional_encoding): PositionalEncoding()\n",
       "  (encoder_layers): ModuleList(\n",
       "    (0-3): 4 x EncoderLayer(\n",
       "      (self_attn): MultiHeadAttention(\n",
       "        (W_q): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (W_k): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (W_v): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (W_o): Linear(in_features=128, out_features=128, bias=True)\n",
       "      )\n",
       "      (feed_forward): PositionWiseFeedForward(\n",
       "        (fc1): Linear(in_features=128, out_features=512, bias=True)\n",
       "        (fc2): Linear(in_features=512, out_features=128, bias=True)\n",
       "        (relu): ReLU()\n",
       "      )\n",
       "      (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (decoder_layers): ModuleList(\n",
       "    (0-3): 4 x DecoderLayer(\n",
       "      (self_attn): MultiHeadAttention(\n",
       "        (W_q): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (W_k): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (W_v): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (W_o): Linear(in_features=128, out_features=128, bias=True)\n",
       "      )\n",
       "      (cross_attn): MultiHeadAttention(\n",
       "        (W_q): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (W_k): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (W_v): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (W_o): Linear(in_features=128, out_features=128, bias=True)\n",
       "      )\n",
       "      (feed_forward): PositionWiseFeedForward(\n",
       "        (fc1): Linear(in_features=128, out_features=512, bias=True)\n",
       "        (fc2): Linear(in_features=512, out_features=128, bias=True)\n",
       "        (relu): ReLU()\n",
       "      )\n",
       "      (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      (norm3): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (fc): Linear(in_features=128, out_features=100, bias=True)\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Instantiate the Transformer model\n",
    "d_model = 128\n",
    "num_heads = 4\n",
    "num_layers = 4\n",
    "d_ff = 512\n",
    "# d_ff = 2048\n",
    "# d_ff = 128\n",
    "max_seq_length = 128\n",
    "dropout = 0.1\n",
    "\n",
    "model = Transformer(\n",
    "    src_vocab_size=100,\n",
    "    tgt_vocab_size=100,\n",
    "    d_model=d_model,\n",
    "    num_heads=num_heads,\n",
    "    num_layers=num_layers,\n",
    "    d_ff=d_ff,\n",
    "    max_seq_length=max_seq_length,\n",
    "    dropout=dropout\n",
    ")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(model,target,start_seq='Wha', gen_len=10,temperature=1.0):\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    \n",
    "    indices = \"\"\n",
    "    tensor = start_seq[0]\n",
    "    for char in tensor:\n",
    "        indices += all_chars[char-1] \n",
    "        \n",
    "    input_seq = start_seq\n",
    "    seen_output = []\n",
    "    \n",
    "    generated_text = start_seq\n",
    "    \n",
    "    for _ in range(gen_len):\n",
    "        with torch.no_grad():\n",
    "            output = model(input_seq, target)\n",
    "        \n",
    "        # Get the last output and convert to probabilities\n",
    "        next_char_logits = output[-1, 0, :] # Shape: (output_dim)\n",
    "        next_char_probs = torch.softmax(next_char_logits, dim=-1)/temperature\n",
    "\n",
    "        predicted_char_index = torch.argmax(next_char_probs)\n",
    "        predicted_char = all_chars[predicted_char_index]\n",
    "        \n",
    "        indices += predicted_char\n",
    "        \n",
    "        # Append the next character to the input sequence\n",
    "        next_char_tensor = torch.tensor([predicted_char_index], dtype=torch.long).unsqueeze(1).to(device)\n",
    "        input_seq = torch.cat([input_seq, next_char_tensor], dim=1)[:, 1:]\n",
    "    \n",
    "    return indices\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Procedure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter:199/20000 loss:0.24093029928859322\n",
      "generated sequence: t solely.\n",
      "\n",
      "AUFIDIUS:\n",
      "I understand thee well; and be thou sure,\n",
      "when he shall come to his account, he knows not\n",
      "What I can urge a\t\t\t\t\t\t\t\t\t\t\n",
      "\n",
      "iter:399/20000 loss:0.004556668101577089\n",
      "generated sequence: earth,\n",
      "That reigns in galled eyes of weeping souls,\n",
      "Thy womb let loose, to chase us to our graves.\n",
      "O upright, just, and true-disbbbbbbbbbb\n",
      "\n",
      "iter:599/20000 loss:0.0019853215874172745\n",
      "generated sequence: rs our hoped-for hay.\n",
      "\n",
      "GLOUCESTER:\n",
      "Away betimes, before his forces join,\n",
      "And take the great-grown traitor unawares:\n",
      "Brave warriotttttttttt\n",
      "\n",
      "iter:799/20000 loss:0.0011533977364888415\n",
      "generated sequence: age-cock\n",
      "Hath twice done salutation to the morn;\n",
      "Your friends are up, and buckle on their armour.\n",
      "\n",
      "KING RICHARD III:\n",
      "O Ratcliff,hhhhhhhhhh\n",
      "\n",
      "iter:999/20000 loss:0.000745466342632426\n",
      "generated sequence: stess:\n",
      "A pair of stocks, you rogue!\n",
      "\n",
      "SLY:\n",
      "Ye are a baggage: the Slys are no rogues; look in\n",
      "the chronicles; we came in with Richuuuuuuuuuu\n",
      "\n",
      "iter:1199/20000 loss:0.0005167337811144534\n",
      "generated sequence:  VINCENTIO:\n",
      "Mended again. The matter; proceed.\n",
      "\n",
      "ISABELLA:\n",
      "In brief, to set the needless process by,\n",
      "How I persuaded, how I pray'WWWWWWWWWW\n",
      "\n",
      "iter:1399/20000 loss:0.0004081395650428021\n",
      "generated sequence: heresoever, I wish him well.\n",
      "\n",
      "LUCIO:\n",
      "It was a mad fantastical trick of him to steal from\n",
      "the state, and usurp the beggary he wasffffffffff\n",
      "\n",
      "iter:1599/20000 loss:0.000304586254642345\n",
      "generated sequence:  asleep, while they do dream things true.\n",
      "\n",
      "MERCUTIO:\n",
      "O, then, I see Queen Mab hath been with you.\n",
      "She is the fairies' midwife, abbbbbbbbbb\n",
      "\n",
      "iter:1799/20000 loss:0.00024885410217393653\n",
      "generated sequence: hem Sir Thomas Vaughan, prisoners.\n",
      "\n",
      "DUCHESS OF YORK:\n",
      "Who hath committed them?\n",
      "\n",
      "Messenger:\n",
      "The mighty dukes\n",
      "Gloucester and Buckinffffffffff\n",
      "\n",
      "iter:1999/20000 loss:0.00019800826281425544\n",
      "generated sequence: known to me,\n",
      "In the preferment of the eldest sister.\n",
      "This liberty is all that I request,\n",
      "That, upon knowledge of my parentage,\n",
      "Ioooooooooo\n",
      "\n",
      "iter:2199/20000 loss:0.00016335292821167968\n",
      "generated sequence: have it,\n",
      "And soundly too: your houses!\n",
      "\n",
      "ROMEO:\n",
      "This gentleman, the prince's near ally,\n",
      "My very friend, hath got his mortal hurt\n",
      "bbbbbbbbbb\n",
      "\n",
      "iter:2399/20000 loss:0.0001287147362381802\n",
      "generated sequence: winter should cut off our spring-time so.\n",
      "\n",
      "WARWICK:\n",
      "Away, away! Once more, sweet lords farewell.\n",
      "\n",
      "GEORGE:\n",
      "Yet let us all togethejjjjjjjjjj\n",
      "\n",
      "iter:2599/20000 loss:0.00011074219582951628\n",
      "generated sequence: eign, whom they must obey?\n",
      "Nay, whom they shall obey, and love thee too,\n",
      "Unless they seek for hatred at my hands;\n",
      "Which if they jjjjjjjjjj\n",
      "\n",
      "iter:2799/20000 loss:9.311113932199078e-05\n",
      "generated sequence: Commit the war of white and damask in\n",
      "Their nicely-gawded cheeks to the wanton spoil\n",
      "Of Phoebus' burning kisses: such a pother\n",
      "Apppppppppp\n",
      "\n",
      "iter:2999/20000 loss:8.399072572501609e-05\n",
      "generated sequence: ife! not life, but love in death!\n",
      "\n",
      "CAPULET:\n",
      "Despised, distressed, hated, martyr'd, kill'd!\n",
      "Uncomfortable time, why camest thou ngggggggggg\n",
      "\n",
      "iter:3199/20000 loss:6.912261143952491e-05\n",
      "generated sequence: in right.\n",
      "\n",
      "PRINCE:\n",
      "My gracious father, by your kingly leave,\n",
      "I'll draw it as apparent to the crown,\n",
      "And in that quarrel use it toooooooooo\n",
      "\n",
      "iter:3399/20000 loss:6.106640017605969e-05\n",
      "generated sequence: ad you did not nurse him:\n",
      "Though he does bear some signs of me, yet you\n",
      "Have too much blood in him.\n",
      "\n",
      "HERMIONE:\n",
      "What is this? spoeeeeeeeeee\n",
      "\n",
      "iter:3599/20000 loss:5.152911224286072e-05\n",
      "generated sequence:  whose captain I account myself,\n",
      "Look on my forces with a gracious eye;\n",
      "Put in their hands thy bruising irons of wrath,\n",
      "That thexxxxxxxxxx\n",
      "\n",
      "iter:3799/20000 loss:4.512523726589279e-05\n",
      "generated sequence: o stick it in their children's sight\n",
      "For terror, not to use, in time the rod\n",
      "Becomes more mock'd than fear'd; so our decrees,\n",
      "De\t\t\t\t\t\t\t\t\t\t\n",
      "\n",
      "iter:3999/20000 loss:3.853301242997986e-05\n",
      "generated sequence: en, how have you run\n",
      "From slaves that apes would beat! Pluto and hell!\n",
      "All hurt behind; backs red, and faces pale\n",
      "With flight anoooooooooo\n",
      "\n",
      "iter:4199/20000 loss:3.4463937372493094e-05\n",
      "generated sequence: GLOUCESTER:\n",
      "Vouchsafe to wear this ring.\n",
      "\n",
      "LADY ANNE:\n",
      "To take is not to give.\n",
      "\n",
      "GLOUCESTER:\n",
      "Look, how this ring encompasseth fingeMMMMMMMMMM\n",
      "\n",
      "iter:4399/20000 loss:3.07347290981852e-05\n",
      "generated sequence: spects thereof are nice and trivial,\n",
      "All circumstances well considered.\n",
      "You say that Edward is your brother's son:\n",
      "So say we tooqqqqqqqqqq\n",
      "\n",
      "iter:4599/20000 loss:2.6387508178231656e-05\n",
      "generated sequence: te friends upon this coast.\n",
      "\n",
      "NORTHUMBERLAND:\n",
      "The news is very fair and good, my lord:\n",
      "Richard not far from hence hath hid his heffffffffff\n",
      "\n",
      "iter:4799/20000 loss:2.3689893323535218e-05\n",
      "generated sequence: g.\n",
      "\n",
      "MENENIUS:\n",
      "Worthy man!\n",
      "\n",
      "First Senator:\n",
      "He cannot but with measure fit the honours\n",
      "Which we devise him.\n",
      "\n",
      "COMINIUS:\n",
      "Our spoils //////////\n",
      "\n",
      "iter:4999/20000 loss:2.1280257924445324e-05\n",
      "generated sequence:  Citizen:\n",
      "In him there is a hope of government,\n",
      "That in his nonage council under him,\n",
      "And in his full and ripen'd years himself,DDDDDDDDDD\n",
      "\n",
      "iter:5199/20000 loss:1.8429127126182722e-05\n",
      "generated sequence: HMOND:\n",
      "Fellows in arms, and my most loving friends,\n",
      "Bruised underneath the yoke of tyranny,\n",
      "Thus far into the bowels of the landNNNNNNNNNN\n",
      "\n",
      "iter:5399/20000 loss:1.582494497142761e-05\n",
      "generated sequence: rt thou dead:\n",
      "Then, as the manner of our country is,\n",
      "In thy best robes uncover'd on the bier\n",
      "Thou shalt be borne to that same anuuuuuuuuuu\n",
      "\n",
      "iter:5599/20000 loss:1.4441159787565994e-05\n",
      "generated sequence: r his accustom'd health.\n",
      "\n",
      "GREY:\n",
      "In that you brook it in, it makes him worse:\n",
      "Therefore, for God's sake, entertain good comfort,\n",
      "\t\t\t\t\t\t\t\t\t\t\n",
      "\n",
      "iter:5799/20000 loss:1.2683391312293678e-05\n",
      "generated sequence: ING RICHARD II:\n",
      "I had forgot myself; am I not king?\n",
      "Awake, thou coward majesty! thou sleepest.\n",
      "Is not the king's name twenty thoOOOOOOOOOO\n",
      "\n",
      "iter:5999/20000 loss:1.1448222251146944e-05\n",
      "generated sequence:  hate one another.\n",
      "\n",
      "Third Servingman:\n",
      "Reason; because they then less need one another.\n",
      "The wars for my money. I hope to see Romaiiiiiiiiii\n",
      "\n",
      "iter:6199/20000 loss:9.881170810785989e-06\n",
      "generated sequence: ice a cup,\n",
      "To give mine enemy a lasting wink;\n",
      "Which draught to me were cordial.\n",
      "\n",
      "CAMILLO:\n",
      "Sir, my lord,\n",
      "I could do this, and thadddddddddd\n",
      "\n",
      "iter:6399/20000 loss:8.89744449295904e-06\n",
      "generated sequence: ss to Burgundy;\n",
      "And, in my company, my brother Gloucester;\n",
      "Who from my cabin tempted me to walk\n",
      "Upon the hatches: thence we looktttttttttt\n",
      "\n",
      "iter:6599/20000 loss:0.16666726525727882\n",
      "generated sequence: ar my standard.\n",
      "Give me some ink and paper in my tent\n",
      "I'll draw the form and model of our battle,\n",
      "Limit each leader to his severssssssssss\n",
      "\n",
      "iter:6799/20000 loss:0.0005873525968490867\n",
      "generated sequence: success.\n",
      "\n",
      "COMINIUS:\n",
      "He'll never hear him.\n",
      "\n",
      "SICINIUS:\n",
      "Not?\n",
      "\n",
      "COMINIUS:\n",
      "I tell you, he does sit in gold, his eye\n",
      "Red as 'twould burvvvvvvvvvv\n",
      "\n",
      "iter:6999/20000 loss:0.0005106858407089021\n",
      "generated sequence: sheet bleaching on the hedge,\n",
      "With heigh! the sweet birds, O, how they sing!\n",
      "Doth set my pugging tooth on edge;\n",
      "For a quart of aiiiiiiiiii\n",
      "\n",
      "iter:7199/20000 loss:0.0003524123115857947\n",
      "generated sequence: SS OF YORK:\n",
      "Ah, so much interest have I in thy sorrow\n",
      "As I had title in thy noble husband!\n",
      "I have bewept a worthy husband's deatTTTTTTTTTT\n",
      "\n",
      "iter:7399/20000 loss:0.00011450714380771387\n",
      "generated sequence: ke an Amazonian trull,\n",
      "Upon their woes whom fortune captivates!\n",
      "But that thy face is, vizard-like, unchanging,\n",
      "Made impudent witffffffffff\n",
      "\n",
      "iter:7599/20000 loss:9.327784626293578e-05\n",
      "generated sequence: r unknown sovereignty!\n",
      "\n",
      "DUKE VINCENTIO:\n",
      "You are pardon'd, Isabel:\n",
      "And now, dear maid, be you as free to us.\n",
      "Your brother's death\t\t\t\t\t\t\t\t\t\t\n",
      "\n",
      "iter:7799/20000 loss:7.393723712084465e-05\n",
      "generated sequence: eath;\n",
      "How can we aid you with our kindred tears?\n",
      "\n",
      "Girl:\n",
      "Our fatherless distress was left unmoan'd;\n",
      "Your widow-dolour likewise bebbbbbbbbbb\n",
      "\n",
      "iter:7999/20000 loss:9.098550613998668e-05\n",
      "generated sequence: y us both be spent most preciously.\n",
      "\n",
      "ARIEL:\n",
      "Is there more toil? Since thou dost give me pains,\n",
      "Let me remember thee what thou ha\t\t\t\t\t\t\t\t\t\t\n",
      "\n",
      "iter:8199/20000 loss:6.379819071298698e-05\n",
      "generated sequence: eisure serves me, pensive daughter, now.\n",
      "My lord, we must entreat the time alone.\n",
      "\n",
      "PARIS:\n",
      "God shield I should disturb devotion!\n",
      "jjjjjjjjjj\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter:8399/20000 loss:5.398545321440906e-05\n",
      "generated sequence:  is great.\n",
      "\n",
      "BAPTISTA:\n",
      "Lucentio is your name; of whence, I pray?\n",
      "\n",
      "TRANIO:\n",
      "Of Pisa, sir; son to Vincentio.\n",
      "\n",
      "BAPTISTA:\n",
      "A mighty manjjjjjjjjjj\n",
      "\n",
      "iter:8599/20000 loss:5.295121457493224e-05\n",
      "generated sequence: roar'd away your victory,\n",
      "That pages blush'd at him and men of heart\n",
      "Look'd wondering each at other.\n",
      "\n",
      "CORIOLANUS:\n",
      "Hear'st thou, pppppppppp\n",
      "\n",
      "iter:8799/20000 loss:4.38573799419828e-05\n",
      "generated sequence: gone, or talk not, I advise you.\n",
      "\n",
      "HORTENSIO:\n",
      "Petruchio, patience; I am Grumio's pledge:\n",
      "Why, this's a heavy chance 'twixt him anpppppppppp\n",
      "\n",
      "iter:8999/20000 loss:3.785760703976848e-05\n",
      "generated sequence: \n",
      "BRUTUS:\n",
      "Lets along.\n",
      "\n",
      "First Senator:\n",
      "So, your opinion is, Aufidius,\n",
      "That they of Rome are entered in our counsels\n",
      "And know how wCCCCCCCCCC\n",
      "\n",
      "iter:9199/20000 loss:4.1333888457302234e-05\n",
      "generated sequence: ad not Grumio come by the worst.\n",
      "\n",
      "PETRUCHIO:\n",
      "A senseless villain! Good Hortensio,\n",
      "I bade the rascal knock upon your gate\n",
      "And coueeeeeeeeee\n",
      "\n",
      "iter:9399/20000 loss:3.0281791123343282e-05\n",
      "generated sequence: hat hath been limed in a bush,\n",
      "With trembling wings misdoubteth every bush;\n",
      "And I, the hapless male to one sweet bird,\n",
      "Have now bbbbbbbbbb\n",
      "\n",
      "iter:9599/20000 loss:2.9296793190951575e-05\n",
      "generated sequence: ste,\n",
      "To season love, that of it doth not taste!\n",
      "The sun not yet thy sighs from heaven clears,\n",
      "Thy old groans ring yet in my anciuuuuuuuuuu\n",
      "\n",
      "iter:9799/20000 loss:2.4945204495452343e-05\n",
      "generated sequence: yself.\n",
      "Why, cousin, wert thou regent of the world,\n",
      "It were a shame to let this land by lease;\n",
      "But for thy world enjoying but thitttttttttt\n",
      "\n",
      "iter:9999/20000 loss:2.5143959665001602e-05\n",
      "generated sequence: to-morrow as to-day,\n",
      "And to be boy eternal.\n",
      "\n",
      "HERMIONE:\n",
      "Was not my lord\n",
      "The verier wag o' the two?\n",
      "\n",
      "POLIXENES:\n",
      "We were as twinn'dpppppppppp\n",
      "\n",
      "iter:10199/20000 loss:2.2854376934446917e-05\n",
      "generated sequence: m as we rode? I think\n",
      "He told me Paris should have married Juliet:\n",
      "Said he not so? or did I dream it so?\n",
      "Or am I mad, hearing hi\t\t\t\t\t\t\t\t\t\t\n",
      "\n",
      "iter:10399/20000 loss:2.0724382798107398e-05\n",
      "generated sequence: , thou honourable\n",
      "man; prove it.\n",
      "\n",
      "ESCALUS:\n",
      "Do you hear how he misplaces?\n",
      "\n",
      "POMPEY:\n",
      "Sir, she came in great with child; and longing\t\t\t\t\t\t\t\t\t\t\n",
      "\n",
      "iter:10599/20000 loss:1.8199237379121767e-05\n",
      "generated sequence: IO:\n",
      "Nay then, I will not: you shall have the mustard,\n",
      "Or else you get no beef of Grumio.\n",
      "\n",
      "KATHARINA:\n",
      "Then both, or one, or any tPPPPPPPPPP\n",
      "\n",
      "iter:10799/20000 loss:1.6134309194058006e-05\n",
      "generated sequence: deeds on't.\n",
      "\n",
      "Time:\n",
      "I, that please some, try all, both joy and terror\n",
      "Of good and bad, that makes and unfolds error,\n",
      "Now take upoffffffffff\n",
      "\n",
      "iter:10999/20000 loss:0.10908199046921255\n",
      "generated sequence: diate on his knees:\n",
      "Which on thy royal party granted once,\n",
      "His glittering arms he will commend to rust,\n",
      "His barbed steeds to stajjjjjjjjjj\n",
      "\n",
      "iter:11199/20000 loss:0.0010266508209315361\n",
      "generated sequence:  to Claudio?\n",
      "\n",
      "DUKE VINCENTIO:\n",
      "O, 'tis an accident that heaven provides!\n",
      "Dispatch it presently; the hour draws on\n",
      "Prefix'd by Anguuuuuuuuuu\n",
      "\n",
      "iter:11399/20000 loss:0.0005797900435936754\n",
      "generated sequence: upper end o' the table, now i' the middle;\n",
      "On his shoulder, and his; her face o' fire\n",
      "With labour and the thing she took to quenqqqqqqqqqq\n",
      "\n",
      "iter:11599/20000 loss:0.00017399828762791004\n",
      "generated sequence: Come, sir page,\n",
      "Look on me with your welkin eye: sweet villain!\n",
      "Most dear'st! my collop! Can thy dam?--may't be?--\n",
      "Affection! thpppppppppp\n",
      "\n",
      "iter:11799/20000 loss:8.602119301940547e-05\n",
      "generated sequence: re he is falsely set;\n",
      "One that hath ever been God's enemy:\n",
      "Then, if you fight against God's enemy,\n",
      "God will in justice ward you ffffffffff\n",
      "\n",
      "iter:11999/20000 loss:0.00011944821026190766\n",
      "generated sequence: ch!\n",
      "To the most of men this is a Caliban\n",
      "And they to him are angels.\n",
      "\n",
      "MIRANDA:\n",
      "My affections\n",
      "Are then most humble; I have no ambiiiiiiiiii\n",
      "\n",
      "iter:12199/20000 loss:6.123413054410775e-05\n",
      "generated sequence: o thy house;\n",
      "Leave us to cure this cause.\n",
      "\n",
      "MENENIUS:\n",
      "For 'tis a sore upon us,\n",
      "You cannot tent yourself: be gone, beseech you.\n",
      "\n",
      "C\t\t\t\t\t\t\t\t\t\t\n",
      "\n",
      "iter:12399/20000 loss:4.470742927878746e-05\n",
      "generated sequence: is wisdom be\n",
      "not tainted! And why meet him at the gates, and\n",
      "redeliver our authorities there\n",
      "\n",
      "ESCALUS:\n",
      "I guess not.\n",
      "\n",
      "ANGELO:\n",
      "Andtttttttttt\n",
      "\n",
      "iter:12599/20000 loss:3.9087872246454935e-05\n",
      "generated sequence: ean\n",
      "But nature makes that mean: so, over that art\n",
      "Which you say adds to nature, is an art\n",
      "That nature makes. You see, sweet maidbbbbbbbbbb\n",
      "\n",
      "iter:12799/20000 loss:0.00025676658365227924\n",
      "generated sequence: D:\n",
      "Here come the Lords of Ross and Willoughby,\n",
      "Bloody with spurring, fiery-red with haste.\n",
      "\n",
      "HENRY BOLINGBROKE:\n",
      "Welcome, my lords;;;;;;;;;;\n",
      "\n",
      "iter:12999/20000 loss:3.2257379934890196e-05\n",
      "generated sequence: mely shepherd; a man, they say, that from\n",
      "very nothing, and beyond the imagination of his\n",
      "neighbours, is grown into an unspeakabffffffffff\n",
      "\n",
      "iter:13199/20000 loss:2.8771326678906917e-05\n",
      "generated sequence: world's exile is death: then banished,\n",
      "Is death mis-term'd: calling death banishment,\n",
      "Thou cutt'st my head off with a golden axepppppppppp\n",
      "\n",
      "iter:13399/20000 loss:2.3657132724110853e-05\n",
      "generated sequence: e, man; the hurt cannot be much.\n",
      "\n",
      "MERCUTIO:\n",
      "No, 'tis not so deep as a well, nor so wide as a\n",
      "church-door; but 'tis enough,'twill----------\n",
      "\n",
      "iter:13599/20000 loss:2.0394071307237026e-05\n",
      "generated sequence:  did we all. But, come, let's home.\n",
      "\n",
      "BRUTUS:\n",
      "I do not like this news.\n",
      "\n",
      "SICINIUS:\n",
      "Nor I.\n",
      "\n",
      "BRUTUS:\n",
      "Let's to the Capitol. Would haleeeeeeeeee\n",
      "\n",
      "iter:13799/20000 loss:1.9544270612641414e-05\n",
      "generated sequence: ese were her words, utter'd with mad disdain:\n",
      "'Tell him, in hope he'll prove a widower shortly,\n",
      "I'll wear the willow garland fortttttttttt\n",
      "\n",
      "iter:13999/20000 loss:0.00036928069178884473\n",
      "generated sequence: to thyself,\n",
      "From their abominable and beastly touches\n",
      "I drink, I eat, array myself, and live.\n",
      "Canst thou believe thy living is apppppppppp\n",
      "\n",
      "iter:14199/20000 loss:4.954934509441955e-05\n",
      "generated sequence: He is our subject, Mowbray; so art thou:\n",
      "Free speech and fearless I to thee allow.\n",
      "\n",
      "THOMAS MOWBRAY:\n",
      "Then, Bolingbroke, as low asffffffffff\n",
      "\n",
      "iter:14399/20000 loss:0.13079108363184333\n",
      "generated sequence: ms and ends\n",
      "Of burning youth.\n",
      "\n",
      "FRIAR THOMAS:\n",
      "May your grace speak of it?\n",
      "\n",
      "DUKE VINCENTIO:\n",
      "My holy sir, none better knows than yotttttttttt\n",
      "\n",
      "iter:14599/20000 loss:0.0018025780898460653\n",
      "generated sequence:  life.\n",
      "Apollo, pardon\n",
      "My great profaneness 'gainst thine oracle!\n",
      "I'll reconcile me to Polixenes,\n",
      "New woo my queen, recall the gommmmmmmmmm\n",
      "\n",
      "iter:14799/20000 loss:0.000290113360460964\n",
      "generated sequence: bolted\n",
      "By the northern blasts twice o'er.\n",
      "\n",
      "POLIXENES:\n",
      "What follows this?\n",
      "How prettily the young swain seems to wash\n",
      "The hand waspppppppppp\n",
      "\n",
      "iter:14999/20000 loss:0.00022007284715073184\n",
      "generated sequence: s I owe you\n",
      "a good turn.\n",
      "\n",
      "Provost:\n",
      "Call hither Barnardine and Claudio:\n",
      "The one has my pity; not a jot the other,\n",
      "Being a murdere\t\t\t\t\t\t\t\t\t\t\n",
      "\n",
      "iter:15199/20000 loss:0.00016106692244648003\n",
      "generated sequence:  run.\n",
      "\n",
      "AUTOLYCUS:\n",
      "I must confess to you, sir, I am no fighter: I am\n",
      "false of heart that way; and that he knew, I warrant\n",
      "him.\n",
      "\n",
      "Cssssssssss\n",
      "\n",
      "iter:15399/20000 loss:0.0002136728431833035\n",
      "generated sequence: h a beggar, though she smelt brown\n",
      "bread and garlic: say that I said so. Farewell.\n",
      "\n",
      "DUKE VINCENTIO:\n",
      "No might nor greatness in mo\t\t\t\t\t\t\t\t\t\t\n",
      "\n",
      "iter:15599/20000 loss:0.00035474377726131935\n",
      "generated sequence: her be patient, and entreat me fair,\n",
      "Or with the clamorous report of war\n",
      "Thus will I drown your exclamations.\n",
      "\n",
      "DUCHESS OF YORK:\n",
      "ffffffffff\n",
      "\n",
      "iter:15799/20000 loss:0.00040200967539931297\n",
      "generated sequence: rew as she's reported?\n",
      "\n",
      "GRUMIO:\n",
      "She was, good Curtis, before this frost: but, thou\n",
      "knowest, winter tames man, woman and beast; fffffffffff\n",
      "\n",
      "iter:15999/20000 loss:0.00029533517921663587\n",
      "generated sequence: entleman, nurse, that loves to hear himself talk,\n",
      "and will speak more in a minute than he will stand\n",
      "to in a month.\n",
      "\n",
      "Nurse:\n",
      "An aoooooooooo\n",
      "\n",
      "iter:16199/20000 loss:0.00015928552913464954\n",
      "generated sequence: mine!\n",
      "Long die thy happy days before thy death;\n",
      "And, after many lengthen'd hours of grief,\n",
      "Die neither mother, wife, nor Englandjjjjjjjjjj\n",
      "\n",
      "iter:16399/20000 loss:0.00010319602301933628\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generated sequence: ENCE:\n",
      "I must perforce. Farewell.\n",
      "\n",
      "GLOUCESTER:\n",
      "Go, tread the path that thou shalt ne'er return.\n",
      "Simple, plain Clarence! I do loveOOOOOOOOOO\n",
      "\n",
      "iter:16599/20000 loss:3.996661939709156e-05\n",
      "generated sequence: se on me;\n",
      "If to reprove you for this suit of yours,\n",
      "So season'd with your faithful love to me.\n",
      "Then, on the other side, I chequeffffffffff\n",
      "\n",
      "iter:16799/20000 loss:3.289875157861388e-05\n",
      "generated sequence: mportune me no farther,\n",
      "For how I firmly am resolved you know;\n",
      "That is, not bestow my youngest daughter\n",
      "Before I have a husband qqqqqqqqqq\n",
      "\n",
      "iter:16999/20000 loss:2.6997839599971486e-05\n",
      "generated sequence: though indeed\n",
      "In aught he merit not.\n",
      "\n",
      "SICINIUS:\n",
      "Let's hence, and hear\n",
      "How the dispatch is made, and in what fashion,\n",
      "More than hiiiiiiiiii\n",
      "\n",
      "iter:17199/20000 loss:2.3256165300153952e-05\n",
      "generated sequence: t alone:\n",
      "I will not go to-day; and ere I do,\n",
      "It shall be what o'clock I say it is.\n",
      "\n",
      "HORTENSIO:\n",
      "\n",
      "TRANIO:\n",
      "Sir, this is the house: \t\t\t\t\t\t\t\t\t\t\n",
      "\n",
      "iter:17399/20000 loss:2.1088307598802202e-05\n",
      "generated sequence: r queen--lo, fool again!--\n",
      "I'll speak of her no more, nor of your children;\n",
      "I'll not remember you of my own lord,\n",
      "Who is lost to\t\t\t\t\t\t\t\t\t\t\n",
      "\n",
      "iter:17599/20000 loss:2.1072725817248285e-05\n",
      "generated sequence: your general talk of Rome,\n",
      "And of his friends there, it is lots to blanks,\n",
      "My name hath touch'd your ears it is Menenius.\n",
      "\n",
      "Firstpppppppppp\n",
      "\n",
      "iter:17799/20000 loss:0.09350104860051943\n",
      "generated sequence: h her daughter.\n",
      "These letters will resolve him of my mind. Farewell.\n",
      "\n",
      "BUCKINGHAM:\n",
      "Will not King Richard let me speak with him?\n",
      "\n",
      "\t\t\t\t\t\t\t\t\t\t\n",
      "\n",
      "iter:17999/20000 loss:0.002937776934486465\n",
      "generated sequence: es; let us shame him with our knees.\n",
      "To his surname Coriolanus 'longs more pride\n",
      "Than pity to our prayers. Down: an end;\n",
      "This istttttttttt\n",
      "\n",
      "iter:18199/20000 loss:0.0006345482153483317\n",
      "generated sequence: mp out our drooping country's broken wing,\n",
      "Redeem from broking pawn the blemish'd crown,\n",
      "Wipe off the dust that hides our sceptrqqqqqqqqqq\n",
      "\n",
      "iter:18399/20000 loss:0.00017388622185535496\n",
      "generated sequence: arly and personally accused,\n",
      "Her shall you hear disproved to her eyes,\n",
      "Till she herself confess it.\n",
      "\n",
      "DUKE VINCENTIO:\n",
      "Good friar,ssssssssss\n",
      "\n",
      "iter:18599/20000 loss:0.00018934183091914747\n",
      "generated sequence: on stands it and wherein?\n",
      "\n",
      "DUKE OF YORK:\n",
      "Even in condition of the worst degree,\n",
      "In gross rebellion and detested treason:\n",
      "Thou aroooooooooo\n",
      "\n",
      "iter:18799/20000 loss:0.00011295189685370133\n",
      "generated sequence: al, princely Buckingham,\n",
      "is this thy vow unto my sickly heart.\n",
      "There wanteth now our brother Gloucester here,\n",
      "To make the perfecmmmmmmmmmm\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Number of iterations.\n",
    "iters       = 20000\n",
    "# Number of printing iterations\n",
    "print_iters = 200   \n",
    "\n",
    "# The loss variables.\n",
    "all_losses = []\n",
    "loss_sum   = 0\n",
    "\n",
    "# Initialize the optimizer and the loss function.\n",
    "opt       = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "loss_func = nn.CrossEntropyLoss()\n",
    "\n",
    "# Training procedure.\n",
    "for i in range(iters):\n",
    "    opt.zero_grad()\n",
    "    input, target = get_input_and_target()            # Fetch input and target.\n",
    "    input, target = input.to(device), target.to(device) # Move to GPU memory.\n",
    "    input = input[None] \n",
    "    target = target[None] \n",
    "    predicted = model(input,target)\n",
    "    predicted = predicted.permute(0,2,1)\n",
    "    loss = loss_func(predicted,target)\n",
    "    \n",
    "    loss.backward()\n",
    "    loss_sum += loss.item()                                # Accumulate the loss.\n",
    "    opt.step()\n",
    "\n",
    "    # Print the log.\n",
    "    if i % print_iters == print_iters - 1:\n",
    "        print('iter:{}/{} loss:{}'.format(i, iters, loss_sum / print_iters))\n",
    "        print('generated sequence: {}\\n'.format(generate_text(model,target,start_seq = input)))\n",
    "              \n",
    "        # Track the loss.\n",
    "        all_losses.append(loss_sum / print_iters)\n",
    "        loss_sum = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training Loss Curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-15T00:38:13.728474Z",
     "start_time": "2019-05-15T00:38:13.559531Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.xlabel('iters')\n",
    "plt.ylabel('loss')\n",
    "plt.plot(np.array(all_losses))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluation: A Sample of Generated Sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-15T03:10:52.267837Z",
     "start_time": "2019-05-15T03:10:51.986701Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "rand_input, rand_target = get_input_and_target()  \n",
    "rand_input = rand_input[None].to(device)\n",
    "rand_target = rand_target[None].to(device)\n",
    "generate_text(model,rand_target,start_seq = rand_input,gen_len=600)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis\n",
    "\n",
    "In attempting to printing the output of the current model, the main change was for the model to pass in the input twice,since the target gave away the answers. This prevented the model from generalizing to other input and thus had bad behavior in printing words."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
