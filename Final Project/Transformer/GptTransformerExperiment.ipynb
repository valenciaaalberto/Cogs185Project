{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "c34c2de2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import string\n",
    "import random\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53cee1e0",
   "metadata": {},
   "source": [
    "### Select Device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "e2a87fdb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac82165d",
   "metadata": {},
   "source": [
    "### Define Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "b0ff0151",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, embed_dim, dropout=0.1, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        pe = torch.zeros(max_len, embed_dim)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, embed_dim, 2).float() * (-math.log(10000.0) / embed_dim))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:x.size(0), :]\n",
    "        return self.dropout(x)\n",
    "\n",
    "class TransformerModel(nn.Module):\n",
    "    def __init__(self, input_dim, embed_dim, nhead, nhid, nlayers, output_dim, dropout=0.5):\n",
    "        super(TransformerModel, self).__init__()\n",
    "        self.model_type = 'Transformer'\n",
    "        self.embed_dim = embed_dim\n",
    "        self.pos_encoder = PositionalEncoding(embed_dim, dropout)\n",
    "        self.encoder = nn.Embedding(input_dim, embed_dim)\n",
    "        self.transformer = nn.Transformer(embed_dim, nhead, nlayers, nlayers, nhid, dropout)\n",
    "        self.decoder = nn.Linear(embed_dim, output_dim)\n",
    "        \n",
    "        self.init_weights()\n",
    "        \n",
    "    def init_weights(self):\n",
    "        initrange = 0.1\n",
    "        self.encoder.weight.data.uniform_(-initrange, initrange)\n",
    "        self.decoder.bias.data.zero_()\n",
    "        self.decoder.weight.data.uniform_(-initrange, initrange)\n",
    "        \n",
    "    def forward(self, src, src_mask):\n",
    "        src = src\n",
    "        src = self.encoder(src) * math.sqrt(self.embed_dim)\n",
    "        src = self.pos_encoder(src)\n",
    "        output = self.transformer(src, src, src_mask)\n",
    "        output = self.decoder(output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d6ea59e",
   "metadata": {},
   "source": [
    "### Process Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "60226d7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_chars = string.printable\n",
    "n_chars = len(all_chars)\n",
    "file = open('../Data/shakespeare.txt').read()\n",
    "file_len = len(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "3c98568c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_random_seq():\n",
    "    seq_len = 128  # The length of an input sequence.\n",
    "    start_index = random.randint(0, file_len - seq_len)\n",
    "    end_index = start_index + seq_len + 1\n",
    "    return file[start_index:end_index]\n",
    "\n",
    "def seq_to_onehot(seq):\n",
    "    tensor = torch.zeros(len(seq), n_chars, dtype=torch.long)\n",
    "    for t, char in enumerate(seq):\n",
    "        index = all_chars.index(char)\n",
    "        tensor[t][index] = 1.0\n",
    "    return tensor\n",
    "\n",
    "def seq_to_index(seq):\n",
    "    tensor = torch.zeros(len(seq), dtype=torch.long)\n",
    "    for t, char in enumerate(seq):\n",
    "        tensor[t] = all_chars.index(char)\n",
    "    return tensor\n",
    "\n",
    "def get_input_and_target():\n",
    "    seq = get_random_seq()\n",
    "    input = seq_to_index(seq[:-1])  # Input is represented in index.\n",
    "    target = seq_to_index(seq[1:])  # Target is represented in index.\n",
    "    return input, target\n",
    "\n",
    "def generate_square_subsequent_mask(sz):\n",
    "    mask = torch.triu(torch.ones(sz, sz) == 1).transpose(0, 1)\n",
    "    mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0)).to(device)\n",
    "    return mask"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfe1750b",
   "metadata": {},
   "source": [
    "### Text Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "befed9ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def generate_text(model, start_seq='W', max_len=100):\n",
    "#     model.eval()\n",
    "#     with torch.no_grad():\n",
    "#         input = seq_to_index(start_seq).unsqueeze(1)  # Shape: (seq_len, batch_size=1)\n",
    "#         generated = input.to(device)\n",
    "\n",
    "#         for _ in range(max_len):\n",
    "#             src_mask = generate_square_subsequent_mask(generated.size(0)).to(device)\n",
    "#             output = model(generated, src_mask)\n",
    "            \n",
    "#             # Select the last time step's output\n",
    "#             next_char_logits = output[-1, 0, :]\n",
    "#             next_char = torch.argmax(next_char_logits, dim=-1).unsqueeze(0).unsqueeze(1)  # Shape: (1, 1)\n",
    "            \n",
    "#             generated = torch.cat((generated, next_char), dim=0)  # Concatenate along the sequence dimension\n",
    "\n",
    "#         generated_seq = ''.join([all_chars[idx] for idx in generated.squeeze().tolist()])\n",
    "#     return generated_seq\n",
    "# gpt\n",
    "# def generate_text(model, start_seq='W', max_len=100):\n",
    "#     predicted_seq = start_seq\n",
    "# #     model.eval()\n",
    "# #     with torch.no_grad():\n",
    "#     init_input = seq_to_index(start_seq).to(device)\n",
    "#     input = init_input[-1]  # Shape: (seq_len, batch_size=1)\n",
    "#     for _ in range(max_len):\n",
    "#         src_mask = generate_square_subsequent_mask(input.size(0)).to(device)\n",
    "#         output = model(input, src_mask)\n",
    "#         predicted_index = torch.multinomial(output.view(-1).exp(), 1)[0]\n",
    "\n",
    "#         # Add predicted character to the sequence and use it as next input.\n",
    "#         predicted_char  = all_chars[predicted_index]\n",
    "#         predicted_seq  += predicted_char\n",
    "\n",
    "#         # Use the predicted character to generate the input of next round.\n",
    "#         input = seq_to_index(predicted_char)[0].to(device)\n",
    "#     return predicted_seq\n",
    "\n",
    "\n",
    "# Evaluation step function.\n",
    "# def generate_text(model, init_seq='W', predicted_len=100):\n",
    " \n",
    "#     init_input    = seq_to_onehot(init_seq).to(device)\n",
    "#     predicted_seq = init_seq\n",
    "\n",
    "#     # Set current input as the last character of the initial string.\n",
    "#     input = init_input[-1]\n",
    "    \n",
    "#     # Predict more characters after the initial string.\n",
    "#     for t in range(predicted_len):\n",
    "#         src_mask = generate_square_subsequent_mask(input.size(0)).to(device)\n",
    "#         output = model(input, src_mask)\n",
    "        \n",
    "#         # Sample from the output as a multinomial distribution.\n",
    "#         output = output.view(-1, output_dim)\n",
    "#         print(output.shape)\n",
    "#         predicted_index = torch.multinomial(output.view(-1).exp(), 1)[0]\n",
    "#         print(predicted_index)\n",
    "        \n",
    "#         # Add predicted character to the sequence and use it as next input.\n",
    "#         predicted_char  = all_chars[predicted_index]\n",
    "#         predicted_seq  += predicted_char\n",
    "        \n",
    "#         # Use the predicted character to generate the input of next round.\n",
    "#         input = seq_to_onehot(predicted_char)[0].to(device)\n",
    "\n",
    "#     return predicted_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "37b0c64b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(model, start_seq='W', gen_len=100,temperature=1.0):\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    \n",
    "    # Convert the starting sequence to indices\n",
    "#     init_input = seq_to_index(start_seq).unsqueeze(1).to(device)\n",
    "    input_seq = seq_to_index(start_seq).unsqueeze(1).to(device)  # Add batch dimension, shape: (seq_len, 1)\n",
    "    \n",
    "    generated_text = start_seq\n",
    "    \n",
    "    for _ in range(gen_len):\n",
    "        src_mask = generate_square_subsequent_mask(input_seq.size(0)).to(device)\n",
    "        with torch.no_grad():\n",
    "            output = model(input_seq, src_mask)\n",
    "        \n",
    "        # Get the last output and convert to probabilities\n",
    "        next_char_logits = output[-1, 0, :] / temperature   # Shape: (output_dim)\n",
    "        next_char_probs = torch.softmax(next_char_logits, dim=-1)\n",
    "  \n",
    "        # Sample the next character\n",
    "        # predicted_char_index = np.random.choice(len(all_chars), p=next_char_probs)\n",
    "        # predicted_char_index = np.random.multinomial(len(all_chars),next_char_probs)\n",
    "        # predicted_char_index = torch.argmax(next_char_probs)\n",
    "        predicted_char_index = torch.multinomial(next_char_probs, num_samples=1)[0]\n",
    "        predicted_char = all_chars[predicted_char_index]\n",
    "        \n",
    "        generated_text += predicted_char\n",
    "        \n",
    "        # Append the next character to the input sequence\n",
    "        next_char_tensor = torch.tensor([predicted_char_index], dtype=torch.long).unsqueeze(1).to(device)\n",
    "        input_seq = torch.cat([input_seq, next_char_tensor], dim=0)\n",
    "    \n",
    "    return generated_text\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "912637f5",
   "metadata": {},
   "source": [
    "## Design choices\n",
    "\n",
    "I had chat gpt help me write  generate text function. Everything seemed correct except that it wanted to use np.random.choice(len(all_chars), p=next_char_probs) to predict the next char. I instead opted to use softmax and then argmax to select the index of the highest probability. However, I noticed my loss going down and the output generated by the model was not any better. I asked chat gpt with my new implementation and suggested to use torch multinomial as it selects from a probability distribution. Given that we used this approach in class, I went with this approach. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35ddbc34",
   "metadata": {},
   "source": [
    "This stack overflow discussion helped resolve my error with probabilities do not sum to 1: https://stackoverflow.com/questions/46539431/np-random-choice-probabilities-do-not-sum-to-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "f98f852b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TransformerModel(\n",
       "  (pos_encoder): PositionalEncoding(\n",
       "    (dropout): Dropout(p=0.2, inplace=False)\n",
       "  )\n",
       "  (encoder): Embedding(512, 128)\n",
       "  (transformer): Transformer(\n",
       "    (encoder): TransformerEncoder(\n",
       "      (layers): ModuleList(\n",
       "        (0-1): 2 x TransformerEncoderLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
       "          )\n",
       "          (linear1): Linear(in_features=128, out_features=256, bias=True)\n",
       "          (dropout): Dropout(p=0.2, inplace=False)\n",
       "          (linear2): Linear(in_features=256, out_features=128, bias=True)\n",
       "          (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout1): Dropout(p=0.2, inplace=False)\n",
       "          (dropout2): Dropout(p=0.2, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (decoder): TransformerDecoder(\n",
       "      (layers): ModuleList(\n",
       "        (0-1): 2 x TransformerDecoderLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
       "          )\n",
       "          (multihead_attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
       "          )\n",
       "          (linear1): Linear(in_features=128, out_features=256, bias=True)\n",
       "          (dropout): Dropout(p=0.2, inplace=False)\n",
       "          (linear2): Linear(in_features=256, out_features=128, bias=True)\n",
       "          (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm3): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout1): Dropout(p=0.2, inplace=False)\n",
       "          (dropout2): Dropout(p=0.2, inplace=False)\n",
       "          (dropout3): Dropout(p=0.2, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       "  (decoder): Linear(in_features=128, out_features=100, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_dim = 512\n",
    "embed_dim = 128\n",
    "nhead = 2\n",
    "nhid = 256\n",
    "nlayers = 2\n",
    "output_dim = len(all_chars)\n",
    "dropout = 0.2\n",
    "\n",
    "model = TransformerModel(input_dim, embed_dim, nhead, nhid, nlayers, output_dim, dropout)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab5e0848",
   "metadata": {},
   "source": [
    "### Training Cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "9769d753",
   "metadata": {},
   "outputs": [],
   "source": [
    "# criterion = nn.CrossEntropyLoss()\n",
    "# optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "# iters = 100\n",
    "# all_losses = []\n",
    "# epochs = 40\n",
    "# for epoch in range(epochs):\n",
    "#     model.train()\n",
    "#     total_loss = 0.\n",
    "#     for _ in range(iters):  # Adjust the number of batches\n",
    "#         input, target = get_input_and_target()\n",
    "#         # Add batch dimension, shape: (seq_len, batch_size=1)\n",
    "#         input = input.unsqueeze(1).to(device)\n",
    "#         target = target.unsqueeze(1).to(device)\n",
    "     \n",
    "#         src_mask = generate_square_subsequent_mask(input.size(0)).to(device)\n",
    "        \n",
    "#         optimizer.zero_grad()\n",
    "#         output = model(input, src_mask)\n",
    "        \n",
    "#         # Reshape output to (seq_len * batch_size, output_dim) and target to (seq_len * batch_size)\n",
    "#         output = output.view(-1, output_dim)\n",
    "#         target = target.view(-1)\n",
    "        \n",
    "#         loss = criterion(output, target)\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "        \n",
    "#         total_loss += loss.item()\n",
    "        \n",
    "#     all_losses.append(total_loss / iters)\n",
    "#     print(f'Epoch {epoch+1}, Loss: {total_loss / iters}')\n",
    "#     print(\"Generated output:\", generate_text(model))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8097c5f7",
   "metadata": {},
   "source": [
    "### Training with Scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "f36a0933",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 3.056742306947708\n",
      "Generated output: Wnde ilof s otouky th or I\n",
      "\n",
      "_LOETulous:\n",
      "TUWhas,\n",
      "OCisofosis he  af ar paly s fouls\n",
      "ASoficlniare or al \n",
      "Epoch 2, Loss: 2.6466616547107695\n",
      "Generated output: Wou myoutaisoano odfe avere at therouind sinound S n prirerve:\n",
      "Gounggonison neatha ouriniakofie loutr\n",
      "Epoch 3, Loss: 2.589185825586319\n",
      "Generated output: WA:\n",
      "K:N:,\n",
      "DIUIREI te ar y T: ICA:\n",
      "'bakn,\n",
      " sove bthar beple, thetomendisil b,\n",
      "\n",
      "LELI O che, ce k he se \n",
      "Epoch 4, Loss: 2.544263023138046\n",
      "Generated output: With l c or echas\n",
      "HerelI th n FI mes youe\n",
      "Itoucus hy mus y nt bulait, s abe\n",
      "Y:\n",
      "?\n",
      "\n",
      "\n",
      "UCPUS:\n",
      "NGDou I t n\n",
      "Epoch 5, Loss: 2.4292625427246093\n",
      "Generated output: WIIwIw ww,\n",
      "E:\n",
      ".\n",
      "\n",
      "\n",
      "NER:\n",
      "RUKIU:\n",
      "I.\n",
      "\n",
      "A:\n",
      "\n",
      "MIORIOES:\n",
      "\n",
      "I.\n",
      "HIs c.\n",
      "\n",
      "\n",
      "\n",
      "OLOLO.\n",
      "\n",
      "\n",
      "\n",
      "BER:\n",
      "MEEROLURO:\n",
      "NIURLIIS:\n",
      "H.\n",
      "\n",
      "Epoch 6, Loss: 1.9610467201471329\n",
      "Generated output: W\n",
      "WALIRIOM\n",
      "\n",
      "K?\n",
      "\n",
      "CHERDORLEORHYDDVERY:\n",
      "CRYSCHSRC+\n",
      "\n",
      "CPUS:\n",
      "YCv.\n",
      "WRDCFHM\n",
      "D:\n",
      "GxI WILIOOSSERRCHOMLERIO:MENRY\n",
      "Epoch 7, Loss: 1.2456202119588853\n",
      "Generated output: W W Ww,,,,,,,,!!!,,,,!,w,!I!;,,,,,,,,,,,,,,,,,,,,,!s,,,,,,,,,,,,,, o,,,,,,,,,, , t, , , t, , , , , s,\n",
      "Epoch 8, Loss: 0.9066299417614937\n",
      "Generated output: Wt t tstssssssssssssssssssssssssssssssssssssssss  s s s  s s s ssssssspsssssesss s ststststststststsi\n",
      "Epoch 9, Loss: 0.8107146838307381\n",
      "Generated output: WWWW w wwwwwwwwwwWWWWWWWWWWWWFWWWWWWWWWWWWWWWWWWWWWWWWWWWWWWWW\n",
      "W\n",
      "WDWWttWttttttttttttttu W,,,,,,,,,,!\n",
      "\n",
      "Epoch 10, Loss: 0.7787899139523506\n",
      "Generated output: WWWWWBCPPP?\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "CECERUCUCERMEREtCERURURURURURUKUKEVUCUCUCRYCKE CERCELEDUCUCUCUFUMU.\n",
      "MKEKEKEKICECK\n",
      "Epoch 11, Loss: 0.7310414992272853\n",
      "Generated output: WWWWWWWWWWWW Wssssss sssssssssssssssssssssssssstststisisis ssssssossssstststststststststststsisssists\n",
      "Epoch 12, Loss: 0.7150476944446563\n",
      "Generated output: WWWWW W WTTTTTTTTTTTTTTTTTTTTtttttttataaatatatatat t t t t e T T T T T T T T T T T T T T T T TTTTT T \n",
      "Epoch 13, Loss: 0.7008519415557385\n",
      "Generated output: WWW WjS:::::::::!\n",
      " ! ? ! WI, I,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,?,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,\n",
      "Epoch 14, Loss: 0.6790111483633519\n",
      "Generated output: WWWWWWWWWWW W Wo W W WoWoWoWoWoWotot titititititititititititititititititititititititititititititititi\n",
      "Epoch 15, Loss: 0.6637492628395557\n",
      "Generated output: WWWW W Woooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooofoooooo\n",
      "Epoch 16, Loss: 0.6370166318118572\n",
      "Generated output: WWWFFFFF\n",
      "iviviviuiviviviviviviHHHHHHHHHHHHGHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHKH\n",
      "Epoch 17, Loss: 0.6505876787006855\n",
      "Generated output: W\n",
      "W!\n",
      "\n",
      "\n",
      "F\n",
      "W\n",
      "W\n",
      "W\n",
      "W!\n",
      "W.\n",
      "W:\n",
      "W:\n",
      "W:\n",
      "W:\n",
      "W:\n",
      "W:\n",
      "W?\n",
      "W?\n",
      "W:\n",
      "W:\n",
      "Y:\n",
      "W:\n",
      "W:\n",
      "W:\n",
      "\n",
      "W:\n",
      "W:\n",
      "W:\n",
      "W:\n",
      "W:\n",
      "W:\n",
      "W:\n",
      "W:\n",
      "D:\n",
      "W:\n",
      "W:\n",
      "W:\n",
      "W\n",
      "Epoch 18, Loss: 0.6426581791043282\n",
      "Generated output: WW\n",
      "WWWWWWWWWWWWWWW\n",
      "OoWoWo W WoSoSoS!\n",
      "SSSoSo\n",
      "\n",
      "\n",
      "\n",
      "WoWoWoWo WoWoWoWoWo WoWoWotototototctotstststctttttttt\n",
      "Epoch 19, Loss: 0.6489806786179543\n",
      "Generated output: WWWWWW\n",
      "W\n",
      "WWWWWW W W W t t t W W W W W W W W W W W W W W, W W W, W, W W, , W, W, W, , W, W, W, Wus, W,\n",
      "Epoch 20, Loss: 0.639906468987465\n",
      "Generated output: WWW W W W W WWWWu W W W W WWWWW W WWWWW W W W W W W W W W W W W u W W W W t s W W W W W W W W W W W W\n",
      "Epoch 21, Loss: 0.6390509520471096\n",
      "Generated output: WWWWWW W W W W W W W W W W W W W W W W W W W W W W, W,,,, , W, W, W? W W, W, W, W, W, W, W,,, W, w, W\n",
      "Epoch 22, Loss: 0.6435233601927757\n",
      "Generated output: WWWWWWWWWWWW,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,\n",
      "Epoch 23, Loss: 0.6310897600650788\n",
      "Generated output: WWLLLLLL\n",
      "\n",
      "D\n",
      "DDDD\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "D:\n",
      "D\n",
      "F\n",
      "F.\n",
      "\n",
      "D D D D D D D I I I I I I VIVICICICICECECECEQUCEINCERERERRD:\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Epoch 24, Loss: 0.6383752845227718\n",
      "Generated output: WWWWWWWWDDCSaSaSaSaSa\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "SSSeteeeeeetttttttttttttttttttttttttttttttttttttttt ttttttttttttyttttttttt\n",
      "Epoch 25, Loss: 0.6497759725153446\n",
      "Generated output: WWWWWWWWWWWWWWWWWWWWWWWWWWWW\n",
      "W\n",
      "W\n",
      "W\n",
      "W\n",
      "W\n",
      "W\n",
      "W\n",
      "W\n",
      "W\n",
      "W\n",
      "W\n",
      "W\n",
      "W\n",
      "W\n",
      "W\n",
      "W\n",
      "W\n",
      "W\n",
      "W\n",
      "W\n",
      "W\n",
      "W\n",
      "W\n",
      "W\n",
      "W\n",
      "W\n",
      "W\n",
      "W\n",
      "W\n",
      "W\n",
      "W\n",
      "W\n",
      "WWWWW\n",
      "W\n",
      "\n",
      "Epoch 26, Loss: 0.6251411172747612\n",
      "Generated output: WWWWWA\n",
      "\n",
      "\n",
      "Wwwwwwww,T,\n",
      "WWWWW,\n",
      "\n",
      "Wwww,,,,,,!!??\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "W?\n",
      "W?\n",
      "W?\n",
      "W?\n",
      "W?\n",
      "W!\n",
      "W?\n",
      "W?\n",
      "\n",
      "W?\n",
      "W7fof,\n",
      "Wh,\n",
      "WIRIRIRINININI\n",
      "Epoch 27, Loss: 0.6406243939697742\n",
      "Generated output: WWWWWWWWWWWW mlllllllllll  W W''lllllllllllllllllllllllllllllllwllllllllllllllllllllllll'llllllllllll\n",
      "Epoch 28, Loss: 0.6405725458264351\n",
      "Generated output: WWWWWWWWW WWWWWWWWWWWW WHHHHHHHHHHHHHHHHHHHHKHHHHHHHHHHHHHHHKHKHKHKHHHHKKHKHHHHKHKHHHHHHHHHHHHHHHHHHH\n",
      "Epoch 29, Loss: 0.6273159097135067\n",
      "Generated output: WWWWWWWsssssssssssssssssssssssssssssssssssssssss s ssssssssssssssssssssssssssssssssssssssssssssssssss\n",
      "Epoch 30, Loss: 0.6459997560083867\n",
      "Generated output: WWWWWWWWWWWWWWWWWWWIWI\n",
      "W\n",
      "W\n",
      "W\n",
      "W\n",
      "W\n",
      "W\n",
      "W\n",
      "W\n",
      "S\n",
      "S\n",
      "F\n",
      "\n",
      "\n",
      "W\n",
      "W\n",
      "W\n",
      "W\n",
      "W\n",
      "W?\n",
      "W\n",
      "W\n",
      "W?\n",
      "W\n",
      "W\n",
      "W:\n",
      "W-\n",
      "WAANAN\n",
      "WAVALILILIDI?\n",
      "\n",
      "W\n",
      "\n",
      "Epoch 31, Loss: 0.6399558955430984\n",
      "Generated output: WWWWWWWWWWWWWDDVOffffffffffofffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffcff f ff\n",
      "Epoch 32, Loss: 0.6402967534959316\n",
      "Generated output: WAHAHHHHHHHHHHHHHHHHHHHHHHHHHH\n",
      "H\n",
      "HVHHHHHHHHH H H G H H H H H HW\n",
      "\n",
      "\n",
      "H]rww\n",
      "W-\n",
      "CHHHHHHBHBHHHHHH\n",
      "H\n",
      "H\n",
      "HHHH\n",
      "\n",
      "Epoch 33, Loss: 0.643789494484663\n",
      "Generated output: WWWWWWWWWWWWDDDWWWWWWWWWWWWWWWWWWWWWWWWWW'WA W WUWAWAWADAOSTWAWASSSSSSSSSSSSSSSSSCSESESSSSS\n",
      "D\n",
      "WxSSSSS\n",
      "Epoch 34, Loss: 0.641365012973547\n",
      "Generated output: WWWWWWWWWWWWWWWWWWWWWWW W W W W W W W 4e u W W W W W W W W W W W W W W W W W W F W W W W W W W Woto W\n",
      "Epoch 35, Loss: 0.6304540051519871\n",
      "Generated output: WWWWWWWWWWWDDOLOLOLOLOLOLOLOLOLOLOLOROROROROROROROSORYORORORIORHORDDOHORORORDORORORDCORORORORKRKRDORE\n",
      "Epoch 36, Loss: 0.6306237185001373\n",
      "Generated output: WWW\n",
      "WWWWWWWW\n",
      "W\n",
      "W\n",
      "W\n",
      "W\n",
      "W\n",
      "WWWWWW.\n",
      "\n",
      "W.\n",
      "W\n",
      "W\n",
      "W.\n",
      "W\n",
      "W\n",
      "W\n",
      "W\n",
      "W\n",
      "W W W!\n",
      "W.\n",
      "W?\n",
      "W?\n",
      "W:\n",
      "W?\n",
      "W:\n",
      "W:\n",
      "W:\n",
      "W:\n",
      "W:\n",
      "W:\n",
      "W\n",
      "WSTSTST\n",
      "Epoch 37, Loss: 0.6421048949658871\n",
      "Generated output: WNKKKKKKKKKKKHKKKKKKKKKKKKKK\n",
      "KMMMMMKKKKKKKKKKKKKKMmmmmmmjmmmmmmmmmmmmm% m m m m m m m m m m M M M M M\n",
      "Epoch 38, Loss: 0.63806511297822\n",
      "Generated output: WWWWWWWWWWWWWWWWWWWWWWWWWWWHHHHHHHHHHHHHHHHHHHHHHHHHKKHHHHQUKRKHKRKUKHK\n",
      "K\n",
      "KVG\n",
      "\n",
      "\n",
      "K\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "F\n",
      "\n",
      "\n",
      "\n",
      "M\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "W\n",
      "\n",
      "Epoch 39, Loss: 0.6457722426950931\n",
      "Generated output: WWWWWWWWWW W W W W W W W W W \n",
      "W, W, , W, W, W,, W, W, W, F W, W, W, W, W, W,  W W, w,, W, W, WWW! W, \n",
      "Epoch 40, Loss: 0.6361854204535484\n",
      "Generated output: WWWWWWWWWWWWWWWWWWWWSSDOSOSSSSSSSSSSSSSSSSDSDSSSSSSSSSYYYSSSSSSSSSSSSSSSSSSSSSSOSOSOSOSOSoSoSoSoSoSoS\n"
     ]
    }
   ],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "exp_lr_scheduler = optim.lr_scheduler.StepLR(optimizer,step_size=7, gamma=0.1)\n",
    "iters = 200\n",
    "all_losses = []\n",
    "epochs = 40\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    total_loss = 0.\n",
    "    for _ in range(iters):  # Adjust the number of batches\n",
    "        input, target = get_input_and_target()\n",
    "        # Add batch dimension, shape: (seq_len, batch_size=1)\n",
    "        input = input.unsqueeze(1).to(device)\n",
    "        target = target.unsqueeze(1).to(device)\n",
    "     \n",
    "        src_mask = generate_square_subsequent_mask(input.size(0)).to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        output = model(input, src_mask)\n",
    "        \n",
    "        # Reshape output to (seq_len * batch_size, output_dim) and target to (seq_len * batch_size)\n",
    "        output = output.view(-1, output_dim)\n",
    "        target = target.view(-1)\n",
    "        \n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        \n",
    "    all_losses.append(total_loss / iters)\n",
    "    print(f'Epoch {epoch+1}, Loss: {total_loss / iters}')\n",
    "    print(\"Generated output:\", generate_text(model))\n",
    "    exp_lr_scheduler.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "822b6f9f",
   "metadata": {},
   "source": [
    "### Reason for Step\n",
    "\n",
    "Noticing that the loss decreased, but text quality decreased, I asked chat-gpt. Could potentially be due to overfitting, so it suggested adding a step scheduler\n",
    "\n",
    "Used this link to implement step: https://discuss.pytorch.org/t/what-does-scheduler-step-do/47764"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c77d7c2",
   "metadata": {},
   "source": [
    "### Loss Curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "57cf0f82",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.xlabel('iters')\n",
    "plt.ylabel('loss')\n",
    "all_losses = [loss for loss in all_losses]\n",
    "plt.plot(np.array(all_losses))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "29b09fb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input\n",
      " so shriek abroad?\n",
      "\n",
      "LADY CAPULET:\n",
      "The people in the street cry Romeo,\n",
      "Some Juliet, and some Paris; and all run,\n",
      "With open outcry t\n",
      "Output\n",
      "so shriek abroad?\n",
      "\n",
      "LADY CAPULET:\n",
      "The people in the street cry Romeo,\n",
      "Some Juliet, and some Paris; and all run,\n",
      "With open outcry t, wioutht lye t toust ise thoun, athou. t the pnopo t nt un, tout y yon My, jureny gel to pounom, nt\n"
     ]
    }
   ],
   "source": [
    "rand_input = get_random_seq() \n",
    "print(\"Input\\n\",rand_input)\n",
    "print(\"Output\")\n",
    "print(generate_text(model, rand_input))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.1.undefined"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
