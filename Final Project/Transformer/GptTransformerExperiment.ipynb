{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "c34c2de2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import string\n",
    "import random\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53cee1e0",
   "metadata": {},
   "source": [
    "### Select Device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "e2a87fdb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac82165d",
   "metadata": {},
   "source": [
    "### Define Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "b0ff0151",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, embed_dim, dropout=0.1, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        pe = torch.zeros(max_len, embed_dim)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, embed_dim, 2).float() * (-math.log(10000.0) / embed_dim))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:x.size(0), :]\n",
    "        return self.dropout(x)\n",
    "\n",
    "class TransformerModel(nn.Module):\n",
    "    def __init__(self, input_dim, embed_dim, nhead, nhid, nlayers, output_dim, dropout=0.5):\n",
    "        super(TransformerModel, self).__init__()\n",
    "        self.model_type = 'Transformer'\n",
    "        self.embed_dim = embed_dim\n",
    "        self.pos_encoder = PositionalEncoding(embed_dim, dropout)\n",
    "        self.encoder = nn.Embedding(input_dim, embed_dim)\n",
    "        self.transformer = nn.Transformer(embed_dim, nhead, nlayers, nlayers, nhid, dropout)\n",
    "        self.decoder = nn.Linear(embed_dim, output_dim)\n",
    "        \n",
    "        self.init_weights()\n",
    "        \n",
    "    def init_weights(self):\n",
    "        initrange = 0.1\n",
    "        self.encoder.weight.data.uniform_(-initrange, initrange)\n",
    "        self.decoder.bias.data.zero_()\n",
    "        self.decoder.weight.data.uniform_(-initrange, initrange)\n",
    "        \n",
    "    def forward(self, src, src_mask):\n",
    "        src = src\n",
    "        src = self.encoder(src) * math.sqrt(self.embed_dim)\n",
    "        src = self.pos_encoder(src)\n",
    "        output = self.transformer(src, src, src_mask)\n",
    "        output = self.decoder(output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d6ea59e",
   "metadata": {},
   "source": [
    "### Process Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "60226d7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_chars = string.printable\n",
    "n_chars = len(all_chars)\n",
    "file = open('../Data/shakespeare.txt').read()\n",
    "file_len = len(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "3c98568c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_random_seq():\n",
    "    seq_len = 128  # The length of an input sequence.\n",
    "    start_index = random.randint(0, file_len - seq_len)\n",
    "    end_index = start_index + seq_len + 1\n",
    "    return file[start_index:end_index]\n",
    "\n",
    "def seq_to_onehot(seq):\n",
    "    tensor = torch.zeros(len(seq), n_chars, dtype=torch.long)\n",
    "    for t, char in enumerate(seq):\n",
    "        index = all_chars.index(char)\n",
    "        tensor[t][index] = 1.0\n",
    "    return tensor\n",
    "\n",
    "def seq_to_index(seq):\n",
    "    tensor = torch.zeros(len(seq), dtype=torch.long)\n",
    "    for t, char in enumerate(seq):\n",
    "        tensor[t] = all_chars.index(char)\n",
    "    return tensor\n",
    "\n",
    "def get_input_and_target():\n",
    "    seq = get_random_seq()\n",
    "    input = seq_to_index(seq[:-1])  # Input is represented in index.\n",
    "    target = seq_to_index(seq[1:])  # Target is represented in index.\n",
    "    return input, target\n",
    "\n",
    "def generate_square_subsequent_mask(sz):\n",
    "    mask = torch.triu(torch.ones(sz, sz) == 1).transpose(0, 1)\n",
    "    mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0)).to(device)\n",
    "    return mask"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfe1750b",
   "metadata": {},
   "source": [
    "### Text Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "befed9ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def generate_text(model, start_seq='W', max_len=100):\n",
    "#     model.eval()\n",
    "#     with torch.no_grad():\n",
    "#         input = seq_to_index(start_seq).unsqueeze(1)  # Shape: (seq_len, batch_size=1)\n",
    "#         generated = input.to(device)\n",
    "\n",
    "#         for _ in range(max_len):\n",
    "#             src_mask = generate_square_subsequent_mask(generated.size(0)).to(device)\n",
    "#             output = model(generated, src_mask)\n",
    "            \n",
    "#             # Select the last time step's output\n",
    "#             next_char_logits = output[-1, 0, :]\n",
    "#             next_char = torch.argmax(next_char_logits, dim=-1).unsqueeze(0).unsqueeze(1)  # Shape: (1, 1)\n",
    "            \n",
    "#             generated = torch.cat((generated, next_char), dim=0)  # Concatenate along the sequence dimension\n",
    "\n",
    "#         generated_seq = ''.join([all_chars[idx] for idx in generated.squeeze().tolist()])\n",
    "#     return generated_seq\n",
    "# gpt\n",
    "# def generate_text(model, start_seq='W', max_len=100):\n",
    "#     predicted_seq = start_seq\n",
    "# #     model.eval()\n",
    "# #     with torch.no_grad():\n",
    "#     init_input = seq_to_index(start_seq).to(device)\n",
    "#     input = init_input[-1]  # Shape: (seq_len, batch_size=1)\n",
    "#     for _ in range(max_len):\n",
    "#         src_mask = generate_square_subsequent_mask(input.size(0)).to(device)\n",
    "#         output = model(input, src_mask)\n",
    "#         predicted_index = torch.multinomial(output.view(-1).exp(), 1)[0]\n",
    "\n",
    "#         # Add predicted character to the sequence and use it as next input.\n",
    "#         predicted_char  = all_chars[predicted_index]\n",
    "#         predicted_seq  += predicted_char\n",
    "\n",
    "#         # Use the predicted character to generate the input of next round.\n",
    "#         input = seq_to_index(predicted_char)[0].to(device)\n",
    "#     return predicted_seq\n",
    "\n",
    "\n",
    "# Evaluation step function.\n",
    "# def generate_text(model, init_seq='W', predicted_len=100):\n",
    " \n",
    "#     init_input    = seq_to_onehot(init_seq).to(device)\n",
    "#     predicted_seq = init_seq\n",
    "\n",
    "#     # Set current input as the last character of the initial string.\n",
    "#     input = init_input[-1]\n",
    "    \n",
    "#     # Predict more characters after the initial string.\n",
    "#     for t in range(predicted_len):\n",
    "#         src_mask = generate_square_subsequent_mask(input.size(0)).to(device)\n",
    "#         output = model(input, src_mask)\n",
    "        \n",
    "#         # Sample from the output as a multinomial distribution.\n",
    "#         output = output.view(-1, output_dim)\n",
    "#         print(output.shape)\n",
    "#         predicted_index = torch.multinomial(output.view(-1).exp(), 1)[0]\n",
    "#         print(predicted_index)\n",
    "        \n",
    "#         # Add predicted character to the sequence and use it as next input.\n",
    "#         predicted_char  = all_chars[predicted_index]\n",
    "#         predicted_seq  += predicted_char\n",
    "        \n",
    "#         # Use the predicted character to generate the input of next round.\n",
    "#         input = seq_to_onehot(predicted_char)[0].to(device)\n",
    "\n",
    "#     return predicted_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "37b0c64b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(model, start_seq='W', gen_len=100,temperature=1.0):\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    \n",
    "    # Convert the starting sequence to indices\n",
    "#     init_input = seq_to_index(start_seq).unsqueeze(1).to(device)\n",
    "    input_seq = seq_to_index(start_seq).unsqueeze(1).to(device)  # Add batch dimension, shape: (seq_len, 1)\n",
    "    \n",
    "    generated_text = start_seq\n",
    "    \n",
    "    for _ in range(gen_len):\n",
    "        src_mask = generate_square_subsequent_mask(input_seq.size(0)).to(device)\n",
    "        with torch.no_grad():\n",
    "            output = model(input_seq, src_mask)\n",
    "        \n",
    "        # Get the last output and convert to probabilities\n",
    "        next_char_logits = output[-1, 0, :] / temperature   # Shape: (output_dim)\n",
    "        next_char_probs = torch.softmax(next_char_logits, dim=-1)\n",
    "  \n",
    "        # Sample the next character\n",
    "        # predicted_char_index = np.random.choice(len(all_chars), p=next_char_probs)\n",
    "        # predicted_char_index = np.random.multinomial(len(all_chars),next_char_probs)\n",
    "        # predicted_char_index = torch.argmax(next_char_probs)\n",
    "        predicted_char_index = torch.multinomial(next_char_probs, num_samples=1)[0]\n",
    "        predicted_char = all_chars[predicted_char_index]\n",
    "        \n",
    "        generated_text += predicted_char\n",
    "        \n",
    "        # Append the next character to the input sequence\n",
    "        next_char_tensor = torch.tensor([predicted_char_index], dtype=torch.long).unsqueeze(1).to(device)\n",
    "        input_seq = torch.cat([input_seq, next_char_tensor], dim=0)\n",
    "    \n",
    "    return generated_text\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "912637f5",
   "metadata": {},
   "source": [
    "## Design choices\n",
    "\n",
    "I had chat gpt help me write  generate text function. Everything seemed correct except that it wanted to use np.random.choice(len(all_chars), p=next_char_probs) to predict the next char. I instead opted to use softmax and then argmax to select the index of the highest probability. However, I noticed my loss going down and the output generated by the model was not any better. I asked chat gpt with my new implementation and suggested to use torch multinomial as it selects from a probability distribution. Given that we used this approach in class, I went with this approach. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35ddbc34",
   "metadata": {},
   "source": [
    "This stack overflow discussion helped resolve my error with probabilities do not sum to 1: https://stackoverflow.com/questions/46539431/np-random-choice-probabilities-do-not-sum-to-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "f98f852b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TransformerModel(\n",
       "  (pos_encoder): PositionalEncoding(\n",
       "    (dropout): Dropout(p=0.2, inplace=False)\n",
       "  )\n",
       "  (encoder): Embedding(512, 128)\n",
       "  (transformer): Transformer(\n",
       "    (encoder): TransformerEncoder(\n",
       "      (layers): ModuleList(\n",
       "        (0-1): 2 x TransformerEncoderLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
       "          )\n",
       "          (linear1): Linear(in_features=128, out_features=256, bias=True)\n",
       "          (dropout): Dropout(p=0.2, inplace=False)\n",
       "          (linear2): Linear(in_features=256, out_features=128, bias=True)\n",
       "          (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout1): Dropout(p=0.2, inplace=False)\n",
       "          (dropout2): Dropout(p=0.2, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (decoder): TransformerDecoder(\n",
       "      (layers): ModuleList(\n",
       "        (0-1): 2 x TransformerDecoderLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
       "          )\n",
       "          (multihead_attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
       "          )\n",
       "          (linear1): Linear(in_features=128, out_features=256, bias=True)\n",
       "          (dropout): Dropout(p=0.2, inplace=False)\n",
       "          (linear2): Linear(in_features=256, out_features=128, bias=True)\n",
       "          (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm3): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout1): Dropout(p=0.2, inplace=False)\n",
       "          (dropout2): Dropout(p=0.2, inplace=False)\n",
       "          (dropout3): Dropout(p=0.2, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       "  (decoder): Linear(in_features=128, out_features=100, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_dim = 512\n",
    "embed_dim = 128\n",
    "nhead = 2\n",
    "nhid = 256\n",
    "nlayers = 2\n",
    "output_dim = len(all_chars)\n",
    "dropout = 0.2\n",
    "\n",
    "model = TransformerModel(input_dim, embed_dim, nhead, nhid, nlayers, output_dim, dropout)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab5e0848",
   "metadata": {},
   "source": [
    "### Training Cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "9769d753",
   "metadata": {},
   "outputs": [],
   "source": [
    "# criterion = nn.CrossEntropyLoss()\n",
    "# optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "# iters = 100\n",
    "# all_losses = []\n",
    "# epochs = 40\n",
    "# for epoch in range(epochs):\n",
    "#     model.train()\n",
    "#     total_loss = 0.\n",
    "#     for _ in range(iters):  # Adjust the number of batches\n",
    "#         input, target = get_input_and_target()\n",
    "#         # Add batch dimension, shape: (seq_len, batch_size=1)\n",
    "#         input = input.unsqueeze(1).to(device)\n",
    "#         target = target.unsqueeze(1).to(device)\n",
    "     \n",
    "#         src_mask = generate_square_subsequent_mask(input.size(0)).to(device)\n",
    "        \n",
    "#         optimizer.zero_grad()\n",
    "#         output = model(input, src_mask)\n",
    "        \n",
    "#         # Reshape output to (seq_len * batch_size, output_dim) and target to (seq_len * batch_size)\n",
    "#         output = output.view(-1, output_dim)\n",
    "#         target = target.view(-1)\n",
    "        \n",
    "#         loss = criterion(output, target)\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "        \n",
    "#         total_loss += loss.item()\n",
    "        \n",
    "#     all_losses.append(total_loss / iters)\n",
    "#     print(f'Epoch {epoch+1}, Loss: {total_loss / iters}')\n",
    "#     print(\"Generated output:\", generate_text(model))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8097c5f7",
   "metadata": {},
   "source": [
    "### Training with Scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "f36a0933",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/albertovalencia/miniconda3/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:143: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 3.299160463809967\n",
      "Generated output: W\n",
      "W7l cuilll \n",
      "\n",
      "TR9edyin wo pe t s 'lo men belep 'on fes n nimanmanhinhe BG!u un they st,lints lantt b\n",
      "Epoch 2, Loss: 2.7375855302810668\n",
      "Generated output: WWl YSouo oullle wis d o ch bu asiss:\n",
      "Moe Iorous theetooue'deror t?bisd thit toint thce at biv I\n",
      "onc\n",
      "\n",
      "Epoch 3, Loss: 2.6508702301979064\n",
      "Generated output: WLL:NCUNAR:\n",
      "VEN\n",
      "\n",
      "B;Y INMNOBNLExAPSbuthA:\n",
      "LAYLHNAf:\n",
      "MR:\n",
      "CTMARLULANNTCANAUA:\n",
      "\n",
      "TAUOS:)No SLIRFU:\n",
      ">MHADTU\n",
      "Epoch 4, Loss: 2.612783308029175\n",
      "Generated output: Whecw mat:\n",
      "\n",
      "\n",
      "BE\n",
      "\n",
      "PIThalouthat, ar\n",
      "Iut mayheakichathousallll; chy.\n",
      "\n",
      "\n",
      "MI fribayous, y kep theye a ay pu\n",
      "Epoch 5, Loss: 2.6118324327468874\n",
      "Generated output: Whes secst:\n",
      "Ah.\n",
      "DECIOR(:\n",
      "HCHEGECUSARE:\n",
      "SO:\n",
      "\n",
      "\n",
      "LAUhandy in br:\n",
      "\n",
      "\n",
      "ONThand t thin:\n",
      "\n",
      "\n",
      "\n",
      "ORHAUMEY:\n",
      "GTGEI and\n",
      "Epoch 6, Loss: 2.5694403147697447\n",
      "Generated output: WRYK:\n",
      "PRMHRDROONORDFNRTT:\n",
      "LURAS:\n",
      "GRKUR\n",
      "PRRUS:\n",
      "\n",
      "M:\n",
      "TRCOROCE iRLIX:\n",
      "OROLARI:\n",
      "ARRIROvO:\n",
      "ARUGWhAs,ORBURAR\n",
      "Epoch 7, Loss: 2.5291664147377015\n",
      "Generated output: WR:\n",
      "S:\n",
      "KINIU:\n",
      "TCOCndeyowivencandand nd STCIES:\n",
      "Bow\n",
      "Fous o, ndee,\n",
      "He eadeatouck\n",
      "Ave aker tousextheng a\n",
      "Epoch 8, Loss: 2.480518002510071\n",
      "Generated output: WTILTRRORDUSVQUNENGKRUIN/LIK+BEUGRIO:\n",
      "WARDONDS:\n",
      "TEDUD:\n",
      "\n",
      "AR:\n",
      "PLGMERRNUNV:\n",
      "\n",
      "STES:\n",
      "SOS:\n",
      ":\n",
      "ANBNIUORKIMUso\n",
      "Epoch 9, Loss: 2.4810359692573547\n",
      "Generated output: Wmed in mpirlt mit o.\n",
      "I gri's MI lyovee lereg me cha.\n",
      "Heiababexmibar, bu w sous c thimes be misitiore\n",
      "Epoch 10, Loss: 2.4693913793563844\n",
      "Generated output: WRGERERCANCLUCININD:\n",
      "UDUKONUTPD\n",
      "TID:\n",
      "FAS\n",
      "NFKARYOI TIROUIIRERLORG IRIH:\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "OLINPRMURELIZNAS:\n",
      "\n",
      "M INAM\n",
      "Epoch 11, Loss: 2.4569324159622195\n",
      "Generated output: WT:\n",
      "\n",
      "\n",
      "IO:\n",
      "IEUSRBDENENICESKIJA:\n",
      "\n",
      "WAIICIKKIKLIO:\n",
      "IIK:\n",
      "HH'KE~IBIMIFeair RICIUCENIs IYETESINCIINI\n",
      ", INIUN\n",
      "Epoch 12, Loss: 2.4781105875968934\n",
      "Generated output: WS:este lded der d nsanfe st ansthalerond h vememeneo t rerere cle rerefestifoorerh howile id havthe \n",
      "Epoch 13, Loss: 2.4650086903572084\n",
      "Generated output: WiH, serintrnvir bll se noman arerie s darthea j ginthe boncemorof p aran me. ofes lithathong d jo s \n",
      "Epoch 14, Loss: 2.451266577243805\n",
      "Generated output: Wer.\n",
      "\n",
      "Oncer s d.\n",
      "DUSS:\n",
      "IO:\n",
      "HARANIU IMEC?\n",
      "LEO:\n",
      "PAMUCHYPLARHENGAIOORIVINIFay ISKRUO:\n",
      "Wa f!\n",
      "WIme KIUSYUK\n",
      "Epoch 15, Loss: 2.460841946601868\n",
      "Generated output: W chagrerar arre thagour lyo gl way therour? loout oujuetourer:\n",
      "Hol cy.\n",
      "K: g sotur,\n",
      "TlyoththovO:\n",
      "\n",
      "S:\n",
      "\n",
      "Epoch 16, Loss: 2.463447389602661\n",
      "Generated output: WPNHHINGUKIIO:\n",
      "LfLf R:\n",
      "PENOANKINDind INLERHI UCUSSTI ERDIIO:\n",
      "D:\n",
      "\n",
      "HBEROMINGA AS:\n",
      "WHPICUATRY-O:\n",
      "USDONII\n",
      "Epoch 17, Loss: 2.4496109867095948\n",
      "Generated output: WMTAKTKRENHI:\n",
      "NG ER:\n",
      "R-NINUCI,\n",
      "HATKCAS:\n",
      "DDRDSOLUROLIL\n",
      "PI;\n",
      "LLILYPUTION:\n",
      "SL:\n",
      "CANPARDLANVAS:\n",
      "\n",
      "ANYIOUOLUS\n",
      "Epoch 18, Loss: 2.4365186429023744\n",
      "Generated output: WIILUDNGUCUCICIIOPEMO:WGLACADUCIDUODG:\n",
      "ACFPUKIUNGRG\n",
      "ILICHer ERICEDUCAt,\n",
      "PUKITELRKHARLFI\n",
      "BCKILIYUCLKIK\n",
      "Epoch 19, Loss: 2.436434633731842\n",
      "Generated output: WHWrino otoncomo mo toototo d foy, atomon; boit a;\n",
      "Who, mu moned. mout can tat INo thoutheat bt athea\n",
      "Epoch 20, Loss: 2.4382065176963805\n",
      "Generated output: Wititithee mt tithast t t wind avesst ino t othot tosprut it the t st st thino s tho th im trs\n",
      "T hats\n",
      "Epoch 21, Loss: 2.4377929329872132\n",
      "Generated output: WILGNHOOURTHOLYLATENG REUCANTO:\n",
      "WLTONIOUK:\n",
      "O:\n",
      "DMI M:\n",
      "C:\n",
      "CTLERDIDES:LARY:\n",
      "AN:\n",
      "\n",
      "ICIOI: I:\n",
      "AARFA:\n",
      "TNGRIU\n",
      "Epoch 22, Loss: 2.4497725224494933\n",
      "Generated output: WDCRTEDUCO:\n",
      "\n",
      "ANARONGOBDLALLOIISDISD: ENS:\n",
      "MEOIICIKG:\n",
      "ARDILTHINGRIUS ECEULIUERENAR STASENIUSROULNG,\n",
      "\n",
      "A\n",
      "Epoch 23, Loss: 2.4633004379272463\n",
      "Generated output: Wsto\n",
      "TENINOENIINCGUD ICENEV:\n",
      "TOUINGNFOLIHIGAENINGCIIB\n",
      "\n",
      "RLIO&IO:\n",
      "TCANGRIORIO:\n",
      "A? ICIUICORUIXICADIRKOLD\n",
      "Epoch 24, Loss: 2.4422050452232362\n",
      "Generated output: Wis.\n",
      "\n",
      "HARENIIENTKATHALO:\n",
      "\n",
      "SANDRIFHAMIOROSTRKICGSHINANATEThe:\n",
      "JBARUCATO:\n",
      "PAVOKD:\n",
      "NG:\n",
      "NERLCAUCIO:\n",
      "IO:\n",
      "T\n",
      "Epoch 25, Loss: 2.4539113569259645\n",
      "Generated output: Wve peche eeus,\n",
      "Tr gelle;\n",
      "\n",
      "\n",
      "SPrn thallan.\n",
      "\n",
      "F;\n",
      "Amyouror rgitorourour mod nge int s av\n",
      "Thar che f tas,\n",
      "\n",
      "Epoch 26, Loss: 2.4474972462654114\n",
      "Generated output: WNETUD:FINVOO:\n",
      "TODOLNDARDUCLO:\n",
      "\n",
      "ICIVICIFNTENO:\n",
      "WOIONGUWinoudf;\n",
      "PENIZD-\n",
      "ARE:\n",
      "CIRIUS:\n",
      "MZEUCDUSCINGHUK:\n",
      "\n",
      "Epoch 27, Loss: 2.4590828800201416\n",
      "Generated output: W:\n",
      "NHOUPKIIANANECINAONANGECYKIRONGFILARTGGHGEeR<ONNEN\n",
      "DLERANAR RDaNIHARD:\n",
      "LETIO:\n",
      "KENDFELHLHOATSRRIXRU\n",
      "Epoch 28, Loss: 2.441916491985321\n",
      "Generated output: WRT\fdd s my E:\n",
      "CETOL:\n",
      "IORLUSGIDUCETENUO:\n",
      "BI IVENTSLR:\n",
      "MO:\n",
      "\n",
      "\n",
      "HANTATTONUUTIO:\n",
      "ANOIONANGRORLKYSBCHCIONGR\n",
      "Epoch 29, Loss: 2.4555886220932006\n",
      "Generated output: W:\n",
      "ONGODUCCIO:\n",
      "BHIIO IY:\n",
      "SLLLIO:\n",
      "AR\n",
      "ARARORD:\n",
      "Y:\n",
      "\n",
      "R:\n",
      "PENTENTRD, :\n",
      "KONANENCUTIRDANARKHLENENABI-ORNSECIM\n",
      "Epoch 30, Loss: 2.4580751252174378\n",
      "Generated output: W :\n",
      "BULNRANUNU3t TCHAIMOUCINWANICBERUICAGTURALLUCEril NTOKI:\n",
      "NGHO:\n",
      "WYCICHANON INGHENTINCADUSCIRTGOADI\n",
      "Epoch 31, Loss: 2.461468997001648\n",
      "Generated output: WANUBIRDI D:\n",
      "TMan IENLIITCOOLHASCCILENEIRORI\\EZTIZAROIONCI! pUGICATDES:\n",
      "\n",
      "OINGC:\n",
      "DGDUTNCAINTLAHBEPARY:\n",
      "Epoch 32, Loss: 2.451912078857422\n",
      "Generated output: WR\n",
      "RE!\n",
      "CHUDUDaRTISINTPENKe ANRURINLOy.\n",
      "\n",
      "\n",
      "LORIELD:\n",
      "TANABIOONI\n",
      "THIOSLDINVOUL.DEeNOSHARKAR:\n",
      "D\n",
      "LUCI:\n",
      "\n",
      "THE\n",
      "Epoch 33, Loss: 2.4338767313957215\n",
      "Generated output: WEOLIHS IOLDUFMANCIO:\n",
      "TUSiFARIPCINIORDORIis, IUHIUTO:\n",
      "\n",
      "IDES:YCIADDof!,,O t,\n",
      "TIUCHARCEMLIRDIANTRD INCA\n",
      "Epoch 34, Loss: 2.413616645336151\n",
      "Generated output: Wrrorurrouno\n",
      "Sorothowororerefr;\n",
      "Cora.\n",
      "Thou hourooouroour tisuryourouarou d urgrWourour,-me tour trerv\n",
      "Epoch 35, Loss: 2.445961651802063\n",
      "Generated output: W:RLILLXSMATY IS:\n",
      "PARLKI:\n",
      "\n",
      "OS:\n",
      "\n",
      "GUNI s.\n",
      "\n",
      "ANIUTARUqT:\n",
      "G IB:\n",
      "ITIE RARIDELICEOLFDTENNG MDUCCEPE\n",
      "\n",
      "BROL:\n",
      "K\n",
      "Epoch 36, Loss: 2.462262716293335\n",
      "Generated output: WTRDINTHARV:\n",
      "AMLIUKER\n",
      "DIUS:\n",
      "\n",
      "DLI RDINI\n",
      "DINATIXINGHAS:\n",
      "TISele, RSll.\n",
      "\n",
      "WTICENSLNIO:\n",
      "\n",
      "HAD:\n",
      "INDARDATCICEC\n",
      "Epoch 37, Loss: 2.4376979184150698\n",
      "Generated output: W th akishur hor ser howhes u hers tho ckar uree di th hourungrsthlsin woum-sthe, hee han, me f t the\n",
      "Epoch 38, Loss: 2.429302816390991\n",
      "Generated output: WELYORNGNRVKOYDUPINNPENTLRONERKELL:\n",
      "OUWh:\n",
      "ANOUEMITINBES:\n",
      "ARHENNDIORIIUSR\n",
      "DUES:\n",
      "YLoARUCARUN@:\n",
      "W:\n",
      "RGNXT\n",
      "Epoch 39, Loss: 2.453892903327942\n",
      "Generated output: WS:\n",
      "TGRRener CHINIV:\n",
      "Desee mlameroee\n",
      "RI's.\n",
      "ARUONIINP:\n",
      "\n",
      "MINCEIUSRY:\n",
      "\n",
      "CINIUCISENENCAXEOTQANIUSS:\n",
      "\n",
      "\n",
      "\n",
      "UCA\n",
      "Epoch 40, Loss: 2.4431710863113403\n",
      "Generated output: Wh w whewhomes, tenThakus t, bou t motho w w g; ave the\n",
      "Fens; Cor wouttd th au wimithoutou thersth th\n"
     ]
    }
   ],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "exp_lr_scheduler = optim.lr_scheduler.StepLR(optimizer,step_size=7, gamma=0.1)\n",
    "iters = 100\n",
    "all_losses = []\n",
    "epochs = 40\n",
    "for epoch in range(epochs):\n",
    "    exp_lr_scheduler.step()\n",
    "    model.train()\n",
    "    total_loss = 0.\n",
    "    for _ in range(iters):  # Adjust the number of batches\n",
    "        input, target = get_input_and_target()\n",
    "        # Add batch dimension, shape: (seq_len, batch_size=1)\n",
    "        input = input.unsqueeze(1).to(device)\n",
    "        target = target.unsqueeze(1).to(device)\n",
    "     \n",
    "        src_mask = generate_square_subsequent_mask(input.size(0)).to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        output = model(input, src_mask)\n",
    "        \n",
    "        # Reshape output to (seq_len * batch_size, output_dim) and target to (seq_len * batch_size)\n",
    "        output = output.view(-1, output_dim)\n",
    "        target = target.view(-1)\n",
    "        \n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        \n",
    "    all_losses.append(total_loss / iters)\n",
    "    print(f'Epoch {epoch+1}, Loss: {total_loss / iters}')\n",
    "    print(\"Generated output:\", generate_text(model))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "822b6f9f",
   "metadata": {},
   "source": [
    "### Reason for Step\n",
    "\n",
    "Noticing that the loss decreased, but text quality decreased, I asked chat-gpt. Could potentially be due to overfitting, so it suggested adding a step scheduler\n",
    "\n",
    "Used this link to implement step: https://discuss.pytorch.org/t/what-does-scheduler-step-do/47764"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c77d7c2",
   "metadata": {},
   "source": [
    "### Loss Curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "57cf0f82",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.xlabel('iters')\n",
    "plt.ylabel('loss')\n",
    "all_losses = [loss for loss in all_losses]\n",
    "plt.plot(np.array(all_losses))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "29b09fb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input\n",
      " ay,\n",
      "Which you are out of, with a gentler spirit,\n",
      "Or never be so noble as a consul,\n",
      "Nor yoke with him for tribune.\n",
      "\n",
      "MENENIUS:\n",
      "Let'\n",
      "Output\n",
      "ay,\n",
      "Which you are out of, with a gentler spirit,\n",
      "Or never be so noble as a consul,\n",
      "Nor yoke with him for tribune.\n",
      "\n",
      "MENENIUS:\n",
      "Let'silllllll at! stofoilagowhe s ORK:\n",
      "\n",
      "\n",
      "Hy bln IS:\n",
      "\n",
      "Doprnmimeant,\n",
      "T?\n",
      "NIRDiserert t:\n",
      "An mat viuneangrnd \n"
     ]
    }
   ],
   "source": [
    "rand_input = get_random_seq() \n",
    "print(\"Input\\n\",rand_input)\n",
    "print(\"Output\")\n",
    "print(generate_text(model, rand_input))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
